{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capstone_QAModel(SQUAD+PretrainedLM).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR2rv5qxizPq",
        "outputId": "d92be6d8-a7f0-46ac-8ea2-ea69e5090dbe"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=53853e419404a17bc2b2e7a957bcb2f4173873a44f691f7be807c0b51fefc7c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3BI6R5Dkel-",
        "outputId": "954ebcc0-0383-4e07-dc30-1221e07dadf2"
      },
      "source": [
        "# %mkdir drive/MyDrive/capstone/squad/\n",
        "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O drive/MyDrive/capstone/squad/train-v2.0.json\n",
        "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O drive/MyDrive/capstone/squad/dev-v2.0.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-02 13:16:08--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘drive/MyDrive/capstone/squad/train-v2.0.json’\n",
            "\n",
            "drive/MyDrive/capst 100%[===================>]  40.17M  55.7MB/s    in 0.7s    \n",
            "\n",
            "2020-12-02 13:16:28 (55.7 MB/s) - ‘drive/MyDrive/capstone/squad/train-v2.0.json’ saved [42123633/42123633]\n",
            "\n",
            "--2020-12-02 13:16:28--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘drive/MyDrive/capstone/squad/dev-v2.0.json’\n",
            "\n",
            "drive/MyDrive/capst 100%[===================>]   4.17M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-12-02 13:16:28 (40.7 MB/s) - ‘drive/MyDrive/capstone/squad/dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS4zhyA9kXjn",
        "outputId": "a78e5e41-6e34-45f4-e886-4e13af7a2bd9"
      },
      "source": [
        "%cd drive/MyDrive/capstone\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/capstone\n",
            "evaluate-v2.0.py  os_base_tokenizer  run_squad.py  run_squad_trainer.py  squad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQeQ3QpOl77o",
        "outputId": "0ea369f1-2f86-474c-d937-de98d988f58a"
      },
      "source": [
        "!python run_squad.py --model_type bert --model_name_or_path os_base_tokenizer --do_train --do_eval --do_lower_case --train_file squad/train-v2.0.json --predict_file squad/dev-v2.0.json --per_gpu_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 384 --doc_stride 128 --output_dir /tmp/debug_squad/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 13:27:50.995260: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/02/2020 13:27:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "[INFO|configuration_utils.py:409] 2020-12-02 13:27:53,906 >> loading configuration file os_base_tokenizer/config.json\n",
            "[INFO|configuration_utils.py:447] 2020-12-02 13:27:53,907 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:409] 2020-12-02 13:27:53,910 >> loading configuration file os_base_tokenizer/config.json\n",
            "[INFO|configuration_utils.py:447] 2020-12-02 13:27:53,911 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1688] 2020-12-02 13:27:53,911 >> Model name 'os_base_tokenizer' not found in model shortcut name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased). Assuming 'os_base_tokenizer' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "[INFO|tokenization_utils_base.py:1721] 2020-12-02 13:27:53,912 >> Didn't find file os_base_tokenizer/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1721] 2020-12-02 13:27:53,913 >> Didn't find file os_base_tokenizer/tokenizer.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1766] 2020-12-02 13:27:53,913 >> loading file os_base_tokenizer/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1766] 2020-12-02 13:27:53,913 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1766] 2020-12-02 13:27:53,913 >> loading file os_base_tokenizer/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1766] 2020-12-02 13:27:53,913 >> loading file os_base_tokenizer/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1766] 2020-12-02 13:27:53,913 >> loading file None\n",
            "[INFO|modeling_utils.py:938] 2020-12-02 13:27:56,266 >> loading weights file os_base_tokenizer/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:1048] 2020-12-02 13:28:18,352 >> Some weights of the model checkpoint at os_base_tokenizer were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1059] 2020-12-02 13:28:18,352 >> Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at os_base_tokenizer and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "12/02/2020 13:28:30 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='os_base_tokenizer', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=2.0, output_dir='/tmp/debug_squad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=12, predict_file='squad/dev-v2.0.json', save_steps=500, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file='squad/train-v2.0.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
            "12/02/2020 13:28:30 - INFO - __main__ -   Creating features from dataset file at .\n",
            "100% 442/442 [00:51<00:00,  8.62it/s]\n",
            "convert squad examples to features: 100% 130319/130319 [30:50<00:00, 70.42it/s]\n",
            "add example index and unique id: 100% 130319/130319 [00:00<00:00, 864448.19it/s]\n",
            "12/02/2020 14:00:18 - INFO - __main__ -   Saving features into cached file ./cached_train_os_base_tokenizer_384\n",
            "tcmalloc: large alloc 1179639808 bytes == 0x7fa9d8718000 @  0x7fabd64162a4 0x591e47 0x4dd737 0x4e0e12 0x4e255b 0x4e2313 0x4e29d0 0x4e33c6 0x5eb562 0x50a1cc 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIk4kxOKoEjq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}