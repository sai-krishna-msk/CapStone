an operating system acts as an intermediary between the user of a computer and computer hardware. the purpose of an operating system is to provide an environment in which a user can execute programs in a convenient and efficient manner.
an operating system is a software that manages the computer hardware. the hardware must provide appropriate mechanisms to ensure the correct operation of the computer system and to prevent user programs from interfering with the proper operation of the system.
operating system – definition:
an operating system is a program that controls the execution of application programs and acts as an interface between the user of a computer and the computer hardware.
a more common definition is that the operating system is the one program running at all times on the computer (usually called the kernel), with all else being application programs.
an operating system is concerned with the allocation of resources and services, such as memory, processors, devices, and information. the operating system correspondingly includes programs to manage these resources, such as a traffic controller, a scheduler, memory management module, i/o programs, and a file system.
functions of operating system – operating system performs three functions:
convenience: an os makes a computer more convenient to use. efficiency: an os allows the computer system resources to be used in an efficient manner. ability to evolve: an os should be constructed in such a way as to permit the effective development, testing and introduction of new system functions at the same time without interfering with service.
operating system as user interface –
user system and application programs operating system hardware
every general-purpose computer consists of the hardware, operating system, system programs, and application programs. the hardware consists of memory, cpu, alu, and i/o devices, peripheral device, and storage device. system program consists of compilers, loaders, editors, os, etc. the application program consists of business programs, database programs.
fig: conceptual view of a computer system
every computer must have an operating system to run other programs. the operating system coordinates the use of the hardware among the various system programs and application programs for various users. it simply provides an environment within which other programs can do useful work.
the operating system is a set of special programs that run on a computer system that allows it to work properly. it performs basic tasks such as recognizing input from the keyboard, keeping track of files and directories on the disk, sending output to the display screen and controlling peripheral devices.
os is designed to serve two basic purposes:
it controls the allocation and use of the computing system’s resources among the various user and tasks. it provides an interface between the computer hardware and the programmer that simplifies and makes feasible for coding, creation, debugging of application programs.
the operating system must support the following tasks. the task are:
i/o system management –
the module that keeps track of the status of devices is called the i/o traffic controller. each i/o device has a device handler that resides in a separate process associated with that device.
the i/o subsystem consists of
a memory management component that includes buffering caching and spooling.
a general device driver interface.
drivers for specific hardware devices.
assembler –
the input to an assembler is an assembly language program. the output is an object program plus information that enables the loader to prepare the object program for execution. at one time, the computer programmer had at his disposal a basic machine that interpreted, through hardware, certain fundamental instructions. he would program this computer by writing a series of ones and zeros (machine language), place them into the memory of the machine.
compiler –
the high-level languages- examples are fortran, cobol, algol and pl/i are processed by compilers and interpreters. a compiler is a program that accepts a source program in a “high-level language “and produces a corresponding object program. an interpreter is a program that appears to execute a source program as if it was machine language. the same name (fortran, cobol, etc.) is often used to designate both a compiler and its associated language.
loader –
a loader is a routine that loads an object program and prepares it for execution. there are various loading schemes: absolute, relocating and direct-linking. in general, the loader must load, relocate and link the object program. the loader is a program that places programs into memory and prepares them for execution. in a simple loading scheme, the assembler outputs the machine language translation of a program on a secondary device and a loader places it in the core. the loader places into memory the machine language version of the user’s program and transfers control to it. since the loader program is much smaller than the assembler, those make more core available to the user’s program.
history of operating system –
operating system has been evolving through the years. following table shows the history of os.
generation year electronic device used types of os device first - vaccum tubes plug boards second - transistors batch systems third - integrated circuits(ic) multiprogramming fourth since  large scale integration pc
types of operating system –
batch operating system- sequence of jobs in a program on a computer without manual interventions.
time sharing operating system- allows many users to share the computer resources.(max utilization of the resources).
distributed operating system- manages a group of different computers and make appear to be a single computer.
network operating system- computers running in different operating system can participate in common network (it is used for security purpose).
real time operating system – meant applications to fix the deadlines.
examples of operating system are –
windows (gui based, pc)
gnu/linux (personal, workstations, isp, file and print server, three-tier client/server)
macos (macintosh), used for apple’s personal computers and work stations (macbook, imac).
android (google’s operating system for smartphones/tablets/smartwatches)
ios (apple’s os for iphone, ipad and ipod touch)
an operating system performs all the basic tasks like managing file,process, and memory. thus operating system acts as manager of all the resources, i.e. resource manager. thus operating system becomes an interface between user and machine.
types of operating systems: some of the widely used operating systems are as follows-
. batch operating system –
this type of operating system does not interact with the computer directly. there is an operator which takes similar jobs having same requirement and group them into batches. it is the responsibility of operator to sort the jobs with similar needs.
advantages of batch operating system:
it is very difficult to guess or know the time required by any job to complete. processors of the batch systems know how long the job would be when it is in queue
multiple users can share the batch systems
the idle time for batch system is very less
it is easy to manage large work repeatedly in batch systems
disadvantages of batch operating system:
the computer operators should be well known with batch systems
batch systems are hard to debug
it is sometime costly
the other jobs will have to wait for an unknown time if any job fails
examples of batch based operating system: payroll system, bank statements etc.
. time-sharing operating systems –
each task is given some time to execute, so that all the tasks work smoothly. each user gets time of cpu as they use single system. these systems are also known as multitasking systems. the task can be from single user or from different users also. the time that each task gets to execute is called quantum. after this time interval is over os switches over to next task.
advantages of time-sharing os:
each task gets an equal opportunity
less chances of duplication of software
cpu idle time can be reduced
disadvantages of time-sharing os:
reliability problem
one must have to take care of security and integrity of user programs and data
data communication problem
examples of time-sharing oss are: multics, unix etc.
. distributed operating system –
these types of operating system is a recent advancement in the world of computer technology and are being widely accepted all-over the world and, that too, with a great pace. various autonomous interconnected computers communicate each other using a shared communication network. independent systems possess their own memory unit and cpu. these are referred as loosely coupled systems or distributed systems. these system’s processors differ in size and function. the major benefit of working with these types of operating system is that it is always possible that one user can access the files or software which are not actually present on his system but on some other system connected within this network i.e., remote access is enabled within the devices connected in that network.
advantages of distributed operating system:
failure of one will not affect the other network communication, as all systems are independent from each other
electronic mail increases the data exchange speed
since resources are being shared, computation is highly fast and durable
load on host computer reduces
these systems are easily scalable as many systems can be easily added to the network
delay in data processing reduces
disadvantages of distributed operating system:
failure of the main network will stop the entire communication
to establish distributed systems the language which are used are not well defined yet
these types of systems are not readily available as they are very expensive. not only that the underlying software is highly complex and not understood well yet
examples of distributed operating system are- locus etc.
. network operating system –
these systems run on a server and provide the capability to manage data, users, groups, security, applications, and other networking functions. these type of operating systems allow shared access of files, printers, security, applications, and other networking functions over a small private network. one more important aspect of network operating systems is that all the users are well aware of the underlying configuration, of all other users within the network, their individual connections etc. and that’s why these computers are popularly known as tightly coupled systems.
advantages of network operating system:
highly stable centralized servers
security concerns are handled through servers
new technologies and hardware up-gradation are easily integrated to the system
server access are possible remotely from different locations and types of systems
disadvantages of network operating system:
servers are costly
user has to depend on central location for most operations
maintenance and updates are required regularly
examples of network operating system are: microsoft windows server , microsoft windows server , unix, linux, mac os x, novell netware, and bsd etc.
. real-time operating system –
these types of oss serves the real-time systems. the time interval required to process and respond to inputs is very small. this time interval is called response time.
real-time systems are used when there are time requirements are very strict like missile systems, air traffic control systems, robots etc.
two types of real-time operating system which are as follows:
hard real-time systems:
these oss are meant for the applications where time constraints are very strict and even the shortest possible delay is not acceptable. these systems are built for saving life like automatic parachutes or air bags which are required to be readily available in case of any accident. virtual memory is almost never found in these systems.
these oss are meant for the applications where time constraints are very strict and even the shortest possible delay is not acceptable. these systems are built for saving life like automatic parachutes or air bags which are required to be readily available in case of any accident. virtual memory is almost never found in these systems. soft real-time systems:
these oss are for applications where for time-constraint is less strict.
advantages of rtos:
maximum consumption: maximum utilization of devices and system,thus more output from all the resources
maximum utilization of devices and system,thus more output from all the resources task shifting: time assigned for shifting tasks in these systems are very less. for example in older systems it takes about  micro seconds in shifting one task to another and in latest systems it takes  micro seconds.
time assigned for shifting tasks in these systems are very less. for example in older systems it takes about  micro seconds in shifting one task to another and in latest systems it takes  micro seconds. focus on application: focus on running applications and less importance to applications which are in queue.
focus on running applications and less importance to applications which are in queue. real time operating system in embedded system: since size of programs are small, rtos can also be used in embedded systems like in transport and others.
since size of programs are small, rtos can also be used in embedded systems like in transport and others. error free: these types of systems are error free.
these types of systems are error free. memory allocation: memory allocation is best managed in these type of systems.
disadvantages of rtos:
prerequisite – introduction of operating system – set 
an operating system acts as a communication bridge (interface) between the user and computer hardware. the purpose of an operating system is to provide a platform on which a user can execute programs in a convenient and efficient manner.
an operating system is a piece of software that manages the allocation of computer hardware. the coordination of the hardware must be appropriate to ensure the correct working of the computer system and to prevent user programs from interfering with the proper working of the system.
example: just like a boss gives order to his employee, in the similar way we request or pass our orders to the operating system. the main goal of the operating system is to thus make the computer environment more convenient to use and the secondary goal is to use the resources in the most efficient manner.
what is operating system ?
an operating system is a program on which application programs are executed and acts as an communication bridge (interface) between the user and the computer hardware.
the main task an operating system carries out is the allocation of resources and services, such as allocation of: memory, devices, processors and information. the operating system also includes programs to manage these resources, such as a traffic controller, a scheduler, memory management module, i/o programs, and a file system.
important functions of an operating system:
security –
the operating system uses password protection to protect user data and similar other techniques. it also prevents unauthorized access to programs and user data. control over system performance –
monitors overall system health to help improve performance. records the response time between service requests and system response to have a complete view of the system health. this can help improve performance by providing important information needed to troubleshoot problems. job accounting –
operating system keeps track of time and resources used by various tasks and users, this information can be used to track resource usage for a particular user or group of user. error detecting aids –
operating system constantly monitors the system to detect errors and avoid the malfunctioning of computer system.
coordination between other software and users –
operating systems also coordinate and assign interpreters, compilers, assemblers and other software to the various users of the computer systems. memory management –
the operating system manages the primary memory or main memory. main memory is made up of a large array of bytes or words where each byte or word is assigned a certain address. main memory is a fast storage and it can be accessed directly by the cpu. for a program to be executed, it should be first loaded in the main memory. an operating system performs the following activities for memory management: it keeps tracks of primary memory, i.e., which bytes of memory are used by which user program. the memory addresses that have already been allocated and the memory addresses of the memory that has not yet been used. in multi programming, the os decides the order in which process are granted access to memory, and for how long. it allocates the memory to a process when the process requests it and deallocates the memory when the process has terminated or is performing an i/o operation. processor management –
in a multi programming environment, the os decides the order in which processes have access to the processor, and how much processing time each process has. this function of os is called process scheduling. an operating system performs the following activities for processor management. keeps tracks of the status of processes. the program which perform this task is known as traffic controller. allocates the cpu that is processor to a process. de-allocates processor when a process is no more required. device management –
an os manages device communication via their respective drivers. it performs the following activities for device management. keeps tracks of all devices connected to system. designates a program responsible for every device known as the input/output controller. decides which process gets access to a certain device and for how long. allocates devices in an effective and efficient way. deallocates devices when they are no longer required. file management –
a file system is organized into directories for efficient or easy navigation and usage. these directories may contain other directories and other files. an operating system carries out the following file management activities. it keeps track of where information is stored, user access settings and status of every file and more… these facilities are collectively known as the file system.
moreover, operating system also provides certain services to the computer system in one form or the other.
the operating system provides certain services to the users which can be listed in the following manner:
program execution: the operating system is responsible for execution of all types of programs whether it be user programs or system programs. the operating system utilises various resources available for the efficient running of all types of functionalities. handling input/output operations: the operating system is responsible for handling all sort of inputs, i.e, from keyboard, mouse, desktop, etc. the operating system does all interfacing in the most appropriate manner regrading all kind of inputs and outputs.
for example, there is difference in nature of all types of peripheral devices such as mouse or keyboard, then operating system is responsible for handling data between them. manipulation of file system: the operating system is responsible for making of decisions regarding the storage of all types of data or files, i.e, floppy disk/hard disk/pen drive, etc. the operating system decides as how should the data should be manipulated and stored. error detection and handling: the operating system is responsible for detection of any types of error or bugs that can occur while any task. the well secured os sometimes also acts as countermeasure for preventing any sort of breach to the computer system from any external source and probably handling them. resource allocation: the operating system ensures the proper use of all the resources available by deciding which resource to be used by whom for how much time. all the decisions are taken by the operating system. accounting: the operating system tracks an account of all the functionalities taking place in the computer system at a time. all the details such as the types of errors occurred are recorded by the operating system. information and resource protection: the operating system is responsible for using all the information and resources available on the machine in the most protected way. the operating system must foil an attempt from any external resource to hamper any sort of data or information.
all these services are ensured by the operating system for the convenience of the users to make the programming task easier. all different kinds of operating system more or less provide the same services.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
amaninder.singh a student of btech interested in coding and know languages like java c and many more
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : karimkamel, skylags
real time system means that the system is subjected to real time, i.e., response should be guaranteed within a specified timing constraint or system should meet the specified deadline. for example: flight control system, real time monitors etc.
types of real time systems based on timing constraints:
hard real time system –
this type of system can never miss its deadline. missing the deadline may have disastrous consequences.the usefulness of result produced by a hard real time system decreases abruptly and may become negative if tardiness increases. tardiness means how late a real time system completes its task with respect to its deadline. example: flight controller system. soft real time system –
this type of system can miss its deadline occasionally with some acceptably low probability. missing the deadline have no disastrous consequences. the usefulness of result produced by a soft real time system decreases gradually with increase in tardiness. example: telephone switches.
reference model of real time system: our reference model is characterized by three elements:
a workload model: it specifies the application supported by system. a resource model: it specifies the resources available to the application. algorithms: it specifies how the application system will use resources.
terms related to real time system:
job – a job is a small piece of work that can be assigned to a processor and may or may not require resources.
a job is a small piece of work that can be assigned to a processor and may or may not require resources. task – a set of related jobs that jointly provide some system functionality.
a set of related jobs that jointly provide some system functionality. release time of a job – it is the time at which job becomes ready for execution.
it is the time at which job becomes ready for execution. execution time of a job – it is the time taken by job to finish its execution.
it is the time taken by job to finish its execution. deadline of a job – it is the time by which a job should finish its execution. deadline is of two types: absolute deadline and relative deadline.
it is the time by which a job should finish its execution. deadline is of two types: absolute deadline and relative deadline. response time of a job – it is the length of time from release time of a job to the instant when it finishes.
it is the length of time from release time of a job to the instant when it finishes. maximum allowable response time of a job is called its relative deadline.
absolute deadline of a job is equal to its relative deadline plus its release time.
processors are also known as active resources. they are essential for execution of a job. a job must have one or more processors in order to execute and proceed towards completion. example: computer, transmission links.
resources are also known as passive resources. a job may or may not require a resource during its execution. example: memory, mutex
two resources are identical if they can be used interchangeably else they are heterogeneous.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : krishnakoumar
the system is subjected to real time, i.e. response should be guaranteed within a specified timing constraint or system should meet the specified deadline. for example flight control system, real-time monitors etc.
there are two types of tasks in real-time systems:
periodic tasks dynamic tasks
periodic tasks: in periodic task, jobs are released at regular intervals. a periodic task is one which repeats itself after a fixed time interval. a periodic task is denoted by five tuples: t i = 
where, φ i – is the phase of the task. phase is release time of the first job in the task. if the phase is not mentioned then release time of first job is assumed to be zero. p i – is the period of the task i.e. the time interval between the release times of two consecutive jobs. e i – is the execution time of the task. d i – is the relative deadline of the task. for example: consider the task t i with period =  and execution time = 
phase is not given so, assume the release time of the first job as zero. so the job of this task is first released at t =  then it executes for s and then next job is released at t =  which executes for s and then next job is released at t = . so jobs are released at t = k where k = , , . . ., n
hyper period of a set of periodic tasks is the least common multiple of periods of all the tasks in that set. for example, two tasks t  and t  having period  and  respectively will have a hyper period, h = lcm(p, p) = lcm(, ) = . the hyper period is the time after which pattern of job release times starts to repeat.
in periodic task, jobs are released at regular intervals. a periodic task is one which repeats itself after a fixed time interval. a periodic task is denoted by five tuples: where, dynamic tasks: it is a sequential program that is invoked by the occurrence of an event. an event may be generated by the processes external to the system or by processes internal to the system. dynamically arriving tasks can be categorized on their criticality and knowledge about their occurrence times. aperiodic tasks: in this type of task, jobs are released at arbitrary time intervals i.e. randomly. aperiodic tasks have soft deadlines or no deadlines. sporadic tasks: they are similar to aperiodic tasks i.e. they repeat at random instances. the only difference is that sporadic tasks have hard deadlines. a speriodic task is denoted by three tuples: t i =(e i , g i , d i )
where
e i – the execution time of the task.
g i – the minimum separation between the occurrence of two consecutive instances of the task.
d i – the relative deadline of the task.
it is a sequential program that is invoked by the occurrence of an event. an event may be generated by the processes external to the system or by processes internal to the system. dynamically arriving tasks can be categorized on their criticality and knowledge about their occurrence times.
jitter: sometimes actual release time of a job is not known. only know that r i is in a range [ r i -, r i + ]. this range is known as release time jitter. here r i – is how early a job can be released and r i + is how late a job can be released. only range [ e i -, e i + ] of the execution time of a job is known. here e i – is the minimum amount of time required by a job to complete its execution and e i + the maximum amount of time required by a job to complete its execution.
precedence constraint of jobs: jobs in a task are independent if they can be executed in any order. if there is a specific order in which jobs in a task have to be executed then jobs are said to have precedence constraints. for representing precedence constraints of jobs a partial order relation < is used. this is called precedence relation. a job j i is a predecessor of job j j if j i < j j i.e. j j cannot begin its execution until j i completes. j i is an immediate predecessor of j j if j i < j j and there is no other job j k such that j i < j k < j j . j i and j j are independent if neither j i < j j nor j j < j i is true.
an efficient way to represent precedence constraints is by using a directed graph g = (j, <) where j is the set of jobs. this graph is known as the precedence graph. jobs are represented by vertices of graph and precedence constraints are represented using directed edges. if there is a directed edge from j i to j j then it means that j i is immediate predecessor of j j . for example: consider a task t having  jobs j  , j  , j  , j  and j  such that j  and j  cannot begin their execution until j  completes and there are no other constraints.
the precedence constraints for this example are:
j  < j  and j  < j 
set representation of precedence graph:
consider another example where precedence graph is given and you have to find precedence constraints
from the above graph, we derive the following precedence constraints:
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : dcbleo
multiprogramming – a computer running more than one program at a time (like running excel and firefox simultaneously). multiprocessing – a computer using more than one cpu at a time. multitasking – tasks sharing a common resource (like  cpu). multithreading is an extension of multitasking.
. multi programming –
in a modern computing system, there are usually several concurrent application processes which want to execute. now it is the responsibility of the operating system to manage all the processes effectively and efficiently.
one of the most important aspects of an operating system is to multi program.
in a computer system, there are multiple processes waiting to be executed, i.e. they are waiting when the cpu will be allocated to them and they begin their execution. these processes are also known as jobs. now the main memory is too small to accommodate all of these processes or jobs into it. thus, these processes are initially kept in an area called job pool. this job pool consists of all those processes awaiting allocation of main memory and cpu.
cpu selects one job out of all these waiting jobs, brings it from the job pool to main memory and starts executing it. the processor executes one job until it is interrupted by some external factor or it goes for an i/o task.
non-multi programmed system’s working –
in a non multi programmed system, as soon as one job leaves the cpu and goes for some other task (say i/o ), the cpu becomes idle. the cpu keeps waiting and waiting until this job (which was executing earlier) comes back and resumes its execution with the cpu. so cpu remains free for all this while.
now it has a drawback that the cpu remains idle for a very long period of time. also, other jobs which are waiting to be executed might not get a chance to execute because the cpu is still allocated to the earlier job.
this poses a very serious problem that even though other jobs are ready to execute, cpu is not allocated to them as the cpu is allocated to a job which is not even utilizing it (as it is busy in i/o tasks).
this poses a very serious problem that even though other jobs are ready to execute, cpu is not allocated to them as the cpu is allocated to a job which is not even utilizing it (as it is busy in i/o tasks). it cannot happen that one job is using the cpu for say  hour while the others have been waiting in the queue for  hours. to avoid situations like this and come up with efficient utilization of cpu, the concept of multi programming came up.
the main idea of multi programming is to maximize the cpu time.
multi programmed system’s working –
in a multi-programmed system, as soon as one job goes for an i/o task, the operating system interrupts that job, chooses another job from the job pool (waiting queue), gives cpu to this new job and starts its execution. the previous job keeps doing its i/o operation while this new job does cpu bound tasks. now say the second job also goes for an i/o task, the cpu chooses a third job and starts executing it. as soon as a job completes its i/o operation and comes back for cpu tasks, the cpu is allocated to it.
in this way, no cpu time is wasted by the system waiting for the i/o task to be completed.
therefore, the ultimate goal of multi programming is to keep the cpu busy as long as there are processes ready to execute. this way, multiple programs can be executed on a single processor by executing a part of a program at one time, a part of another program after this, then a part of another program and so on, hence executing multiple programs. hence, the cpu never remains idle.
in the image below, program a runs for some time and then goes to waiting state. in the mean time program b begins its execution. so the cpu does not waste its resources and gives program b an opportunity to run.
. multiprocessing –
in a uni-processor system, only one process executes at a time.
multiprocessing is the use of two or more cpus (processors) within a single computer system. the term also refers to the ability of a system to support more than one processor within a single computer system. now since there are multiple processors available, multiple processes can be executed at a time. these multi processors share the computer bus, sometimes the clock, memory and peripheral devices also.
multi processing system’s working –
with the help of multiprocessing, many processes can be executed simultaneously. say processes p, p, p and p are waiting for execution. now in a single processor system, firstly one process will execute, then the other, then the other and so on.
but with multiprocessing, each process can be assigned to a different processor for its execution. if its a dual-core processor ( processors), two processes can be executed simultaneously and thus will be two times faster, similarly a quad core processor will be four times as fast as a single processor.
why use multi processing –
the main advantage of multiprocessor system is to get more work done in a shorter period of time. these types of systems are used when very high speed is required to process a large volume of data. multi processing systems can save money in comparison to single processor systems because the processors can share peripherals and power supplies.
it also provides increased reliability in the sense that if one processor fails, the work does not halt, it only slows down. e.g. if we have  processors and  fails, then the work does not halt, rather the remaining  processors can share the work of the th processor. thus the whole system runs only  percent slower, rather than failing altogether.
multiprocessing refers to the hardware (i.e., the cpu units) rather than the software (i.e., running processes). if the underlying hardware provides more than one processor then that is multiprocessing. it is the ability of the system to leverage multiple processors’ computing power.
difference between multi programming and multi processing –
a system can be both multi programmed by having multiple programs running at the same time and multiprocessing by having more than one physical processor. the difference between multiprocessing and multi programming is that multiprocessing is basically executing multiple processes at the same time on multiple processors, whereas multi programming is keeping several programs in main memory and executing them concurrently using a single cpu only.
multiprocessing occurs by means of parallel processing whereas multi programming occurs by switching from one process to other (phenomenon called as context switching).
. multitasking –
as the name itself suggests, multi tasking refers to execution of multiple tasks (say processes, programs, threads etc.) at a time. in the modern operating systems, we are able to play mp music, edit documents in microsoft word, surf the google chrome all simultaneously, this is accomplished by means of multi tasking.
multitasking is a logical extension of multi programming. the major way in which multitasking differs from multi programming is that multi programming works solely on the concept of context switching whereas multitasking is based on time sharing alongside the concept of context switching.
multi tasking system’s working –
in a time sharing system, each process is assigned some specific quantum of time for which a process is meant to execute. say there are  processes p, p, p, p ready to execute. so each of them are assigned some time quantum for which they will execute e.g time quantum of  nanoseconds ( ns). as one process begins execution (say p), it executes for that quantum of time ( ns). after  ns the cpu starts the execution of the other process (say p) for the specified quantum of time.
thus the cpu makes the processes to share time slices between them and execute accordingly. as soon as time quantum of one process expires, another process begins its execution.
here also basically a context switch is occurring but it is occurring so fast that the user is able to interact with each program separately while it is running. this way, the user is given the illusion that multiple processes/ tasks are executing simultaneously. but actually only one process/ task is executing at a particular instant of time. in multitasking, time sharing is best manifested because each running process takes only a fair quantum of the cpu time.
in a more general sense, multitasking refers to having multiple programs, processes, tasks, threads running at the same time. this term is used in modern operating systems when multiple tasks share a common processing resource (e.g., cpu and memory).
as depicted in the above image, at any time the cpu is executing only one task while other tasks are waiting for their turn. the illusion of parallelism is achieved when the cpu is reassigned to another task. i.e all the three tasks a, b and c are appearing to occur simultaneously because of time sharing.
so for multitasking to take place, firstly there should be multiprogramming i.e. presence of multiple programs ready for execution. and secondly the concept of time sharing.
. multi threading –
e.g. vlc media player, where one thread is used for opening the vlc media player, one thread for playing a particular song and another thread for adding new songs to the playlist.
multi threading is the ability of a process to manage its use by more than one user at a time and to manage multiple requests by the same user without having to have multiple copies of the program.
multi threading system’s working –
example  –
say there is a web server which processes client requests. now if it executes as a single threaded process, then it will not be able to process multiple requests at a time. firstly one client will make its request and finish its execution and only then, the server will be able to process another client request. this is really costly, time consuming and tiring task. to avoid this, multi threading can be made use of.
now, whenever a new client request comes in, the web server simply creates a new thread for processing this request and resumes its execution to hear more client requests. so the web server has the task of listening to new client requests and creating threads for each individual request. each newly created thread processes one client request, thus reducing the burden on web server.
example  –
we can think of threads as child processes that share the parent process resources but execute independently. now take the case of a gui. say we are performing a calculation on the gui (which is taking very long time to finish). now we can not interact with the rest of the gui until this command finishes its execution. to be able to interact with the rest of the gui, this command of calculation should be assigned to a separate thread. so at this point of time,  threads will be executing i.e. one for calculation, and one for the rest of the gui. hence here in a single process, we used multiple threads for multiple functionality.
the image below completely describes the vlc player example:
advantages of multi threading –
benefits of multi threading include increased responsiveness. since there are multiple threads in a program, so if one thread is taking too long to execute or if it gets blocked, the rest of the threads keep executing without any problem. thus the whole program remains responsive to the user by means of remaining threads.
another advantage of multi threading is that it is less costly. creating brand new processes and allocating resources is a time consuming task, but since threads share resources of the parent process, creating threads and switching between them is comparatively easy. hence multi threading is the need of modern operating systems.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
memory is the most essential element of a computing system because without it computer can’t perform simple tasks. computer memory is of two basic type – primary memory(ram and rom) and secondary memory(hard drive,cd,etc.). random access memory (ram) is primary-volatile memory and read only memory (rom) is primary-non-volatile memory.
. random access memory (ram) –
. read only memory (rom) –
types of read only memory (rom) –
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please improve this article if you find anything incorrect by clicking on the "improve article" button below.
in computing, there exist two type processor i.e., -bit and -bit. this type of processor tells us how much memory a processor can have access from a cpu register. for instance,
a -bit system can access  memory addresses, i.e  gb of ram or physical memory ideally, it can access more than  gb of ram also.
a -bit system can access  memory addresses, i.e actually -quintillion bytes of ram. in short, any amount of memory greater than  gb can be easily handled by it.
most computers made in the s and early s were -bit machines. the cpu register stores memory addresses, which is how the processor accesses data from ram. one bit in the register can reference an individual byte in memory, so a -bit system can address a maximum of  gb (,,, bytes) of ram. the actual limit is often less around . gb since part of the register is used to store other temporary values besides memory addresses. most computers released over the past two decades were built on a -bit architecture, hence most operating systems were designed to run on a -bit processor.
a -bit register can theoretically reference ,,,,,, bytes, or ,,, gb ( exabytes) of memory. this is several million times more than an average workstation would need to access. what’s important is that a -bit computer (which means it has a -bit processor) can access more than  gb of ram. if a computer has  gb of ram, it better has a -bit processor. otherwise, at least  gb of the memory will be inaccessible by the cpu.
a major difference between -bit processors and -bit processors is the number of calculations per second they can perform, which affects the speed at which they can complete tasks. -bit processors can come in dual-core, quad-core, six-core, and eight-core versions for home computing. multiple cores allow for an increased number of calculations per second that can be performed, which can increase the processing power and help make a computer run faster. software programs that require many calculations to function smoothly can operate faster and more efficiently on the multi-core -bit processors, for the most part.
advantages of -bit over -bit
using -bit one can do a lot in multi-tasking, user can easily switch between various applications without any windows hanging problems.
gamers can easily play high graphical games like modern warfare, gta v, or use high-end software like photoshop or cad which takes a lot of memory since it makes multi-tasking with big software, easy and efficient for users. however upgrading the video card instead of getting a -bit processor would be more beneficial.
note:
a computer with a -bit processor can have a -bit or -bit version of an operating system installed. however, with a -bit operating system, the -bit processor would not run at its full capability.
on a computer with a -bit processor, we can’t run a -bit legacy program. many -bit programs will work with a -bit processor and operating system, but some older -bit programs may not function properly, or at all, due to limited or no compatibility.
a computer without a program running is just an inert hunk of electronics. the first thing a computer has to do when it is turned on is to start up a special program called an operating system. the operating system’s job is to help other computer programs to work by handling the messy details of controlling the computer’s hardware.
an overview of the boot process
the boot process is something that happens every time you turn your computer on. you don’t really see it, because it happens so fast. you press the power button come back a few minutes later and windows xp, or windows vista, or whatever operating system you use is all loaded.
the bios chip tells it to look in a fixed place, usually on the lowest-numbered hard disk (the boot disk) for a special program called a boot loader (under linux the boot loader is called grub or lilo). the boot loader is pulled into memory and started. the boot loader’s job is to start the real operating system.
functions of bios
post (power on self test) the power on self test happens each time you turn your computer on. it sounds complicated and that’s because it kind of is. your computer does so much when its turned on and this is just part of that.
it initializes the various hardware devices. it is an important process so as to ensure that all the devices operate smoothly without any conflicts. bioses following acpi create tables describing the devices in the computer.
master boot record
the master boot record (mbr) is a small program that starts when the computer is booting, in order to find the operating system (eg. windows xp). this complicated process (called the boot process) starts with the post (power on self test) and ends when the bios searches for the mbr on the hard drive, which is generally located in the first sector, first head, first cylinder (cylinder , head , sector ).
a typical structure looks like:
the bootstrap loader is stored in computer’s eprom, rom, or another non-volatile memory. when the computer is turned on or restarted, it first performs the power-on-self-test, also known as post. if the post is successful and no issues are found, the bootstrap loader will load the operating system for the computer into memory. the computer will then be able to quickly access, load, and run the operating system.
init
init is the last step of the kernel boot sequence. it looks for the file /etc/inittab to see if there is an entry for initdefault. it is used to determine initial run-level of the system. a run-level is used to decide the initial state of the operating system.
some of the run levels are:
level
 –> system halt
 –> single user mode
 –> full multiuser mode with network
 –> full multiuser mode with network and x display manager
 –> reboot
the above design of init is called sysv- pronounced as system five. several other implementations of init have been written now. some of the popular implementatios are systemd and upstart. upstart is being used by ubuntu since . more details of the upstart can be found here.
the next step of init is to start up various daemons that support networking and other services. x server daemon is one of the most important daemon. it manages display, keyboard, and mouse. when x server daemon is started you see a graphical interface and a login screen is displayed.
basically for a computer to start running to get an instance when it is powered up or rebooted it need to have an initial program to run. and this initial program which is known as bootstrap need to be simple. it must initialize all aspects of the system, from cpu registers to device controllers and the contents of the main memory and then starts the operating system.
to do this job the bootstrap program basically finds the operating system kernel on disk and then loads the kernel into memory and after this, it jumps to the initial address to begin the operating-system execution.
why rom:
for most of today’s computer bootstrap is stored in read only memory (rom).
this location is good for storage because this place doesn’t require initialization and moreover location here it is fixed so that processor can start executing when powered up or reset.
rom is basically read-only memory and hence it cannot be affected by the computer virus.
example:
let us try to understand this using an example of the boot process in windows .
the following figure shows the booting from disk in windows .
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : helpful_thief
the unified extensible firmware interface (uefi), like bios (basic input output system) is a firmware that runs when the computer is booted. it initializes the hardware and loads the operating system into the memory. however, being the more modern solution and overcoming various limitations of bios, uefi is all set to replace the former.
but what makes bios outdated?
present in all ibm pc-compatible personal computers, bios has been around since the late s. since then, it has incorporated some major improvements such as addition of a user interface, and advanced power management functions, which allow bios to easily configure the pcs and create better power management plans. yet, it hasn’t advanced as much as the computer hardware and software technology since the s.
limitations of bios
bios can boot from drives of less than  tb. + tb drives are now standard, and a system with a bios can’t boot from them.
bios runs in -bit processor mode, and has only  mb space to execute.
it can’t initialize multiple hardware devices at once, thus leading to slow booting process.
difference between the booting process with uefi and the booting process with bios
booting process with bios : when bios begins it’s execution, it first goes for the power-on self test (post), which ensures that the hardware devices are functioning correctly. after that, it checks for the master boot record in the first sector of the selected boot device. from the mbr, the location of the boot-loader is retrieved, which, after being loaded by bios into the computer’s ram, loads the operating system into the main memory.
when bios begins it’s execution, it first goes for the power-on self test (post), which ensures that the hardware devices are functioning correctly. after that, it checks for the master boot record in the first sector of the selected boot device. from the mbr, the location of the boot-loader is retrieved, which, after being loaded by bios into the computer’s ram, loads the operating system into the main memory. booting process with uefi : unlike bios, uefi doesn’t look for the mbr in the first sector of the boot device. it maintains a list of valid boot volumes called efi service partitions. during the post procedure the uefi firmware scans all of the bootable storage devices that are connected to the system for a valid guid partition table (gpt), which is an improvement over mbr. unlike the mbr, gpt doesn’t contain a boot-loader. the firmware itself scans the gpt to find an efi service partition to boot from, and directly loads the os from the right partition. if it fails to find one, it goes back the bios-type booting process called ‘legacy boot’.
advantages of uefi over bios
breaking out of size limitations : the uefi firmware can boot from drives of . tb or larger with the theoretical upper limit being . zettabytes, which is roughly  times the size of the total information present on the internet. this is due to the fact that gpt uses -bit entries in it’s table, thereby dramatically expanding the possible boot-device size.
the uefi firmware can boot from drives of . tb or larger with the theoretical upper limit being . zettabytes, which is roughly  times the size of the total information present on the internet. this is due to the fact that gpt uses -bit entries in it’s table, thereby dramatically expanding the possible boot-device size. speed and performance : uefi can run in -bit or -bit mode and has more addressable address space than bios, which means your boot process is faster.
uefi can run in -bit or -bit mode and has more addressable address space than bios, which means your boot process is faster. more user-friendly interface : since uefi can run in -bit and -bit mode, it provides better ui configuration that has better graphics and also supports mouse cursor.
since uefi can run in -bit and -bit mode, it provides better ui configuration that has better graphics and also supports mouse cursor. security: uefi also provides the feature of secure boot. it allows only authentic drivers and services to load at boot time, to make sure that no malware can be loaded at computer startup. it also requires drivers and the kernel to have digital signature, which makes it an effective tool in countering piracy and boot-sector malware.
uefi doesn’t require a boot-loader, and can also operate alongside bios, supporting legacy boot, which in turn, makes it compatible with older operating systems. intel plans to completely replace bios with uefi, for all it’s chipsets, by .
my personal notes arrow_drop_up save
parthdutt intern technical content writing at geeksforgeeks
kernel is the core part of an operating system which manages system resources. it also acts like a bridge between application and hardware of the computer. it is one of the first programs loaded on start-up (after the bootloader).
kernel mode and user mode of cpu operation
the cpu can execute certain instruction only when it is in the kernel mode. these instruction are called privilege instruction. they allow implementation of special operation whose execution by the user program could interface with the functioning of operating system or activity of another user program. for example, instruction for managing memory protection.
the operating system puts the cpu in kernel mode when it is executing in the kernel so, that kernel can execute some special operation.
the operating system puts the cpu in user mode when a user program is in execution so, that user program cannot interface with the operating system program.
user-level instruction does not require special privilege. example are add,push,etc.
the concept of modes can be extended beyond two, requiring more than a single mode bit cpus that support virtualization use one of these extra bits to indicate when the virtual machine manager, vmm, is in control of the system. the vmm has more privileges than ordinary user programs, but not so many as the full kernel.
system calls are typically implemented in the form of software interrupts, which causes the hardware’s interrupt handler to transfer control over to an appropriate interrupt handler, which is part of the operating system, switching the mode bit to kernel mode in the process. the interrupt handler checks exactly which interrupt was generated, checks additional parameters ( generally passed through registers ) if appropriate, and then calls the appropriate kernel service routine to handle the service requested by the system call.
user programs’ attempts to execute illegal instructions ( privileged or non-existent instructions ), or to access forbidden memory areas, also generate software interrupts, which are trapped by the interrupt handler and control is transferred to the os, which issues an appropriate error message, possibly dumps data to a log ( core ) file for later analysis, and then terminates the offending program.
what is microkernel?
microkernel is one of the classification of the kernel. being a kernel it manages all system resources. but in a microkernel, the user services and kernel services are implemented in different address space. the user services are kept in user address space, and kernel services are kept under kernel address space, thus also reduces the size of kernel and size of operating system as well.
it provides minimal services of process and memory management. the communication between client program/application and services running in user address space is established through message passing, reducing the speed of execution microkernel. the operating system remains unaffected as user services and kernel services are isolated so if any user service fails it does not affect kernel service. thus it adds to one of the advantages in a microkernel. it is easily extendable i.e. if any new services are to be added they are added to user address space and hence requires no modification in kernel space. it is also portable, secure and reliable.
microkernel architecture –
since kernel is the core part of the operating system, so it is meant for handling the most important services only. thus in this architecture only the most important services are inside kernel and rest of the os services are present inside system application program. thus users are able to interact with those not-so important services within the system application. and the microkernel is solely responsible for the most important services of operating system they are named as follows:
inter process-communication
memory management
cpu-scheduling
advantages of microkernel –
the architecture of this kernel is small and isolated hence it can function better.
expansion of the system is easier, it is simply added in the system application without disturbing the kernel.
eclipse ide is a good example of microkernel architecture.
read next – monolithic kernel and key differences from microkernel
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
prerequisite – microkernel
the kernel provides many services related to i/o. several services such as scheduling, caching, spooling, device reservation, and error handling – are provided by the kernel, s i/o subsystem built on the hardware and device-driver infrastructure. the i/o subsystem is also responsible for protecting itself from the errant processes and malicious users.
i/o scheduling –
to schedule a set of i/o request means to determine a good order in which to execute them. the order in which application issues the system call are the best choice. scheduling can improve the overall performance of the system, can share device access permission fairly to all the processes, reduce the average waiting time, response time, turnaround time for i/o to complete. os developers implement scheduling by maintaining a wait queue of the request for each device. when an application issue a blocking i/o system call, the request is placed in the queue for that device. the i/o scheduler rearrange the order to improve the efficiency of the system. buffering –
a buffer is a memory area that stores data being transferred between two devices or between a device and an application. buffering is done for three reasons. first is to cope with a speed mismatch between producer and consumer of a data stream. the second use of buffering is to provide adaptation for data that have different data-transfer sizes. third use of buffering is to support copy semantics for the application i/o, “copy semantic ” means, suppose that an application wants to write data on a disk that is stored in its buffer. it calls the write() system’s call, providing a pointer to the buffer and the integer specifying the number of bytes to write. q. after the system call returns, what happens if the application of the buffer changes the content of the buffer?
ans. with copy semantic, the version of the data written to the disk is guaranteed to be the version at the time of the application system call. caching –
a cache is a region of fast memory that holds a copy of data. access to the cached copy is much easier than the original file. for instance, the instruction of the currently running process is stored on the disk, cached in physical memory, and copied again in the cpu’s secondary and primary cache.
the main difference between a buffer and a cache is that a buffer may hold only the existing copy of a data item, while cache, by definition, holds a copy on faster storage of an item that resides elsewhere. spooling and device reservation –
a spool is a buffer that holds the output of a device, such as a printer that cannot accept interleaved data streams. although a printer can serve only one job at a time, several applications may wish to print their output concurrently, without having their output mixes together. the os solves this problem by preventing all output continuing to the printer. the output of all applications is spooled in a separate disk file. when an application finishes printing then the spooling system queues the corresponding spool file for output to the printer. error handling –
an os that uses protected memory can guard against many kinds of hardware and application errors, so that a complete system failure is not the usual result of each minor mechanical glitch, devices, and i/o transfers can fail in many ways, either for transient reasons, as when a network becomes overloaded or for permanent reasons, as when a disk controller becomes defective. i/o protection –
errors and the issue of protection are closely related. a user process may attempt to issue illegal i/o instructions to disrupt the normal function of a system. we can use the various mechanisms to ensure that such disruption cannot take place in the system. to prevent illegal i/o access, we define all i/o instructions to be privileged instructions. the user cannot issue i/o instruction directly.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : shreyashagrawal, singhpriansh
apart from microkernel, monolithic kernel is another classification of kernel. like microkernel this one also manages system resources between application and hardware, but user services and kernel services are implemented under same address space. it increases the size of the kernel, thus increases size of operating system as well.
this kernel provides cpu scheduling, memory management, file management and other operating system functions through system calls. as both services are implemented under same address space, this makes operating system execution faster.
below is the diagrammatic representation of monolithic kernel:
if any service fails the entire system crashes, and it is one of the drawbacks of this kernel. the entire operating system needs modification if user adds a new service.
advantages of monolithic kernel –
one of the major advantage of having monolithic kernel is that it provides cpu scheduling, memory management, file management and other operating system functions through system calls.
the other one is that it is a single large process running entirely in a single address space.
it is a single static binary file. example of some monolithic kernel based oss are: unix, linux, open vms, xts-, z/tpf.
disadvantages of monolithic kernel –
one of the major disadvantage of monolithic kernel is that, if anyone service fails it leads to entire system failure.
if user has to add any new service. user needs to modify entire operating system.
key differences between monolithic kernel and microkernel –
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
types of system calls : there are  different categories of system calls –
examples of windows and unix system calls –
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please improve this article if you find anything incorrect by clicking on the "improve article" button below.
the getrlimit() and setrlimit() system calls can be used to get and set the resource limits such as files, cpu, memory etc. associated with a process.
each resource has an associated soft and hard limit. soft limit: the soft limit is the actual limit enforced by the kernel for the corresponding resource.
the soft limit is the actual limit enforced by the kernel for the corresponding resource. hard limit: the hard limit acts as a ceiling for the soft limit. the soft limit ranges in between  and hard limit.
the two limits are defined by the following structure
the signatures of the system calls are
resource refers to the resource limits you want to retrieve or modify.
to set both the limits, set the values with the new values to the elements of rlimit structure.
to get both the limits, pass the address of rlim. successful call to getrlimit(), sets the rlimit elements to the limits.
on success, both return . on error, - is returned, and errno is set appropriately.
here, is a program demonstrating the system calls by changing the value one greater than the maximum file descriptor number to .
" , old_lim.rlim_cur, old_lim.rlim_max); else fprintf (stderr, "%s
" , strerror ( errno )); lim.rlim_cur = ; lim.rlim_max = ; if (setrlimit(rlimit_nofile, &lim) == -) fprintf (stderr, "%s
" , strerror ( errno )); if ( getrlimit(rlimit_nofile, &new_lim) == ) printf ( "new limits -> soft limit= %ld " "\t hard limit= %ld
" , new_lim.rlim_cur, new_lim.rlim_max); else fprintf (stderr, "%s
" , strerror ( errno )); return ; }
output:
old limits -> soft limit=  hard limit=  new limits -> soft limit=  hard limit= 
the old limits values may vary depending upon the system.
now, if you try to open a new file, it will show run time error, because maximum  files can be opened and that are already being opened by the system too many open filesman  prlimit
there is another system callthat combines both the system calls.for more details, check manual by typing
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
an error in one program can adversely affect many processes, it might modify data of another program, or also can affect the operating system. for example, if a process stuck in infinite loop then this infinite loop could affect correct operation of other processes. so to ensure the proper execution of the operating system, there are two modes of operation:
user mode –
when the computer system run user applications like creating a text document or using any application program, then the system is in the user mode. when the user application requests for a service from the operating system or an interrupt occurs or system call, then there will be a transition from user to kernel mode to fulfill the requests.
note: to switch from kernel mode to user mode, mode bit should be .
given below image describes what happen when an interrupt occurs:
kernel mode –
when the system boots, hardware starts in kernel mode and when operating system is loaded, it start user application in user mode. to provide protection to the hardware, we have privileged instructions which execute only in kernel mode. if user attempt to run privileged instruction in user mode then it will treat instruction as illegal and traps to os. some of the privileged instructions are:
handling interrupts to switch from user mode to kernel mode. input-output management.
note: to switch from user mode to kernel mode mode bit should be .
read next – user level thread vs kernel level thread
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : midhunxda
in any operating system, it is necessary to have dual mode operation to ensure protection and security of the system from unauthorized or errant users . this dual mode separates the user mode from the system mode or kernel mode.
what are privileged instructions?
the instructions that can run only in kernel mode are called privileged instructions .
privileged instructions possess the following characteristics :
(i) if any attempt is made to execute a privileged instruction in user mode, then it will not be executed and treated as an illegal instruction. the hardware traps it to the operating system.
(ii) before transferring the control to any user program, it is the responsibility of the operating system to ensure that the timer is set to interrupt. thus, if the timer interrupts then the operating system regains the control.
thus, any instruction which can modify the contents of the timer is a privileged instruction.
(iii) privileged instructions are used by the operating system in order to achieve correct operation.
(iv) various examples of privileged instructions include:
i/o instructions and halt instructions
turn off all interrupts
set the timer
context switching
clear the memory or remove a process from the memory
modify entries in device-status table
what are non-privileged instructions?
the instructions that can run only in user mode are called non-privileged instructions .
various examples of non-privileged instructions include:
reading the status of processor
reading the system time
generate any trap instruction
sending the final prinout of printer
also, it is important to note that in order to change the mode from privileged to non-privileged, we require a non-privileged instruction that does not generate any interrupt.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : vaibhavrai
program vs process
a process is an ‘active’ entity, as opposed to a program, which is considered to be a ‘passive’ entity. a single program can create many processes when run multiple times; for example, when we open a .exe or binary file multiple times, multiple instances begin (multiple processes are created).
what does a process look like in memory?
text section:a process, sometimes known as the text section, also includes the current activity represented by the value of the program counter.
stack: the stack contains the temporary data, such as function parameters, returns addresses, and local variables.
data section: contains the global variable.
heap section: dynamically allocated memory to process during its run time.
refer this for more details on sections.
attributes or characteristics of a process
a process has following attributes.
. process id: a unique identifier assigned by the operating system . process state: can be ready, running, etc. . cpu registers: like the program counter (cpu registers must be saved and restored when a process is swapped in and out of cpu) . accounts information: . i/o status information: for example, devices allocated to the process, open files, etc . cpu scheduling information: for example, priority (different processes may have different priorities, for example a short process may be assigned a low priority in the shortest job first scheduling)
all of the above attributes of a process are also known as thecontext of the process.
every process has its own process control block(pcb), i.e each process will have a unique pcb. all of the above attributes are part of the pcb.
states of process:
a process is in one of the following states:
. new: newly created process (or) being-created process. . ready: after creation process moves to ready state, i.e. the process is ready for execution. . run: currently running process in cpu (only one process at a time can be under execution in a single processor). . wait (or block): when a process requests i/o access. . complete (or terminated): the process completed its execution. . suspended ready: when the ready queue becomes full, some processes are moved to suspended ready state . suspended block: when waiting queue becomes full.
context switching
the process of saving the context of one process and loading the context of another process is known as context switching. in simple terms, it is like loading and unloading the process from running state to ready state.
when does context switching happen?
. when a high-priority process comes to ready state (i.e. with higher priority than the running process)
. an interrupt occurs
. user and kernel mode switch (it is not necessary though)
. preemptive cpu scheduling used.
context switch vs mode switch
a mode switch occurs when cpu privilege level is changed, for example when a system call is made or a fault occurs. the kernel works in more a privileged mode than a standard user task. if a user process wants to access things which are only accessible to the kernel, a mode switch must occur. the currently executing process need not be changed during a mode switch.
a mode switch typically occurs for a process context switch to occur. only the kernel can cause a context switch.
cpu-bound vs i/o-bound processes:
a cpu-bound process requires more cpu time or spends more time in the running state.
an i/o-bound process requires more i/o time and less cpu time. an i/o-bound process spends more time in the waiting state.
exercise:
. which of the following need not necessarily be saved on a context switch between processes? (gate-cs-)
(a) general purpose registers
(b) translation lookaside buffer
(c) program counter
(d) all of the above
answer (b)
explanation:
in a process context switch, the state of the first process must be saved somehow, so that when the scheduler gets back to the execution of the first process, it can restore this state and continue. the state of the process includes all the registers that the process may be using, especially the program counter, plus any other operating system-specific data that may be necessary. a translation look-aside buffer (tlb) is a cpu cache that memory management hardware uses to improve virtual address translation speed. a tlb has a fixed number of slots that contain page table entries, which map virtual addresses to physical addresses. on a context switch, some tlb entries can become invalid, since the virtual-to-physical mapping is different. the simplest strategy to deal with this is to completely flush the tlb.
. the time taken to switch between user and kernel modes of execution is t while the time taken to switch between two processes is t. which of the following is true? (gate-cs-)
(d) nothing can be said about the relation between t and t.
answer: (c)
explanation: process switching involves mode switch. context switching can occur only in kernel mode.
quiz on process management
prerequisite – introduction, process scheduler
states of a process are as following:
new (create) – in this step, the process is about to be created but not yet created, it is the program which is present in secondary memory that will be picked up by os to create the process.
in this step, the process is about to be created but not yet created, it is the program which is present in secondary memory that will be picked up by os to create the process. ready – new -> ready to run. after the creation of a process, the process enters the ready state i.e. the process is loaded into the main memory. the process here is ready to run and is waiting to get the cpu time for its execution. processes that are ready for execution by the cpu are maintained in a queue for ready processes.
new -> ready to run. after the creation of a process, the process enters the ready state i.e. the process is loaded into the main memory. the process here is ready to run and is waiting to get the cpu time for its execution. processes that are ready for execution by the cpu are maintained in a queue for ready processes. run – the process is chosen by cpu for execution and the instructions within the process are executed by any one of the available cpu cores.
the process is chosen by cpu for execution and the instructions within the process are executed by any one of the available cpu cores. blocked or wait – whenever the process requests access to i/o or needs input from the user or needs access to a critical region(the lock for which is already acquired) it enters the blocked or wait state. the process continues to wait in the main memory and does not require cpu. once the i/o operation is completed the process goes to the ready state.
whenever the process requests access to i/o or needs input from the user or needs access to a critical region(the lock for which is already acquired) it enters the blocked or wait state. the process continues to wait in the main memory and does not require cpu. once the i/o operation is completed the process goes to the ready state. terminated or completed – process is killed as well as pcb is deleted.
process is killed as well as pcb is deleted. suspend ready – process that was initially in the ready state but were swapped out of main memory(refer virtual memory topic) and placed onto external storage by scheduler are said to be in suspend ready state. the process will transition back to ready state whenever the process is again brought onto the main memory.
process that was initially in the ready state but were swapped out of main memory(refer virtual memory topic) and placed onto external storage by scheduler are said to be in suspend ready state. the process will transition back to ready state whenever the process is again brought onto the main memory. suspend wait or suspend blocked – similar to suspend ready but uses the process which was performing i/o operation and lack of main memory caused them to move to secondary memory.
when work is finished it may go to suspend ready.
cpu and io bound processes:
if the process is intensive in terms of cpu operations then it is called cpu bound process. similarly, if the process is intensive in terms of i/o operations then it is called io bound process.
types of schedulers:
long term – performance – makes a decision about how many processes should be made to stay in the ready state, this decides the degree of multiprogramming. once a decision is taken it lasts for a long time hence called long term scheduler. short term – context switching time – short term scheduler will decide which process to be executed next and then it will call dispatcher. a dispatcher is a software that moves process from ready to run and vice versa. in other words, it is context switching. medium term – swapping time – suspension decision is taken by medium term scheduler. medium term scheduler is used for swapping that is moving the process from main memory to secondary and vice versa.
multiprogramming – we have many processes ready to run. there are two types of multiprogramming:
pre-emption – process is forcefully removed from cpu. pre-emption is also called as time sharing or multitasking. non pre-emption – processes are not removed until they complete the execution.
degree of multiprogramming –
the number of processes that can reside in the ready state at maximum decides the degree of multiprogramming, e.g., if the degree of programming = , this means  processes can reside in the ready state at maximum.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : kbapat, nitishanon, shreyashagrawal
while creating a process the operating system performs several operations. to identify the processes, it assigns a process identification number (pid) to each process. as the operating system supports multi-programming, it needs to keep track of all the processes. for this task, the process control block (pcb) is used to track the process’s execution status. each block of memory contains information about the process state, program counter, stack pointer, status of opened files, scheduling algorithms, etc. all these information is required and must be saved when the process is switched from one state to another. when the process makes a transition from one state to another, the operating system must update information in the process’s pcb.
a process control block (pcb) contains information about the process, i.e. registers, quantum, priority, etc. the process table is an array of pcb’s, that means logically contains a pcb for all of the current processes in the system.
pointer – it is a stack pointer which is required to be saved when the process is switched from one state to another to retain the current position of the process.
it is a stack pointer which is required to be saved when the process is switched from one state to another to retain the current position of the process. process state – it stores the respective state of the process.
it stores the respective state of the process. process number – every process is assigned with a unique id known as process id or pid which stores the process identifier.
every process is assigned with a unique id known as process id or pid which stores the process identifier. program counter – it stores the counter which contains the address of the next instruction that is to be executed for the process.
it stores the counter which contains the address of the next instruction that is to be executed for the process. register – these are the cpu registers which includes: accumulator, base, registers and general purpose registers.
these are the cpu registers which includes: accumulator, base, registers and general purpose registers. memory limits – this field contains the information about memory management system used by operating system. this may include the page tables, segment tables etc.
this field contains the information about memory management system used by operating system. this may include the page tables, segment tables etc. open files list – this information includes the list of files opened for a process.
miscellaneous accounting and status data – this field includes information about the amount of cpu used, time constraints, jobs or process number, etc.
the process control block stores the register content also known as execution content of the processor when it was blocked from running. this execution content architecture enables the operating system to restore a process’s execution context when the process returns to the running state. when the process makes a transition from one state to another, the operating system updates its information in the process’s pcb. the operating system maintains pointers to each process’s pcb in a process table so that it can access the pcb quickly.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
this article is contributed by rajshree srivastava. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : magbene, caogdue
there are three types of process scheduler.
long term or job scheduler :
it brings the new process to the ‘ready state’. it controls degree of multi-programming, i.e., number of process present in ready state at any point of time. it is important that the long-term scheduler make a careful selection of both io and cpu bound process. io bound tasks are which use much of their time in input and output operations while cpu bound processes are which spend their time on cpu. the job scheduler increases efficiency by maintaining a balance between the two. short term or cpu scheduler :
it is responsible for selecting one process from ready state for scheduling it on the running state. note: short-term scheduler only selects the process to schedule it doesn’t load the process on running. here is when all the scheduling algorithms are used. the cpu scheduler is responsible for ensuring there is no starvation owing to high burst time processes.
dispatcher is responsible for loading the process selected by short-term scheduler on the cpu (ready to running state) context switching is done by dispatcher only. a dispatcher does the following: switching context. switching to user mode. jumping to the proper location in the newly loaded program. medium-term scheduler :
it is responsible for suspending and resuming the process. it mainly does swapping (moving processes from main memory to disk and vice versa). swapping may be necessary to improve the process mix or because a change in memory requirements has overcommitted available memory, requiring memory to be freed up. it is helpful in maintaining a perfect balance between the i/o bound and the cpu bound. it reduces the degree of multiprogramming.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : magbene, vaibhavrai, jyotibisht
scheduling of processes/work is done to finish the work on time.
below are different time with respect to a process.
arrival time: time at which the process arrives in the ready queue.
completion time: time at which process completes its execution.
burst time: time required by a process for cpu execution.
turn around time: time difference between completion time and arrival time.
turn around time = completion time – arrival time waiting time(w.t): time difference between turn around time and burst time.
waiting time = turn around time – burst time
why do we need scheduling?
a typical process involves both i/o time and cpu time. in a uni programming system like ms-dos, time spent waiting for i/o is wasted and cpu is free during this time. in multi programming systems, one process can use cpu while another is waiting for i/o. this is possible only with process scheduling.
objectives of process scheduling algorithm
max cpu utilization [keep cpu as busy as possible]
fair allocation of cpu.
max throughput [number of processes that complete their execution per time unit]
min turnaround time [time taken by a process to finish execution]
min waiting time [time a process waits in ready queue]
min response time [time when a process produces first response]
different scheduling algorithms
first come first serve (fcfs): simplest scheduling algorithm that schedules according to arrival times of processes. first come first serve scheduling algorithm states that the process that requests the cpu first is allocated the cpu first. it is implemented by using the fifo queue. when a process enters the ready queue, its pcb is linked onto the tail of the queue. when the cpu is free, it is allocated to the process at the head of the queue. the running process is then removed from the queue. fcfs is a non-preemptive scheduling algorithm.
note:first come first serve suffers from convoy effect.
shortest job first (sjf): process which have the shortest burst time are scheduled first.if two processes have the same bust time then fcfs is used to break the tie. it is a non-preemptive scheduling algorithm.
longest job first (ljf): it is similar to sjf scheduling algorithm. but, in this scheduling algorithm, we give priority to the process having the longest burst time. this is non-preemptive in nature i.e., when any process starts executing, can’t be interrupted before complete execution.
shortest remaining time first (srtf): it is preemptive mode of sjf algorithm in which jobs are schedule according to shortest remaining time.
longest remaining time first (lrtf): it is preemptive mode of ljf algorithm in which we give priority to the process having largest burst time remaining.
round robin scheduling: each process is assigned a fixed time(time quantum/time slice) in cyclic way.it is designed especially for the time-sharing system. the ready queue is treated as a circular queue. the cpu scheduler goes around the ready queue, allocating the cpu to each process for a time interval of up to -time quantum. to implement round robin scheduling, we keep the ready queue as a fifo queue of processes. new processes are added to the tail of the ready queue. the cpu scheduler picks the first process from the ready queue, sets a timer to interrupt after -time quantum, and dispatches the process. one of two things will then happen. the process may have a cpu burst of less than -time quantum. in this case, the process itself will release the cpu voluntarily. the scheduler will then proceed to the next process in the ready queue. otherwise, if the cpu burst of the currently running process is longer than -time quantum, the timer will go off and will cause an interrupt to the operating system. a context switch will be executed, and the process will be put at the tail of the ready queue. the cpu scheduler will then select the next process in the ready queue.
priority based scheduling (non-preemptive): in this scheduling, processes are scheduled according to their priorities, i.e., highest priority process is scheduled first. if priorities of two processes match, then schedule according to arrival time. here starvation of process is possible.
highest response ratio next (hrrn): in this scheduling, processes with highest response ratio is scheduled. this algorithm avoids starvation.
response ratio = (waiting time + burst time) / burst time
multilevel queue scheduling: according to the priority of process, processes are placed in the different queues. generally high priority process are placed in the top level queue. only after completion of processes from top level queue, lower level queued processes are scheduled. it can suffer from starvation.
multi level feedback queue scheduling: it allows the process to move in between queues. the idea is to separate processes according to the characteristics of their cpu bursts. if a process uses too much cpu time, it is moved to a lower-priority queue.
some useful facts about scheduling algorithms:
fcfs can cause long waiting times, especially when the first job takes too much cpu time. both sjf and shortest remaining time first algorithms may cause starvation. consider a situation when the long process is there in the ready queue and shorter processes keep coming. if time quantum for round robin scheduling is very large, then it behaves same as fcfs scheduling. sjf is optimal in terms of average waiting time for a given set of processes,i.e., average waiting time is minimum with this scheduling, but problems are, how to know/predict the time of next job.
exercise:
consider a system which requires -time units of burst time. the multilevel feedback queue scheduling is used and time quantum is  unit for the top queue and is incremented by  unit at each level, then in what queue the process will terminate the execution?
which of the following is false about sjf?
s: it causes minimum average waiting time
s: it can cause starvation
(a) only s
(b) only s
(c) both s and s
(d) neither s nor s
answer (d)
s is true sjf will always give minimum average waiting time.
s is true sjf can cause starvation.
consider the following table of arrival time and burst time for three processes p, p and p. (gate-cs-) process arrival time burst time p  ms  ms p  ms  ms p  ms  ms the pre-emptive shortest job first scheduling algorithm is used. scheduling is carried out only at arrival or completion of processes. what is the average waiting time for the three processes?
process p is allocated processor at  ms as there is no other process in the ready queue. p is preempted after  ms as p arrives at  ms and burst time for p is less than remaining time of p. p runs for ms. p arrived at  ms but p continued as burst time of p is longer than p. after p completes, p is scheduled again as the remaining time for p is less than the burst time of p.
p waits for  ms, p waits for  ms and p waits for  ms. so average waiting time is (++)/ = .
consider the following set of processes, with the arrival times and the cpu-burst times given in milliseconds (gate-cs-) process arrival time burst time p   p   p   p   what is the average turnaround time for these processes with the preemptive shortest remaining processing time first (srpt) algorithm ?
the following is gantt chart of execution p p p p p      turn around time = completion time – arrival time
avg turn around time = ( +  + + )/ = .
an operating system uses the shortest remaining time first (srtf) process scheduling algorithm. consider the arrival times and execution times for the following processes: process execution time arrival time p   p   p   p   what is the total waiting time for process p?
at time , p is the only process, p runs for  time units.
at time , p arrives, but p has the shortest remaining time. so p continues for  more time units.
at time , p is the only process. so it runs for  time units
at time , p is the shortest remaining time process. so it runs for  time units
at time , p runs as it is the only process. p runs for  time units.
at time , p arrives, but p has the shortest remaining time. so p continues for  more time units.
p completes its execution at time  total waiting time for p = completion time - (arrival time + execution time) =  - ( + ) = 
please refer quiz on cpu scheduling for more questions.
prerequisite – cpu scheduling
. preemptive scheduling:
preemptive scheduling is used when a process switches from running state to ready state or from waiting state to ready state. the resources (mainly cpu cycles) are allocated to the process for the limited amount of time and then is taken away, and the process is again placed back in the ready queue if that process still has cpu burst time remaining. that process stays in ready queue till it gets next chance to execute.
algorithms based on preemptive scheduling are: round robin (rr),shortest remaining time first (srtf), priority (preemptive version), etc.
. non-preemptive scheduling:
non-preemptive scheduling is used when a process terminates, or a process switches from running to waiting state. in this scheduling, once the resources (cpu cycles) is allocated to a process, the process holds the cpu till it gets terminated or it reaches a waiting state. in case of non-preemptive scheduling does not interrupt a process running cpu in middle of the execution. instead, it waits till the process complete its cpu burst time and then it can allocate the cpu to another process.
algorithms based on non-preemptive scheduling are: shortest job first (sjf basically non preemptive) and priority (non preemptive version), etc.
key differences between preemptive and non-preemptive scheduling:
in preemptive scheduling the cpu is allocated to the processes for the limited time whereas in non-preemptive scheduling, the cpu is allocated to the process till it terminates or switches to waiting state.
the executing process in preemptive scheduling is interrupted in the middle of execution when higher priority one comes whereas, the executing process in non-preemptive scheduling is not interrupted in the middle of execution and wait till its execution.
in preemptive scheduling, there is the overhead of switching the process from ready state to running state, vise-verse, and maintaining the ready queue. whereas in case of non-preemptive scheduling has no overhead of switching the process from running state to ready state.
in preemptive scheduling, if a high priority process frequently arrives in the ready queue then the process with low priority has to wait for a long, and it may have to starve. on the other hands, in the non-preemptive scheduling, if cpu is allocated to the process having larger burst time then the processes with small burst time may have to starve.
preemptive scheduling attain flexible by allowing the critical processes to access cpu as they arrive into the ready queue, no matter what process is executing currently. non-preemptive scheduling is called rigid as even if a critical process enters the ready queue the process running cpu is not disturbed.
the preemptive scheduling has to maintain the integrity of shared data that’s why it is cost associative as it which is not the case with non-preemptive scheduling.
comparison chart:
paramenter preemptive scheduling non-preemptive scheduling basic in this resources(cpu cycle) are allocated to a process for a limited time. once resources(cpu cycle) are allocated to a process, the process holds it till it completes its burst time or switches to waiting state. interrupt process can be interrupted in between. process can not be interrupted until it terminates itself or its time is up. starvation if a process having high priority frequently arrives in the ready queue, low priority process may starve. if a process with long burst time is running cpu, then later coming process with less cpu burst time may starve. overhead it has overheads of scheduling the processes. it does not have overheads. flexibility flexible rigid cost cost associated no cost associated cpu utilization in preemptive scheduling, cpu utilization is high. it is low in non preemptive scheduling. examples examples of preemptive scheduling are round robin and shortest remaining time first. examples of non-preemptive scheduling are first come first serve and shortest job first.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : niharika, ashushrma, kajalshekhawat
a context switch is the time spent between two processes (i.e., bringing a waiting process into execution and sending an executing process into waiting state). this happens in multitasking.the operating system must bring the state information if waiting process into memory and save the state information of the currently running process.
in order to solve this problem, we would like to record the timestamps of the first and last instruction of the swapping processes.the context switch time is the difference between the two processes.
let’s take an example: assume there are only two processes, p and p.
p is executing and p is waiting for execution. at some point, the operating system must swap p and p, let’s assume it happens at the nth instruction of p. if t(x, k) indicates the timestamp in microseconds of the kth instruction of process x, then the context switch would take t(, ) – t(, n).
another issue is that swapping is governed by the scheduling algorithm of the operating system and there may be many kernel level threads which are also doing context switches. other processes could be contending for the cpu or the kernel handling interrupts. the user does not have any control over these extraneous context switches. for instance, if at time t(, n) the kernel decides to handle an interrupt, then the context switch time would be overstated.
in order to avoid these obstacles, we must construct an environment such that after p executes, the task scheduler immediately selects p to run. this may be accomplished by constructing a data channel, such as a pipe between p and p.
that is, let’s allow p to be the initial sender and p be the receiver. initially, p is blocked(sleeping) as it awaits the data token. when p executes, it delivers the data token over the data channel to p and immediately attempts to read the response token. a context switch results and the task scheduler must selects another process to run.since p is now in a ready-to-run state, it is a desirable candidate to be selected by the task scheduler for execution.when p runs, the role of p and p are swapped. p is now acting as the sender and p as the blocked receiver.
to summaries –
p blocks awaiting data from p p marks the starting time. p sends token to p. p attempts to read a response token from p. this induces a context switch. p is scheduled and receives the token. p sends a response token to p. p attempts read a response token from p. this induces a context switch. p is scheduled and receives the token. p marks the end time.
the key is that the delivery of a data token induces a context switch. let td and tr be the time it takes to deliver and receive a data token, respectively, and let tc be the amount of time spent in a context switch. at step , p records the timestamp of the delivery of the token, and at step , it records the timestamp of the response. the amount of time elapsed, t, between these events may be expressed by:
t =  * (td + tc + tr)
this formula arises because of the following events:
p sends the token ()
cpu context switches ()
p receives it ()
p then sends the response token ()
cpu context switches ()
and finally, p receives it ()
gate cs practice questions –
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
brahmanisai studying in national institute of technology raipur computer science interested in diving into computer languages placed in capgemini
schedulers are special system software which handle process scheduling in various ways. their main task is to select the jobs to be submitted into the system and to decide which process to run. there are three types of scheduler:
long term (job) scheduler – due to the smaller size of main memory initially all program are stored in secondary memory. when they are stored or loaded in the main memory they are called process. this is the decision of long term scheduler that how many processes will stay in the ready queue. hence, in simple words, long term scheduler decides the degree of multi-programming of system. medium term scheduler – most often, a running process needs i/o operation which doesn’t requires cpu. hence during the execution of a process when a i/o operation is required then the operating system sends that process from running queue to blocked queue. when a process completes its i/o operation then it should again be shifted to ready queue. all these decisions are taken by the medium-term scheduler. medium-term scheduling is a part of swapping. short term (cpu) scheduler – when there are lots of processes in main memory initially all are present in the ready queue. among all of the process, a single process is to be selected for execution. this decision is handled by short term scheduler. let’s have a look at the figure given below. it may make a more clear view for you.
dispatcher –
a dispatcher is a special program which comes into play after the scheduler. when the scheduler completes its job of selecting a process, it is the dispatcher which takes that process to the desired state/queue. the dispatcher is the module that gives a process control over the cpu after it has been selected by the short-term scheduler. this function involves the following:
switching context
switching to user mode
jumping to the proper location in the user program to restart that program
the difference between the scheduler and dispatcher –
consider a situation, where various processes are residing in the ready queue waiting to be executed. the cpu cannot execute all of these processes simultaneously, so the operating system has to choose a particular process on the basis of the scheduling algorithm used. so, this procedure of selecting a process among various processes is done by the scheduler. once the scheduler has selected a process from the queue, the dispatcher comes into the picture, and it is the dispatcher who takes that process from the ready queue and moves it into the running state. therefore, the scheduler gives the dispatcher an ordered list of processes which the dispatcher moves to the cpu over time.
example –
there are  processes in the ready queue, p, p, p, p; their arrival times are t, t, t, t respectively. a first in first out (fifo) scheduling algorithm is used. because p arrived first, the scheduler will decide it is the first process that should be executed, and the dispatcher will remove p from the ready queue and give it to the cpu. the scheduler will then determine p to be the next process that should be executed, so when the dispatcher returns to the queue for a new process, it will take p and give it to the cpu. this continues in the same way for p, and then p.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : vaibhavrai, bentrono, adrian_monk
given n processes with their burst times, the task is to find average waiting time and average turn around time using fcfs scheduling algorithm.
first in, first out (fifo), also known as first come, first served (fcfs), is the simplest scheduling algorithm. fifo simply queues processes in the order that they arrive in the ready queue.
in this, the process that comes first will be executed first and next process starts only after the previous gets fully executed.
here we are considering that arrival time for all processes is .
how to compute below times in round robin using a program?
completion time: time at which process completes its execution. turn around time: time difference between completion time and arrival time. turn around time = completion time – arrival time waiting time(w.t): time difference between turn around time and burst time.
waiting time = turn around time – burst time
in this post, we have assumed arrival times as , so turn around and completion times are same.
implementation:
- input the processes along with their burst time (bt). - find waiting time (wt) for all processes. - as first process that comes need not to wait so waiting time for process  will be  i.e. wt[] = . - find waiting time for all other processes i.e. for process i -> wt[i] = bt[i-] + wt[i-] . - find turnaround time = waiting_time + burst_time for all processes. - find average waiting time = total_waiting_time / no_of_processes. - similarly, find average turnaround time = total_turn_around_time / no_of_processes.
" ; for ( int i=; i<n; i++) { total_wt = total_wt + wt[i]; total_tat = total_tat + tat[i]; cout << " " << i+ << "\t\t" << bt[i] << "\t " << wt[i] << "\t\t " << tat[i] <<endl; } cout << "average waiting time = " << ( float )total_wt / ( float )n; cout << "
" ); for ( int i=; i<n; i++) { total_wt = total_wt + wt[i]; total_tat = total_tat + tat[i]; printf ( " %d " ,(i+)); printf ( " %d " , bt[i] ); printf ( " %d" ,wt[i] ); printf ( " %d
" ,tat[i] ); } int s=( float )total_wt / ( float )n; int t=( float )total_tat / ( float )n; printf ( "average waiting time = %d" ,s); printf ( "
" ); for ( int i =  ; i < n; i++) { total_wt = total_wt + wt[i]; total_tat = total_tat + tat[i]; system.out.printf( " %d " , (i +  )); system.out.printf( " %d " , bt[i]); system.out.printf( " %d" , wt[i]); system.out.printf( " %d
" , tat[i]); } float s = ( float )total_wt /( float ) n; int t = total_tat / n; system.out.printf( "average waiting time = %f" , s); system.out.printf( "
" ); for ( int i = ; i < n; i++) { total_wt = total_wt + wt[i]; total_tat = total_tat + tat[i]; console.write( " {} " , (i + )); console.write( " {} " , bt[i]); console.write( " {}" , wt[i]); console.write( " {}
" , tat[i]); } float s = ( float )total_wt /( float ) n; int t = total_tat / n; console.write( "average waiting time = {}" , s); console.write( "
processes burst time waiting time turn around time             average waiting time = . average turn around time = 
important points:
we have already discussed fcfs scheduling of processes with same arrival time. in this post, scenario when processes have different arrival times are discussed. given n processes with their burst times and arrival times, the task is to find average waiting time and average turn around time using fcfs scheduling algorithm.
fifo simply queues processes in the order they arrive in the ready queue. here, the process that comes first will be executed first and next process will start only after the previous gets fully executed.
completion time: time at which process completes its execution. turn around time: time difference between completion time and arrival time. turn around time = completion time – arrival time waiting time(w.t): time difference between turn around time and burst time.
waiting time = turn around time – burst time
process wait time : service time - arrival time p  -  =  p  -  =  p  -  =  p  -  =  average wait time: ( +  +  + ) /  = .
service time : service time means amount of time after which a process can start execution. it is summation of burst time of previous processes (processes that came before)
to find waiting time: time taken by all processes before the current process to be started (i.e. burst time of all previous processes) – arrival time of current process
processes burst time arrival time waiting time turn-around time completion time                   average waiting time = . average turn around time = .
this article is contributed by sahil chhabra (akku). if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
prerequisites : basics of fcfs scheduling (program for fcfs scheduling | set , program for fcfs scheduling | set  )
convoy effect is phenomenon associated with the first come first serve (fcfs) algorithm, in which the whole operating system slows down due to few slow processes.
fcfs algorithm is non-preemptive in nature, that is, once cpu time has been allocated to a process, other processes can get cpu time only after the current process has finished. this property of fcfs scheduling leads to the situation called convoy effect.
suppose there is one cpu intensive (large burst time) process in the ready queue, and several other processes with relatively less burst times but are input/output (i/o) bound (need i/o operations frequently).
steps are as following below:
the i/o bound processes are first allocated cpu time. as they are less cpu intensive, they quickly get executed and goto i/o queues.
now, the cpu intensive process is allocated cpu time. as its burst time is high, it takes time to complete.
while the cpu intensive process is being executed, the i/o bound processes complete their i/o operations and are moved back to ready queue.
however, the i/o bound processes are made to wait as the cpu intensive process still hasn’t finished. this leads to i/o devices being idle.
when the cpu intensive process gets over, it is sent to the i/o queue so that it can access an i/o device.
meanwhile, the i/o bound processes get their required cpu time and move back to i/o queue.
however, they are made to wait because the cpu intensive process is still accessing an i/o device. as a result, the cpu is sitting idle now.
hence in convoy effect, one slow process slows down the performance of the entire set of processes, and leads to wastage of cpu time and other devices.
to avoid convoy effect, preemptive scheduling algorithms like round robin scheduling can be used – as the smaller processes don’t have to wait much for cpu time – making their execution faster and leading to less resources sitting idle.
prerequisite – page replacement algorithms
in operating system, process data is loaded in fixed sized chunks and each chunk is referred to as a page. the processor loads these pages in the fixed sized chunks of memory called frames. typically the size of each page is always equal to the frame size.
a page fault occurs when a page is not found in the memory, and needs to be loaded from the disk. if a page fault occurs and all memory frames have been already allocated, then replacement of a page in memory is required on the request of a new page. this is referred to as demand-paging. the choice of which page to replace is specified by a page replacement algorithms. the commonly used page replacement algorithms are fifo, lru, optimal page replacement algorithms etc.
generally, on increasing the number of frames to a process’ virtual memory, its execution becomes faster as less number of page faults occur. sometimes the reverse happens, i.e. more number of page faults occur when more frames are allocated to a process. this most unexpected result is termed as belady’s anomaly.
bélády’s anomaly is the name given to the phenomenon where increasing the number of page frames results in an increase in the number of page faults for a given memory access pattern.
this phenomenon is commonly experienced in following page replacement algorithms:
first in first out (fifo) second chance algorithm random page replacement algorithm
reason of belady’s anomaly –
the other two commonly used page replacement algorithms are optimal and lru, but belady’s anamoly can never occur in these algorithms for any reference string as they belong to a class of stack based page replacement algorithms.
a stack based algorithm is one for which it can be shown that the set of pages in memory for n frames is always a subset of the set of pages that would be in memory with n +  frames. for lru replacement, the set of pages in memory would be the n most recently referenced pages. if the number of frames increases then these n pages will still be the most recently referenced and so, will still be in the memory. while in fifo, if a page named b came into physical memory before a page – a then priority of replacement of b is greater than that of a, but this is not independent of the number of page frames and hence, fifo does not follow a stack page replacement policy and therefore suffers belady’s anomaly.
example: consider the following diagram to understand the behaviour of a stack-based page replacement algorithm
the diagram illustrates that given the set of pages i.e. {, , } in  frames of memory is not a subset of the pages in memory – {, , , } with  frames and it is a violation in the property of stack based algorithms. this situation can be frequently seen in fifo algorithm.
belady’s anomaly in fifo –
assuming a system that has no pages loaded in the memory and uses the fifo page replacement algorithm. consider the following reference string:
case-: if the system has  frames, the given reference string on using fifo page replacement algorithm yields a total of  page faults. the diagram below illustrates the pattern of the page faults occurring in the example.
case-: if the system has  frames, the given reference string on using fifo page replacement algorithm yields a total of  page faults. the diagram below illustrates the pattern of the page faults occurring in the example.
it can be seen from the above example that on increasing the number of frames while using the fifo page replacement algorithm, the number of page faults increased from  to .
note – it is not necessary that every string reference pattern cause belady anomaly in fifo but there are certain kind of string 
shortest job first (sjf) or shortest job next, is a scheduling policy that selects the waiting process with the smallest execution time to execute next. sjn is a non-preemptive algorithm.
shortest job first has the advantage of having a minimum average waiting time among all scheduling algorithms.
it is a greedy algorithm.
it may cause starvation if shorter processes keep coming. this problem can be solved using the concept of ageing.
it is practically infeasible as operating system may not know burst time and therefore may not sort them. while it is not possible to predict execution time, several methods can be used to estimate the execution time for a job, such as a weighted average of previous execution times. sjf can be used in specialized environments where accurate estimates of running time are available.
algorithm:
sort all the process according to the arrival time. then select that process which has minimum arrival time and minimum burst time. after completion of process make a pool of process which after till the completion of previous process and select that process among the pool which is having minimum burst time.
how to compute below times in sjf using a program?
completion time: time at which process completes its execution. turn around time: time difference between completion time and arrival time. turn around time = completion time – arrival time waiting time(w.t): time difference between turn around time and burst time.
waiting time = turn around time – burst time
in this post, we have assumed arrival times as , so turn around and completion times are same.
" ; for ( int i=; i<num; i++) { cout<< "...process " <<i+<< "...
" ; cout>mat[i][]; cout>mat[i][]; cout>mat[i][]; } cout<< "before arrange...
" ; cout<< "process id\tarrival time\tburst time
" ; for ( int i=; i<num; i++) { cout<<mat[i][]<< "\t\t" <<mat[i][]<< "\t\t" <<mat[i][]<< "
" ; } arrangearrival(num, mat); completiontime(num, mat); cout<< "final result...
" ; cout<< "process id\tarrival time\tburst time\twaiting time\tturnaround time
" ; for ( int i=; i<num; i++) { cout<<mat[i][]<< "\t\t" <<mat[i][]<< "\t\t" <<mat[i][]<< "\t\t" <<mat[i][]<< "\t\t" <<mat[i][]<< "
" , mat[i][  ], mat[i][  ], mat[i][  ]); } arrangearrival(num, mat); completiontime(num, mat); system.out.println( "final result..." ); system.out.println( "process id\tarrival time\tburst" + " time\twaiting time\tturnaround time" ); for ( int i =  ; i < num; i++) { system.out.printf( "%d\t\t%d\t\t%d\t\t%d\t\t%d
output:
process id arrival time burst time             final result... process id arrival time burst time waiting time turnaround time                    
in set- we will discuss the preemptive version of sjf i.e. shortest remaining time first
this article is contributed by mahesh kumar(nce, chandi). if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : anish, msujawal, sanjeev
in previous post, we have discussed set  of sjf i.e. non-preemptive. in this post we will discuss the preemptive version of sjf known as shortest remaining time first (srtf).
shortest remaining time first (srtf) scheduling
in the shortest remaining time first (srtf) scheduling algorithm, the process with the smallest amount of time remaining until completion is selected to execute. since the currently executing process is the one with the shortest amount of time remaining by definition, and since that time should only reduce as execution progresses, processes will always run until they complete or a new process is added that requires a smaller amount of time.
shortest remaining time first (preemptive sjf): example
process duration order arrival time p    p   
p waiting time: - = 
p waiting time: 
the average waiting time(awt): ( + ) /  = 
advantage:
- short processes are handled very quickly.
- the system also requires very little overhead since it only makes a decision when a process completes or a new process is added.
- when a new process is added the algorithm only needs to compare the currently executing process with the new process, ignoring all other processes currently waiting to execute.
disadvantage:
- like shortest job first, it has the potential for process starvation.
- long processes may be held off indefinitely if short processes are continually added.
source:wiki
implementation:
- traverse until all process gets completely executed. a) find process with minimum remaining time at every single time lap. b) reduce its time by . c) check if its remaining time becomes  d) increment the counter of process completion. e) completion time of current process = current_time +; e) calculate waiting time for each completed process. wt[i]= completion time - arrival_time-burst_time f)increment time lap by one. - find turnaround time (waiting_time+burst_time).
" ; for ( int i = ; i < n; i++) { total_wt = total_wt + wt[i]; total_tat = total_tat + tat[i]; cout << " " << proc[i].pid << "\t\t" << proc[i].bt << "\t\t " << wt[i] << "\t\t " << tat[i] << endl; } cout << "
average waiting time = " << ( float )total_wt / ( float )n; cout << "
processes burst time waiting time turn around time                 average waiting time = . average turn around time = .
this article is contributed by sahil chhabra. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : eagleateme, shubhamsingh, ajaykumar
prerequisite – cpu scheduling, sjf – set  (non- preemptive), set  (preemptive)
shortest job first (sjf) is an optimal scheduling algorithm as it gives maximum throughput and minimum average waiting time(wt) and turn around time (tat) but it is not practically implementable because burst-time of a process can’t be predicted in advance.
we may not know the length of the next cpu burst, but we may be able to predict its value. we expect the next cpu burst will be similar in length to the previous ones. by computing an approximation of the length of the next cpu burst, we can pick the process with the shortest predicted cpu burst.
there are two methods by which we can predict the burst time of the process :
. static method – we can predict the burst-time by two factors :
prerequisite – cpu scheduling | longest remaining time first (lrtf) algorithm
we have given some process with arrival time and burst time and we have to find the completion time (ct), turn around time(tat), average turn around time (avg tat), waiting time(wt), average waiting time (awt) for the given processes.
example: consider the following table of arrival time and burst time for four processes p, p, p and p.
process arrival time burst time p  ms  ms p  ms  ms p  ms  ms p  ms  ms
gantt chart will be as following below,
since, complietion time (ct) can be directly determined by gantt chart, and
turn around time (tat) = (complition time) - (arival time) also, waiting time (wt) = (turn around time) - (burst time)
therefore,
output:
total turn around time =  ms so, average turn around time = / = . ms and, total waiting time =  ms so, average waiting time = . ms
algorithm –
step-: create a structure of process containing all necessary fields like at (arrival time), bt(burst time), ct(completion time), tat(turn around time), wt(waiting time).
create a structure of process containing all necessary fields like at (arrival time), bt(burst time), ct(completion time), tat(turn around time), wt(waiting time). step-: sort according to the at;
sort according to the at; step-: find the process having largest burst time and execute for each single unit. increase the total time by  and reduce the burst time of that process with .
find the process having largest burst time and execute for each single unit. increase the total time by  and reduce the burst time of that process with . step-: when any process have  bt left, then update the ct(completion time of that process ct will be total time at that time).
when any process have  bt left, then update the ct(completion time of that process ct will be total time at that time). step-: after calculating the ct for each process, find tat and wt. (tat = ct - at) (wt = tat - bt)
implementation of algorithm –
" ; for (i = ; i < ; i++) { cout << p[i].processno << "\t" ; cout << p[i].at << "\t" ; cout << p[i].bt << "\t" ; cout << endl; } cout << endl; sort(p, p + , compare); totaltime += p[].at; prefinaltotal += p[].at; findct(); int totalwt = ; int totaltat = ; for (i = ; i < ; i++) { p[i].tat = p[i].ct - p[i].at; p[i].wt = p[i].tat - p[i].btbackup; totalwt += p[i].wt; totaltat += p[i].tat; } cout << "after execution of all processes ...
" ; cout << "pno\tat\tbt\tct\ttat\twt
output:
pno at bt             process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process executing at time  is: p process p is completed at  process executing at time  is: p process p is completed at  process executing at time  is: p process p is completed at  process executing at time  is: p process p is completed at  after execution of all processes ... pno at bt ct tat wt                         total tat =  average tat =  total wt =  average wt = 
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : shubhamsingh
prerequisite – process management | cpu scheduling
this is a pre-emptive version of longest job first (ljf) scheduling algorithm. in this scheduling algorithm, we find the process with the maximum remaining time and then process it. we check for the maximum remaining time after some interval of time(say  unit each) to check if another process having more burst time arrived up to that time.
procedure:
step-: first, sort the processes in increasing order of their arrival time.
first, sort the processes in increasing order of their arrival time. step-: choose the process having least arrival time but with most burst time. then process it for  unit. check if any other process arrives upto that time of execution or not.
choose the process having least arrival time but with most burst time. then process it for  unit. check if any other process arrives upto that time of execution or not. step-: repeat the above both steps until execute all the processes.
example-: consider the following table of arrival time and burst time for four processes p, p, p and p.
process arrival time burst time p  ms  ms p  ms  ms p  ms  ms p  ms  ms
working: (for input ):
at t = , available process : p. so, select p and execute  ms. at t = , available process : p, p. so, select p and execute  ms (since bt(p)= which is less than bt(p) = ) at t = , available process : p, p, p. so, select p and execute  ms (since, bt(p) =  , bt(p) =  , bt(p) = ). repeat the above steps until the execution of all processes.
note that cpu will be idle for  to  unit time since there is no process available in the given interval.
gantt chart will be as following below,
since, complietion time (ct) can be directly determined by gantt chart, and
turn around time (tat) = (complition time) - (arival time) also, waiting time (wt) = (turn around time) - (burst time)
therefore, final table look like,
output:
total turn around time =  ms so, average turn around time = / = . ms and, total waiting time =  ms so average waiting time = / = . ms
example-: consider the following table of arrival time and burst time for four processes p, p, p,p and p.
process arrival time burst time p  ms  ms p  ms  ms p  ms  ms p  ms  ms p  ms  ms
similarly example-, gantt chart for this example,
since, complietion time (ct) can be directly determined by gantt chart, and
turn around time (tat) = (complition time) - (arival time) also, waiting time (wt) = (turn around time) - (burst time)
therefore, final table look like,
output:
total turn around time =  ms so, average turn around time = / = . ms and, total waiting time =  ms so, average waiting time = / = . ms
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
round robin is a cpu scheduling algorithm where each process is assigned a fixed time slot in a cyclic way.
it is simple, easy to implement, and starvation-free as all processes get fair share of cpu.
one of the most commonly used technique in cpu scheduling as a core.
it is preemptive as processes are assigned cpu only for a fixed slice of time at most.
the disadvantage of it is more overhead of context switching.
serial no. advantages disadvantages . there is fairness since every process gets equal share of cpu. there is larger waiting time and response time. . the newly created process is added to end of ready queue. there is low throughput. . a round-robin scheduler generally employs time-sharing, giving each job a time slot or quantum. there is context switches. . while performing a round-robin scheduling,a particular time quantum is alloted to different jobs. gantt chart seems to come too big (if quantum time is less for scheduling.for example: ms for big scheduling.) . each process get a chance to reschedule after a particular quantum time in this scheduling. time consuming scheduling for small quantums .
illustration:
how to compute below times in round robin using a program?
completion time: time at which process completes its execution. turn around time: time difference between completion time and arrival time. turn around time = completion time – arrival time waiting time(w.t): time difference between turn around time and burst time.
waiting time = turn around time – burst time
in this post, we have assumed arrival times as , so turn around and completion times are same.
the tricky part is to compute waiting times. once waiting times are computed, turn around times can be quickly computed.
steps to find waiting times of all processes:
- create an array rem_bt[] to keep track of remaining burst time of processes. this array is initially a copy of bt[] (burst times array) - create another array wt[] to store waiting times of processes. initialize this array as . - initialize time : t =  - keep traversing the all processes while all processes are not done. do following for i'th process if it is not done yet. a- if rem_bt[i] > quantum (i) t = t + quantum (ii) bt_rem[i] -= quantum; c- else // last cycle for this process (i) t = t + bt_rem[i]; (ii) wt[i] = t - bt[i] (ii) bt_rem[i] = ; // this process is over
once we have waiting times, we can compute turn around time tat[i] of a process as sum of waiting and burst times, i.e., wt[i] + bt[i]
below is implementation of above steps.
" ; for ( int i=; i<n; i++) { total_wt = total_wt + wt[i]; total_tat = total_tat + tat[i]; cout << " " << i+ << "\t\t" << bt[i] << "\t " << wt[i] << "\t\t " << tat[i] <<endl; } cout << "average waiting time = " << ( float )total_wt / ( float )n; cout << "
processes burst time waiting time turn around time             average waiting time =  average turn around time = .
this article is contributed by sahil chhabra. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
prerequisite – program for round robin scheduling
in the traditional round robin scheduling algorithm all processes were treated equally for processing. the objective of the selfish round robin is to give better service to processes that have been executing for a while than to newcomers. its a more logical and superior implementation compared to the normal round robin algorithm.
implimentation :-
processes in the ready list are partitioned into two lists: new and accepted.
the new processes wait while accepted processes are serviced by the round robin.
priority of a new process increases at rate ‘a’ while the priority of an accepted process increases at rate ‘b’.
when the priority of a new process reaches the priority of an accepted process, that new process becomes accepted.
if all accepted processes finish, the highest priority new process is accepted.
let’s trace out the general working of this algorithm :-
step  : assume that initially there are no ready processes, when the first one, a, arrives. it has priority  to begin with. since there are no other accepted processes, a is accepted immediately.
step  : after a while another process, b, arrives. as long as b / a < , b’s priority will eventually catch up to a’s, so it is accepted; now both a and b have the same priority.
step  : all accepted processes share a common priority (which rises at rate b ); that makes this policy easy to implement i.e any new process’s priority is bound to get accepted at some point. so no process has to experience starvation.
step  : even if b / a > , a will eventually finish, and then b can be accepted.
adjusting the parameters a and b : -> if b / a >= , a new process is not accepted until all the accepted processes have finished, so srr becomes fcfs. -> if b / a = , all processes are accepted immediately, so srr becomes rr. -> if  < b / a < , accepted processes are selfish, but not completely.
example on selfish round robin –
solution (where a =  and b = ) –
explanation –
process a gets accepted as soon as it comes at time t = . so its priority is increased only by ‘b’ i.e ‘’ after each second. b enters at time t =  and goes to the waiting queue. so its priority gets increased by ‘a’ i.e. ‘’ at time t = . at this point priority of a = priority of b = .
so now both process a & b are in the accepted queue and are executed in a round robin fashion. at time t =  process c enters the waiting queue. at time t =  the priority of process c catches up to the priority of process b and then they start executing in a round robin manner. when b finishes execution at time t = , d is automatically promoted to the accepted queue.
similarly when d finishes execution at time t = , e is automatically promoted to the accepted queue.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
siddhant-bajaj interested in everything cs/it aspire with my acer aspire r to crack gate avid follower of ravindrababu ravula trying my best to keep right up my alley with competitive coding open source and web development projects i am somewhat good at chess and spend loads of time on geeksforgeeks
prerequisite: round robin scheduling with arrival time as 
round robin scheduling algorithm is used to schedule process fairly each job a time slot or quantum and the interrupting the job if it is not completed by then the job come after the other job which is arrived in the quantum time that makes these scheduling fairly
note:
round robin is cyclic in nature so starvation doesn’t occur
round robin is a variant of first come, first served scheduling
no priority, special importance given to any process or task
rr scheduling is also known as time slicing scheduling
advantages:
each process is served by cpu for a fixed time so priority is the same for each one
starvation does not occur because of its cyclic nature.
disadvantages:
throughput depends on quantum time.
if we want to give some process priority, we cannot.
process arrival time burst time completion time turn around time waiting time p      p      p      p     
quantum time is  this means each process is only executing for  units of time at a time.
how to compute these process requests:-
take the process which occurs first and start executing the process.(for quantum time only) check if any other process request has arrived. if a process request arrives during the quantum time in which another process is executing, then add the new process to the ready queue after the quantum time has passed, check for any processes in the ready queue. if the ready queue is empty continue the current process. if the queue not empty and the current process is not complete, then add the current process to the end of the ready queue. take the first process from the ready queue and start executing it (same rules) repeat all steps above from - if the process is complete and the ready queue is empty then the task is complete
after all these we get the three times which are:
completion time: the time taken for a process to complete. turn around time: total time the process exists in the system. (completion time – arrival time). waiting time: total time the waiting for there complete execution. (turn around time – burst time ).
how to implement in a programming language
. create two arrays of burst time res_b[] and of arrival time res_a[] and copy the value of the b[] and a[] array for calculate the remaining time.(b[] is burst time, a[] arrival time). . create an another array for wt[] to store waiting time. . initialize time : t=; . keep traversing the all process while all process are not done. do following for i'th process if it is not done yet. a- if res_a[i]q a. t=t+q b. res_b[i]-=q; c. a[i]+=q; . else res_b[i]q a. t=t+q b. res_b[j]-=q; c. a[j]+=q; . else res_b[j]q a. t=t+q b. res_b[i]-=q; c. a[i]+=q; . else res_b[i]<=q a. t=t+b[i]; b. wt[i]=t-b[i]-a[i]; c.res_b[i]=;
below is the implementation of the above approach:
" ; for ( int i = ; i < n; i++) { total_wt = total_wt + wt_time[i]; total_tat = total_tat + tat_time[i]; cout << setw() << i +  << setw() << arrival_time[i] << setw() << burst_time[i] << setw() << completion_time[i] << setw() << tat_time[i] << setw() << wt_time[i] << endl; } cout << "
average waiting time : " << ( float )total_wt / ( float )n; cout << "
output processes arrival time burst time completion time turn around time waiting time                         average waiting time = . average turn around time = .
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
abhisheksharmaabhi my name is abhishek i am currenty persuing the be from cgc langranmohalli
priority scheduling is one of the most common scheduling algorithms in batch systems. each process is assigned a priority. process with the highest priority is to be executed first and so on.
processes with the same priority are executed on first come first served basis. priority can be decided based on memory requirements, time requirements or any other resource requirement.
implementation :
- first input the processes with their burst time and priority. - sort the processes, burst time and priority according to the priority. - now simply apply fcfs algorithm.
note: a major problem with priority scheduling is indefinite blocking or starvation. a solution to the problem of indefinite blockage of the low-priority process is aging. aging is a technique of gradually increasing the priority of processes that wait in the system for a long period of time.
processes " << " burst time " << " waiting time " << " turn around time
" ; for ( int i=; i<n; i++) { total_wt = total_wt + wt[i]; total_tat = total_tat + tat[i]; cout << " " << proc[i].pid << "\t\t" << proc[i].bt << "\t " << wt[i] << "\t\t " << tat[i] <<endl; } cout << "
average waiting time = " << ( float )total_wt / ( float )n; cout << "
average turn around time = " << ( float )total_tat / ( float )n; } void priorityscheduling(process proc[], int n) { sort(proc, proc + n, comparison); cout<< "order in which processes gets executed
processes burst time waiting" , "time turn-around time" ) total_wt =  total_tat =  for i in range (n): total_wt = total_wt + wt[i] total_tat = total_tat + tat[i] print ( " " , processes[i][  ], "\t\t" , processes[i][  ], "\t\t" , wt[i], "\t\t" , tat[i]) print ( "
output:
order in which processes gets executed    processes burst time waiting time turn around time             average waiting time = . average turn around time = 
in this post, the processes with arrival time  are discussed. in next set, we will be considering different arrival times to evaluate waiting times.
this article is contributed by sahil chhabra (akku). if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : shubhamsingh
implementing priority cpu scheduling. in this problem, we are using min heap as the data structure for implementing priority scheduling.
in this problem smaller numbers denote higher priority.
struct process { processid, burst time, response time, priority, arrival time. }
void quicksort(process array[], low, high)– this function is used to arrange the processes in ascending order according to their arrival time.
int partition(process array[], int low, int high)– this function is used to partition the array for sorting.
void insert(process heap[], process value, int *heapsize, int *currenttime)– it is used to include all the valid and eligible processes in the heap for execution. heapsize defines the number of processes in execution depending on the current time currenttime keeps record of the current cpu time.
void order(process heap[], int *heapsize, int start)– it is used to reorder the heap according to priority if the processes after insertion of new process.
void extractminimum(process heap[], int *heapsize, int *currenttime)– this function is used to find the process with highest priority from the heap. it also reorders the heap after extracting the highest priority process.
void scheduling(process heap[], process array[], int n, int *heapsize, int *currenttime)– this function is responsible for executing the highest priority extracted from heap[].
void process(process array[], int n)– this function is responsible for managing the entire execution of the processes as they arrive in the cpu according to their arrival time.
" , min.processid, *currenttime); if (min.bursttime > ) { insert(heap, min, heapsize, currenttime); return ; } for ( int i = ; i < n; i++) if (array[i].processid == min.processid) { array[i] = min; break ; } } void priority(process array[], int n) { sort(array, array + n, compare); int totalwaitingtime = , totalbursttime = , totalturnaroundtime = , i, insertedprocess = , heapsize = , currenttime = array[].arrivaltime, totalresponsetime = ; process heap[ * n]; for ( int i = ; i < n; i++) { totalbursttime += array[i].bursttime; array[i].tempbursttime = array[i].bursttime; } do { if (insertedprocess != n) { for (i = ; i < n; i++) { if (array[i].arrivaltime == currenttime) { ++insertedprocess; array[i].intime = -; array[i].responsetime = -; insert(heap, array[i], &heapsize, ¤ttime); } } } scheduling(heap, array, n, &heapsize, ¤ttime); ++currenttime; if (heapsize ==  && insertedprocess == n) break ; } while (); for ( int i = ; i < n; i++) { totalresponsetime += array[i].responsetime; totalwaitingtime += (array[i].outtime - array[i].intime - array[i].tempbursttime); totalbursttime += array[i].bursttime; } printf ( "average waiting time = %f
" , (( float )totalwaitingtime / ( float )n)); printf ( "average response time =%f
" , (( float )totalresponsetime / ( float )n)); printf ( "average turn around time = %f
" , (( float )(totalwaitingtime + totalbursttime) / ( float )n)); } int main() { int n, i; process a[]; a[].processid = ; a[].arrivaltime = ; a[].priority = ; a[].bursttime = ; a[].processid = ; a[].arrivaltime = ; a[].priority = ; a[].bursttime = ; a[].processid = ; a[].arrivaltime = ; a[].priority = ; a[].bursttime = ; a[].processid = ; a[].arrivaltime = ; a[].priority = ; a[].bursttime = ; a[].processid = ; a[].arrivaltime = ; a[].priority = ; a[].bursttime = ; priority(a, ); return ; }
output: process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  process id =  current time =  average waiting time = . average response time =. average turn around time = .
the output displays the order in which the processes are executed in the memory and also shows the average waiting time, average response time and average turn around time for each process.
this article is contributed by hardik gaur. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : gitinkakkar
prerequisite – program for priority scheduling – set 
priority scheduling is a non-preemptive algorithm and one of the most common scheduling algorithms in batch systems. each process is assigned first arrival time (less arrival time process first) if two processes have same arrival time, then compare to priorities (highest process first). also, if two processes have same priority then compare to process number (less process number first). this process is repeated while all process get executed.
implementation –
first input the processes with their arrival time, burst time and priority. sort the processes, according to arrival time if two process arrival time is same then sort according process priority if two process priority are same then sort according to process number. now simply apply fcfs algorithm.
gantt chart –
examples –
input : process no->      arrival time->      burst time->      priority->      output : process_no start_time complete_time trun_around_time wating_time                          average wating time is : . average trun around time is : .
process_no start_time complete_time trun_around_time wating_time                          average wating time is : . average trun around time is : .
this article is contributed by amit verma . if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important java and collections concepts with the fundamentals of java and java collections course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : cyberfreak, shubhamsingh, checkway
prerequisites : priority scheduling
we have already discussed about the priority scheduling in this post. it is one of the most common scheduling algorithms in batch systems. each process is assigned a priority. process with the highest priority is to be executed first and so on.
in this post we will discuss a major problem related to priority scheduling and it’s solution.
starvation or indefinite blocking is phenomenon associated with the priority scheduling algorithms, in which a process ready to run for cpu can wait indefinitely because of low priority. in heavily loaded computer system, a steady stream of higher-priority processes can prevent a low-priority process from ever getting the cpu.
there has been rumors that in  priority scheduling was used in ibm  at mit , and they found a low-priority process that had not been submitted till .
as we see in the above example process having higher priority than other processes getting cpu earlier. we can think of a scenario in which only one process is having very low-priority (for example ) and we are giving other process with high-priority, this can lead indefinitely waiting for the process for cpu which is having low-priority, this leads to starvation. further we have also discuss about the solution of starvation.
differences between deadlock and starvation in os :
deadlock occurs when none of the processes in the set is able to move ahead due to occupancy of the required resources by some other process as shown in the figure below, on the other hand starvation occurs when a process waits for an indefinite period of time to get the resource it requires. other name of deadlock is circular waiting. other name of starvation is lived lock. when deadlock occurs no process can make progress, while in starvation apart from the victim process other processes can progress or proceed.
solution to starvation : aging
aging is a technique of gradually increasing the priority of processes that wait in the system for a long time.for example, if priority range from (low) to (high), we could increase the priority of a waiting process by  every  minutes. eventually even a process with an initial priority of  would take no more than  hours for priority  process to age to a priority- process.
this article is contributed by saloni gupta. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
prerequisite – cpu scheduling
given n processes with their arrival times and burst times, the task is to find average waiting time and average turn around time using hrrn scheduling algorithm.
the name itself states that we need to find the response ratio of all available processes and select the one with the highest response ratio. a process once selected will run till completion.
criteria – response ratio
mode – non-preemptive
response ratio = (w + s)/s
here, w is the waiting time of the process so far and s is the burst time of the process.
performance of hrrn –
shorter processes are favoured. aging without service increases ratio, longer jobs can get past shorter jobs.
gantt chart –
explanation –
at t =  we have only one process available, so a gets scheduled.
similarly at t =  we have only one process available, so b gets scheduled.
now at t =  we have  processes available, c, d and e. since, c, d and e were available after ,  and  units respectively. therefore, waiting time for c, d and e are ( –  =), ( –  =), and ( –  =) unit respectively.
using the formula given above we calculate the response ratios of c, d and e respectively as ., . and ..
clearly c has the highest response ratio and so it gets scheduled
next at t =  we have  jobs available d and e.
response ratios of d and e are . and . respectively.
so process e is selected next and process d is selected last.
implementation of hrrn scheduling –
input the number of processes, their arrival times and burst times. sort them according to their arrival times. at any given time calculate the response ratios and select the appropriate process to be scheduled. calculate the turn around time as completion time – arrival time. calculate the waiting time as turn around time – burst time. turn around time divided by the burst time gives the normalized turn around time. sum up the waiting and turn around times of all processes and divide by the number of processes to get the average waiting and turn around time.
below is the implementation of above approach:
" << p[loc].name << "\t" << p[loc].at; cout << "\t\t" << p[loc].bt << "\t\t" << p[loc].wt; cout << "\t\t" << p[loc].tt << "\t\t" << p[loc].ntt; } cout << "
name\tarrival time\tburst time\twaiting time" ); printf ( "\tturnaround time\t normalized tt" ); for (t = p[].at; t < sum_bt;) { float hrr = -; float temp; int loc; for (i = ; i < n; i++) { if (p[i].at <= t && p[i].completed != ) { temp = (p[i].bt + (t - p[i].at)) / p[i].bt; if (hrr < temp) { hrr = temp; loc = i; } } } t += p[loc].bt; p[loc].wt = t - p[loc].at - p[loc].bt; p[loc].tt = t - p[loc].at; avgtt += p[loc].tt; p[loc].ntt = (( float )p[loc].tt / p[loc].bt); p[loc].completed = ; avgwt += p[loc].wt; printf ( "
%c\t\t%d\t\t" , p[loc].name, p[loc].at); printf ( "%d\t\t%d\t\t" , p[loc].bt, p[loc].wt); printf ( "%d\t\t%f" , p[loc].tt, p[loc].ntt); } printf ( "
average waiting time:%f
" , avgwt / n); printf ( "average turn around time:%f
name arrival time burst time waiting time turnaround time normalized tt a     . b     . c     . e     . d     . average waiting time:. average turn around time:.
this article is contributed by siddhant bajaj. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : shivi_aggarwal
prerequisite : cpu scheduling
it may happen that processes in the ready queue can be divided into different classes where each class has its own scheduling needs. for example, a common division is a foreground (interactive) process and background (batch) processes.these two classes have different scheduling needs. for this kind of situation multilevel queue scheduling is used.now, let us see how it works.
ready queue is divided into separate queues for each class of processes. for example, let us take three different types of process system processes, interactive processes and batch processes. all three process have there own queue. now,look at the below figure.
all three different type of processes have there own queue. each queue have its own scheduling algorithm. for example, queue  and queue  uses round robin while queue  can use fcfs to schedule there processes.
scheduling among the queues : what will happen if all the queues have some processes? which process should get the cpu? to determine this scheduling among the queues is necessary. there are two ways to do so –
fixed priority preemptive scheduling method – each queue has absolute priority over lower priority queue. let us consider following priority order queue  > queue  > queue .according to this algorithm no process in the batch queue(queue ) can run unless queue  and  are empty. if any batch process (queue ) is running and any system (queue ) or interactive process(queue ) entered the ready queue the batch process is preempted. time slicing – in this method each queue gets certain portion of cpu time and can use it to schedule its own processes.for instance, queue  takes  percent of cpu time queue  takes  percent and queue  gets  percent of cpu time.
example problem :
consider below table of four processes under multilevel queue scheduling.queue number denotes the queue of the process.
priority of queue  is greater than queue . queue  uses round robin (time quantum = ) and queue  uses fcfs.
below is the gantt chart of the problem :
at starting both queues have process so process in queue  (p, p) runs first (because of higher priority) in the round robin fashion and completes after  units then process in queue  (p) starts running (as there is no process in queue ) but while it is running p comes in queue  and interrupts p and start running for  second and after its completion p takes the cpu and completes its execution.
advantages:
the processes are permanently assigned to the queue, so it has advantage of low scheduling overhead.
disadvantages:
some processes may starve for cpu if some higher priority queues are never becoming empty.
it is inflexible in nature.
this article is contributed by ashish sharma. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : itskawal
prerequisite – cpu scheduling, multilevel queue scheduling
this scheduling is like multilevel queue(mlq) scheduling but in this process can move between the queues. multilevel feedback queue scheduling (mlfq) keep analyzing the behavior (time of execution) of processes and according to which it changes its priority.now, look at the diagram and explanation below to understand it properly.
now let us suppose that queue  and  follow round robin with time quantum  and  respectively and queue  follow fcfs.one implementation of mfqs is given below –
when a process starts executing then it first enters queue . in queue  process executes for  unit and if it completes in this  unit or it gives cpu for i/o operation in this  unit than the priority of this process does not change and if it again comes in the ready queue than it again starts its execution in queue . if a process in queue  does not complete in  unit then its priority gets reduced and it shifted to queue . above points  and  are also true for queue  processes but the time quantum is  unit.in a general case if a process does not complete in a time quantum than it is shifted to the lower priority queue. in the last queue, processes are scheduled in fcfs manner. a process in lower priority queue can only execute only when higher priority queues are empty. a process running in the lower priority queue is interrupted by a process arriving in the higher priority queue.
well, above implementation may differ for example the last queue can also follow round-robin scheduling.
problems in the above implementation – a process in the lower priority queue can suffer from starvation due to some short processes taking all the cpu time.
solution – a simple solution can be to boost the priority of all the process after regular intervals and place them all in the highest priority queue.
what is the need of such complex scheduling?
firstly, it is more flexible than the multilevel queue scheduling.
to optimize turnaround time algorithms like sjf is needed which require the running time of processes to schedule them. but the running time of the process is not known in advance. mfqs runs a process for a time quantum and then it can change its priority(if it is a long process). thus it learns from past behavior of the process and then predicts its future behavior.this way it tries to run shorter process first thus optimizing turnaround time.
mfqs also reduces the response time.
example –
consider a system which has a cpu bound process, which requires the burst time of  seconds.the multilevel feed back queue scheduling algorithm is used and the queue time quantum ‘’ seconds and in each level it is incremented by ‘’ seconds.then how many times the process will be interrupted and on which queue the process will terminate the execution?
solution –
process p needs  seconds for total execution.
at queue  it is executed for  seconds and then interrupted and shifted to queue .
at queue  it is executed for  seconds and then interrupted and shifted to queue .
at queue  it is executed for  seconds and then interrupted and shifted to queue .
at queue  it is executed for  seconds and then interrupted and shifted to queue .
at queue  it executes for  seconds and then it completes.
hence the process is interrupted  times and completes on queue .
advantages:
it is more flexible. it allows different processes to move between different queues. it prevents starvation by moving a process that waits too long for lower priority queue to the higher priority queue.
disadvantages:
for the selection of the best scheduler, it require some other means to select the values. it produces more cpu overheads. it is most complex algorithm.
this article is contributed by ashish sharma. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : itskawal
prerequisite – cpu scheduling, process management
lottery scheduling is type of process scheduling, somewhat different from other scheduling. processes are scheduled in a random manner. lottery scheduling can be preemptive or non-preemptive. it also solves the problem of starvation. giving each process at least one lottery ticket guarantees that it has non-zero probability of being selected at each scheduling operation.
in this scheduling every process have some tickets and scheduler picks a random ticket and process having that ticket is the winner and it is executed for a time slice and then another ticket is picked by the scheduler. these tickets represent the share of processes. a process having a higher number of tickets give it more chance to get chosen for execution.
example – if we have two processes a and b having  and  tickets respectively out of total  tickets. cpu share of a is % and that of b is %.these shares are calculated probabilistically and not deterministically.
explanation –
we have two processes a and b. a has  tickets (ticket number  to ) and b have  tickets (ticket no.  to ). scheduler picks a random number from  to . if the picked no. is from  to  then a is executed otherwise b is executed. an example of  tickets picked by scheduler may look like this – ticket number -          . resulting schedule - b b a a a b a a a a. a is executed  times and b is executed  times. as you can see that a takes % of cpu and b takes % which is not the same as what we need as we need a to have % of cpu and b should have % of cpu.this happens because shares are calculated probabilistically but in a long run(i.e when no. of tickets picked is more than  or ) we can achieve a share percentage of approx.  and  for a and b respectively.
ways to manipulate tickets –
ticket currency –
scheduler give a certain number of tickets to different users in a currency and users can give it to there processes in a different currency. e.g. two users a and b are given  and  tickets respectively. user a is running two process and give  tickets to each in a’s own currency. b is running  process and gives it all  tickets in b’s currency. now at the time of scheduling tickets of each process are converted into global currency i.e a’s process will have  tickets each and b’s process will have  tickets and scheduling is done on this basis.
scheduler give a certain number of tickets to different users in a currency and users can give it to there processes in a different currency. e.g. two users a and b are given  and  tickets respectively. user a is running two process and give  tickets to each in a’s own currency. b is running  process and gives it all  tickets in b’s currency. now at the time of scheduling tickets of each process are converted into global currency i.e a’s process will have  tickets each and b’s process will have  tickets and scheduling is done on this basis. transfer tickets –
a process can pass its tickets to another process.
a process can pass its tickets to another process. ticket inflation –
with this technique a process can temporarily raise or lower the number of tickets it own.
in multiple-processor scheduling multiple cpu’s are available and hence load sharing becomes possible. however multiple processor scheduling is more complex as compared to single processor scheduling. in multiple processor scheduling there are cases when the processors are identical i.e. homogeneous, in terms of their functionality, we can use any processor available to run any process in the queue.
approaches to multiple-processor scheduling –
a second approach uses symmetric multiprocessing where each processor is self scheduling. all processes may be in a common ready queue or each processor may have its own private queue for ready processes. the scheduling proceeds further by having the scheduler for each processor examine the ready queue and select a process to execute.
processor affinity –
processor affinity means a processes has an affinity for the processor on which it is currently running.
when a process runs on a specific processor there are certain effects on the cache memory. the data most recently accessed by the process populate the cache for the processor and as a result successive memory access by the process are often satisfied in the cache memory. now if the process migrates to another processor, the contents of the cache memory must be invalidated for the first processor and the cache for the second processor must be repopulated. because of the high cost of invalidating and repopulating caches, most of the smp(symmetric multiprocessing) systems try to avoid migration of processes from one processor to another and try to keep a process running on the same processor. this is known as processor affinity.
there are two types of processor affinity:
soft affinity – when an operating system has a policy of attempting to keep a process running on the same processor but not guaranteeing it will do so, this situation is called soft affinity. hard affinity – hard affinity allows a process to specify a subset of processors on which it may run. some systems such as linux implements soft affinity but also provide some system calls like sched_setaffinity() that supports hard affinity.
load balancing –
load balancing is the phenomena which keeps the workload evenly distributed across all processors in an smp system. load balancing is necessary only on systems where each processor has its own private queue of process which are eligible to execute. load balancing is unnecessary because once a processor becomes idle it immediately extracts a runnable process from the common run queue. on smp(symmetric multiprocessing), it is important to keep the workload balanced among all processors to fully utilize the benefits of having more than one processor else one or more processor will sit idle while other processors have high workloads along with lists of processors awaiting the cpu.
there are two general approaches to load balancing :
push migration – in push migration a task routinely checks the load on each processor and if it finds an imbalance then it evenly distributes load on each processors by moving the processes from overloaded to idle or less busy processors. pull migration – pull migration occurs when an idle processor pulls a waiting task from a busy processor for its execution.
multicore processors –
in multicore processors multiple processor cores are places on the same physical chip. each core has a register set to maintain its architectural state and thus appears to the operating system as a separate physical processor. smp systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip.
however multicore processors may complicate the scheduling problems. when processor accesses memory then it spends a significant amount of time waiting for the data to become available. this situation is called memory stall. it occurs for various reasons such as cache miss, which is accessing the data that is not in the cache memory. in such cases the processor can spend upto fifty percent of its time waiting for data to become available from the memory. to solve this problem recent hardware designs have implemented multithreaded processor cores in which two or more hardware threads are assigned to each core. therefore if one thread stalls while waiting for the memory, core can switch to another thread.
there are two ways to multithread a processor :
coarse-grained multithreading – in coarse grained multithreading a thread executes on a processor until a long latency event such as a memory stall occurs, because of the delay caused by the long latency event, the processor must switch to another thread to begin execution. the cost of switching between threads is high as the instruction pipeline must be terminated before the other thread can begin execution on the processor core. once this new thread begins execution it begins filling the pipeline with its instructions. fine-grained multithreading – this multithreading switches between threads at a much finer level mainly at the boundary of an instruction cycle. the architectural design of fine grained systems include logic for thread switching and as a result the cost of switching between threads is small.
virtualization and threading –
in this type of multiple-processor scheduling even a single cpu system acts like a multiple-processor system. in a system with virtualization, the virtualization presents one or more virtual cpu to each of virtual machines running on the system and then schedules the use of physical cpu among the virtual machines. most virtualized environments have one host operating system and many guest operating systems. the host operating system creates and manages the virtual machines. each virtual machine has a guest operating system installed and applications run within that guest.each guest operating system may be assigned for specific use cases,applications or users including time sharing or even real-time operation. any guest operating-system scheduling algorithm that assumes a certain amount of progress in a given amount of time will be negatively impacted by the virtualization. a time sharing operating system tries to allot  milliseconds to each time slice to give users a reasonable response time. a given  millisecond time slice may take much more than  milliseconds of virtual cpu time. depending on how busy the system is, the time slice may take a second or more which results in a very poor response time for users logged into that virtual machine. the net effect of such scheduling layering is that individual virtualized operating systems receive only a portion of the available cpu cycles, even though they believe they are receiving all cycles and that they are scheduling all of those cycles.commonly, the time-of-day clocks in virtual machines are incorrect because timers take no longer to trigger than they would on dedicated cpu’s.
virtualizations can thus undo the good scheduling-algorithm efforts of the operating systems within virtual machines.
reference –
operating system principles – galvin
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : vighneshkamath, mangoman
on the basis of synchronization, processes are categorized as one of the following two types:
independent process : execution of one process does not affects the execution of other processes.
: execution of one process does not affects the execution of other processes. cooperative process : execution of one process affects the execution of other processes.
process synchronization problem arises in the case of cooperative process also because resources are shared in cooperative processes.
race condition
a race condition is a situation that may occur inside a critical section. this happens when the result of multiple thread execution in the critical section differs according to the order in which the threads execute.
race conditions in critical sections can be avoided if the critical section is treated as an atomic instruction. also, proper thread synchronization using locks or atomic variables can prevent race conditions.
critical section problem
in the entry section, the process requests for entry in the critical section.
any solution to the critical section problem must satisfy three requirements:
mutual exclusion : if a process is executing in its critical section, then no other process is allowed to execute in the critical section.
: if a process is executing in its critical section, then no other process is allowed to execute in the critical section. progress : if no process is executing in the critical section and other processes are waiting outside the critical section, then only those processes that are not executing in their remainder section can participate in deciding which will enter in the critical section next, and the selection can not be postponed indefinitely.
: if no process is executing in the critical section and other processes are waiting outside the critical section, then only those processes that are not executing in their remainder section can participate in deciding which will enter in the critical section next, and the selection can not be postponed indefinitely. bounded waiting : a bound must exist on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted.
peterson’s solution
peterson’s solution is a classical software based solution to the critical section problem.
in peterson’s solution, we have two shared variables:
boolean flag[i] :initialized to false, initially no one is interested in entering the critical section
int turn : the process whose turn is to enter the critical section.
peterson’s solution preserves all three conditions :
mutual exclusion is assured as only one process can access the critical section at any time.
progress is also assured, as a process outside the critical section does not block other processes from entering the critical section.
bounded waiting is preserved as every process gets a fair chance.
disadvantages of peterson’s solution it involves busy waiting
it is limited to  processes. testandset
testandset is a hardware solution to the synchronization problem. in testandset, we have a shared lock variable which can take either of the two values,  or .
 unlock  lock
before entering into the critical section, a process inquires about the lock. if it is locked, it keeps on waiting until it becomes free and if it is not locked, it takes the lock and executes the critical section.
in testandset, mutual exclusion and progress are preserved but bounded waiting cannot be preserved.
question : the enter_cs() and leave_cs() functions to implement critical section of a process are realized using test-and-set instruction as follows:
int testandset(int &lock) { int initial = lock; lock = ; return initial; } void enter_cs(x) { while test-and-set(x) ; } void leave_cs(x) { x = ; }
in the above solution, x is a memory location associated with the cs and is initialized to . now, consider the following statements:
i. the above solution to cs problem is deadlock-free
ii. the solution is starvation free.
iii. the processes enter cs in fifo order.
iv. more than one process can enter cs at the same time.
which of the above statements is true?
click here for the solution.
true
semaphores
a semaphore is a signaling mechanism and a thread that is waiting on a semaphore can be signaled by another thread. this is different than a mutex as the mutex can be signaled only by the thread that called the wait function.
a semaphore uses two atomic operations, wait and signal for process synchronization.
a semaphore is an integer variable, which can be accessed only through two operations wait() and signal().
there are two types of semaphores: binary semaphores and counting semaphores
binary semaphores: they can only be either  or . they are also known as mutex locks, as the locks can provide mutual exclusion. all the processes can share the same mutex semaphore that is initialized to . then, a process has to wait until the lock becomes . then, the process can make the mutex semaphore  and start its critical section. when it completes its critical section, it can reset the value of mutex semaphore to  and some other process can enter its critical section.
counting semaphores: they can have any value and are not restricted over a certain domain. they can be used to control access to a resource that has a limitation on the number of simultaneous accesses. the semaphore can be initialized to the number of instances of the resource. whenever a process wants to use that resource, it checks if the number of remaining instances is more than zero, i.e., the process has an instance available. then, the process can enter its critical section thereby decreasing the value of the counting semaphore by . after the process is over with the use of the instance of the resource, it can leave the critical section thereby adding  to the number of available instances of the resource.
true
prerequisite – process synchronization | introduction, critical section, semaphores
process synchronization is a technique which is used to coordinate the process that use shared data. there are two types of processes in an operating systems:-
independent process –
the process that does not affect or is affected by the other process while its execution then the process is called independent process. example the process that does not share any shared variable, database, files, etc. cooperating process –
the process that affect or is affected by the other process while execution, is called a cooperating process. example the process that share file, variable, database, etc are the cooperating process.
process synchronization is mainly used for cooperating process that shares the resources.let us consider an example of
//racing condition image
it is the condition where several processes tries to access the resources and modify the shared data concurrently and outcome of the process depends on the particular order of execution that leads to data inconsistency, this condition is called race condition.this condition can be avoided using the technique called synchronization or process synchronization, in which we allow only one process to enter and manipulates the shared data in critical section.
//diagram of the view of cs
this setup can be defined in various regions like:
entry section –
it is part of the process which decide the entry of a particular process in the critical section, out of many other processes.
it is part of the process which decide the entry of a particular process in the critical section, out of many other processes. critical section –
it is the part in which only one process is allowed to enter and modify the shared variable.this part of the process ensures that only no other process can access the resource of shared data.
it is the part in which only one process is allowed to enter and modify the shared variable.this part of the process ensures that only no other process can access the resource of shared data. exit section –
this process allows the other process that are waiting in the entry section, to enter into the critical sections. it checks that a process that after a process has finished execution in critical section can be removed through this exit section.
this process allows the other process that are waiting in the entry section, to enter into the critical sections. it checks that a process that after a process has finished execution in critical section can be removed through this exit section. remainder section –
critical section problems must satisfy these three requirements:
mutual exclusion –
it states that no other process is allowed to execute in the critical section if a process is executing in critical section. progress –
when no process is in the critical section, then any process from outside that request for execution can enter in the critical section without any delay. only those process can enter that have requested and have finite time to enter the process. bounded waiting –
an upper bound must exist on the number of times a process enters so that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted.
process synchronization are handled by two approaches:
software approach –
in software approach, some specific algorithm approach is used to maintain synchronization of the data. like in approach one or approach two, for a number of two process, a temporary variable like (turn) or boolean variable (flag) value is used to store the data. when the condition is true then the process in waiting state, known as busy waiting state. this does not satisfy all the critical section requirements. another software approach known as peterson’s solution is best for synchronization. it uses two variables in the entry section so as to maintain consistency, like flag (boolean variable) and turn variable(storing the process states). it satisfy all the three critical section requirements. //image of peterson’s algorithm hardware approach –
the hardware approach of synchronization can be done through lock & unlock technique.locking part is done in the entry section, so that only one process is allowed to enter into the critical section, after it complete its execution, the process is moved to the exit section, where unlock operation is done so that another process in the lock section can repeat this process of execution.this process is designed in such a way that all the three conditions of the critical sections are satisfied.
//image of lock
using interrupts –
these are easy to implement.when interrupt are disabled then no other process is allowed to perform context switch operation that would allow only one process to enter into the critical state.
//image of interrupts
test_and_set operation –
this allows boolean value (true/false) as a hardware synchronization, which is atomic in nature i.e no other interrupt is allowed to access.this is mainly used in mutual exclusion application. similar type operation can be achieved through compare and swap function. in this process, a variable is allowed to accessed in critical section while its lock operation is on.till then, the other process is in busy waiting state. hence critical section requirements are achieved.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
kartikeya shukla  loves coding##
critical section:
in concurrent programming, if one thread tries to change the value of shared data at the same time as another thread tries to read the value (i.e. data race across threads), the result is unpredictable.
the access to such shared variable (shared memory, shared files, shared port, etc…) to be synchronized. few programming languages have built-in support for synchronization.
it is critical to understand the importance of race condition while writing kernel mode programming (a device driver, kernel thread, etc.). since the programmer can directly access and modifying kernel data structures.
a simple solution to the critical section can be thought as shown below,
acquirelock(); process critical section releaselock();
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
a process can be of two types:
independent process.
co-operating process.
an independent process is not affected by the execution of other processes while a co-operating process can be affected by other executing processes. though one can think that those processes, which are running independently, will execute very efficiently, in reality, there are many situations when co-operative nature can be utilised for increasing computational speed, convenience and modularity. inter process communication (ipc) is a mechanism which allows processes to communicate with each other and synchronize their actions. the communication between these processes can be seen as a method of co-operation between them. processes can communicate with each other through both:
shared memory message passing
the figure  below shows a basic structure of communication between processes via the shared memory method and via the message passing method.
an operating system can implement both method of communication. first, we will discuss the shared memory methods of communication and then message passing. communication between processes using shared memory requires processes to share some variable and it completely depends on how programmer will implement it. one way of communication using shared memory can be imagined like this: suppose process and process are executing simultaneously and they share some resources or use some information from another process. process generate information about certain computations or resources being used and keeps it as a record in shared memory. when process needs to use the shared information, it will check in the record stored in shared memory and take note of the information generated by process and act accordingly. processes can use shared memory for extracting information as a record from another process as well as for delivering any specific information to other processes.
let’s discuss an example of communication between processes using shared memory method.
i) shared memory method
ex: producer-consumer problem
shared data between the two processes
ii) messaging passing method
now, we will start our discussion of the communication between processes via message passing. in this method, processes communicate with each other without using any kind of shared memory. if two processes p and p want to communicate with each other, they proceed as follows:
establish a communication link (if a link already exists, no need to establish it again.)
start exchanging messages using basic primitives.
we need at least two primitives:
– send(message, destinaion) or send(message)
– receive(message, host) or receive(message)
the message size can be of fixed size or of variable size. if it is of fixed size, it is easy for an os designer but complicated for a programmer and if it is of variable size then it is easy for a programmer but complicated for the os designer. a standard message can have two parts: header and body.
the header part is used for storing message type, destination id, source id, message length, and control information. the control information contains information like what to do if runs out of buffer space, sequence number, priority. generally, message is sent using fifo style.
message passing through communication link.
direct and indirect communication link
now, we will start our discussion about the methods of implementing communication link. while implementing the link, there are some questions which need to be kept in mind like :
how are links established? can a link be associated with more than two processes? how many links can there be between every pair of communicating processes? what is the capacity of a link? is the size of a message that the link can accommodate fixed or variable? is a link unidirectional or bi-directional?
a link has some capacity that determines the number of messages that can reside in it temporarily for which every link has a queue associated with it which can be of zero capacity, bounded capacity, or unbounded capacity. in zero capacity, the sender waits until the receiver informs the sender that it has received the message. in non-zero capacity cases, a process does not know whether a message has been received or not after the send operation. for this, the sender must communicate with the receiver explicitly. implementation of the link depends on the situation, it can be either a direct communication link or an in-directed communication link.
direct communication links are implemented when the processes uses a specific process identifier for the communication, but it is hard to identify the sender ahead of time.
for example: the print server.
in-direct communication is done via a shared mailbox (port), which consists of a queue of messages. the sender keeps the message in mailbox and the receiver picks them up.
message passing through exchanging the messages.
synchronous and asynchronous message passing:
a process that is blocked is one that is waiting for some event, such as a resource becoming available or the completion of an i/o operation. ipc is possible between the processes on same computer as well as on the processes running on different computer i.e. in networked/distributed system. in both cases, the process may or may not be blocked while sending a message or attempting to receive a message so message passing may be blocking or non-blocking. blocking is considered synchronous and blocking send means the sender will be blocked until the message is received by receiver. similarly, blocking receive has the receiver block until a message is available. non-blocking is considered asynchronous and non-blocking send has the sender sends the message and continue. similarly, non-blocking receive has the receiver receive a valid message or null. after a careful analysis, we can come to a conclusion that for a sender it is more natural to be non-blocking after message passing as there may be a need to send the message to different processes. however, the sender expects acknowledgement from the receiver in case the send fails. similarly, it is more natural for a receiver to be blocking after issuing the receive as the information from the received message may be used for further execution. at the same time, if the message send keep on failing, the receiver will have to wait indefinitely. that is why we also consider the other possibility of message passing. there are basically three preferred combinations:
blocking send and blocking receive
non-blocking send and non-blocking receive
non-blocking send and blocking receive (mostly used)
in direct message passing, the process which want to communicate must explicitly name the recipient or sender of communication.
e.g. send(p, message) means send the message to p.
similarly, receive(p, message) means receive the message from p.
in this method of communication, the communication link gets established automatically, which can be either unidirectional or bidirectional, but one link can be used between one pair of the sender and receiver and one pair of sender and receiver should not possess more than one pair of links. symmetry and asymmetry between sending and receiving can also be implemented i.e. either both process will name each other for sending and receiving the messages or only the sender will name receiver for sending the message and there is no need for receiver for naming the sender for receiving the message. the problem with this method of communication is that if the name of one process changes, this method will not work.
in indirect message passing, processes use mailboxes (also referred to as ports) for sending and receiving messages. each mailbox has a unique id and processes can communicate only if they share a mailbox. link established only if processes share a common mailbox and a single link can be associated with many processes. each pair of processes can share several communication links and these links may be unidirectional or bi-directional. suppose two process want to communicate though indirect message passing, the required operations are: create a mail box, use this mail box for sending and receiving messages, then destroy the mail box. the standard primitives used are: send(a, message) which means send the message to mailbox a. the primitive for the receiving the message also works in the same way e.g. received (a, message). there is a problem in this mailbox implementation. suppose there are more than two processes sharing the same mailbox and suppose the process p sends a message to the mailbox, which process will be the receiver? this can be solved by either enforcing that only two processes can share a single mailbox or enforcing that only one process is allowed to execute the receive at a given time or select any process randomly and notify the sender about the receiver. a mailbox can be made private to a single sender/receiver pair and can also be shared between multiple sender/receiver pairs. port is an implementation of such mailbox which can have multiple sender and single receiver. it is used in client/server applications (in this case the server is the receiver). the port is owned by the receiving process and created by os on the request of the receiver process and can be destroyed either on request of the same receiver process or when the receiver terminates itself. enforcing that only one process is allowed to execute the receive can be done using the concept of mutual exclusion. mutex mailbox is create which is shared by n process. sender is non-blocking and sends the message. the first process which executes the receive will enter in the critical section and all other processes will be blocking and will wait.
examples of ipc systems
posix : uses shared memory method. mach : uses message passing windows xp : uses message passing using local procedural calls
communication in client/server architecture:
there are various mechanism:
pipe
socket
remote procedural calls (rpcs)
the above three methods will be discussed in later articles as all of them are quite conceptual and deserve their own separate articles.
prerequisite – inter process communication,
inter-process communication (ipc) is set of interfaces, which is usually programmed in order for the programs to communicate between series of processes. this allows running programs concurrently in an operating system. these are the methods in ipc:
pipes (same process) –
this allows flow of data in one direction only. analogous to simplex systems (keyboard). data from the output is usually buffered until input process receives it which must have a common origin. names pipes (different processes) –
this is a pipe with a specific name it can be used in processes that don’t have a shared common process origin. e.g. is fifo where the details written to a pipe is first named. message queuing –
this allows messages to be passed between processes using either a single queue or several message queue. this is managed by system kernel these messages are coordinated using an api. semaphores –
this is used in solving problems associated with synchronization and to avoid race condition. these are integer values which are greater than or equal to . shared memory –
this allows the interchange of data through a defined area of memory. semaphore values have to be obtained before data can get access to shared memory. sockets –
this method is mostly used to communicate over a network between a client and a server. it allows for a standard connection which is computer and os independent.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : shreyashagrawal
inter process communication through shared memory is a concept where two or more process can access the common memory. and communication is done via this shared memory where changes made by one process can be viewed by another process.
the problem with pipes, fifo and message queue – is that for two process to exchange information. the information has to go through the kernel.
prerequisite : inter process communication
a message queue is a linked list of messages stored within the kernel and identified by a message queue identifier. a new queue is created or an existing queue opened by msgget().
new messages are added to the end of a queue by msgsnd(). every message has a positive long integer type field, a non-negative length, and the actual data bytes (corresponding to the length), all of which are specified to msgsnd() when the message is added to a queue. messages are fetched from a queue by msgrcv(). we don’t have to fetch the messages in a first-in, first-out order. instead, we can fetch messages based on their type field.
all processes can exchange information through access to a common system message queue. the sending process places a message (via some (os) message-passing module) onto a queue which can be read by another process. each message is given an identification or type so that processes can select the appropriate message. process must share a common key in order to gain access to the queue in the first place.
system calls used for message queues:
ftok(): is use to generate a unique key. msgget(): either returns the message queue identifier for a newly created message queue or returns the identifiers for a queue which exists with the same key value. msgsnd(): data is placed on to a message queue by calling msgsnd(). msgrcv(): messages are retrieved from a queue. msgctl(): it performs various operations on a queue. generally it is use to destroy message queue.
message queue for writer process
" , message.mesg_text); return ; }
message queue for reader process
" , message.mesg_text); msgctl(msgid, ipc_rmid, null); return ; }
output:
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
prerequisites – cloud computing, load balancing in cloud computing, inter-process communication
in the development of models and technologies, message abstraction is a necessary aspect that enables distributed computing. distributed system is defined as a system in which components reside at networked communication and synchronise its functions only by movement of messages. in this, message recognizes any discrete data that is moved from one entity to another. it includes any kind of data representation having restriction of size and time, whereas it invokes a remote procedure or a sequence of object instance or a common message. this is the reason that “message-based communication model” can be beneficial to refer various model for inter-process communication, which is based on the data streaming abstraction.
various distributed programming model use this type of communication despite of the abstraction which is shown to developers for programming the co-ordination of shared components. below are some major distributed programming models that uses “message-based communication model”.
message passing –
this model explores the keys of procedure call beyond the restrictions of a single process, thus pointing the execution of program in remote processes. in this, primary client-server is implied. a remote process maintains a server component, thus enabling client processes to invoke the approaches and returns the output of the execution. messages, created by the remote procedure call (rpc) implementation, retrieve the information of the procedure itself and that procedure is to execute having necessary arguments and also returns the values. the use of messages regarding this referred as marshal-ling of the arguments and return values.
this model explores the keys of procedure call beyond the restrictions of a single process, thus pointing the execution of program in remote processes. in this, primary client-server is implied. a remote process maintains a server component, thus enabling client processes to invoke the approaches and returns the output of the execution. messages, created by the remote procedure call (rpc) implementation, retrieve the information of the procedure itself and that procedure is to execute having necessary arguments and also returns the values. the use of messages regarding this referred as marshal-ling of the arguments and return values. distributed objects –
it is an implementation of remote procedure call (rpc) model for the object-oriented model, and contextualizes this for the remote invocation of methods extended by objects. every process assigns a set of interfaces which are accessible remotely. client process can demand a reference to these interfaces and invoke the methods available through them. the basic runtime infrastructure is in transformation the local method calls into a request to a remote process and collecting the result of the execution. the interaction within the caller and the remote process is done trough messages. this model is stateless by design, distributed object models introduce the complexity of object state management and lifetime. common object request broker architecture (corba), component object model (com, dcom and com+), java remote method invocation (rmi), and .net remoting are some major examples which falls under distributed object infrastructure.
it is an implementation of remote procedure call (rpc) model for the object-oriented model, and contextualizes this for the remote invocation of methods extended by objects. every process assigns a set of interfaces which are accessible remotely. client process can demand a reference to these interfaces and invoke the methods available through them. the basic runtime infrastructure is in transformation the local method calls into a request to a remote process and collecting the result of the execution. the interaction within the caller and the remote process is done trough messages. this model is stateless by design, distributed object models introduce the complexity of object state management and lifetime. common object request broker architecture (corba), component object model (com, dcom and com+), java remote method invocation (rmi), and .net remoting are some major examples which falls under distributed object infrastructure. active objects –
programming models based on active objects comprise by definition the presence of instances, whether they are agent of objects, despite the availability of requests. it means, that objects having particular control thread, which enables them to convey their activity. these models sometime make manual use of messages to encounter the execution of functions and a more complex and a more complex semantics is attached to the messages.
programming models based on active objects comprise by definition the presence of instances, whether they are agent of objects, despite the availability of requests. it means, that objects having particular control thread, which enables them to convey their activity. these models sometime make manual use of messages to encounter the execution of functions and a more complex and a more complex semantics is attached to the messages. web services –
web service technology delivers an approach of the rpc concept over the  thus enabling the communication of components that are evolved with numerous technologies. a web service is revealed as a remote object maintained on a web server, and method invocations are transformed in http requests wrapped with the help of specific protocol. it is necessary to observe that the concept of message is a basic abstraction of inter-process communication and it is utilized either implicitly or explicitly.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
prerequisite : c signal handling
in this post, the communication between child and parent processes is done using kill() and signal(), fork() system call.
fork() creates the child process from the parent. the pid can be checked to decide whether it is the child (if pid == ) or the parent (pid = child process id).
creates the child process from the parent. the pid can be checked to decide whether it is the child (if pid == ) or the parent (pid = child process id). the parent can then send messages to child using the pid and kill().
the child picks up these signals with signal() and calls appropriate functions.
example of how  processes can talk to each other using kill() and signal():
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
prerequisite: process-synchronization, mutex vs semaphore
semaphore was proposed by dijkstra in  which is a very significant technique to manage concurrent processes by using a simple integer value, which is known as a semaphore. semaphore is simply a variable which is non-negative and shared between threads. this variable is used to solve the critical section problem and to achieve process synchronization in the multiprocessing environment.
semaphores are of two types:
binary semaphore – this is also known as mutex lock. it can have only two values –  and . its value is initialized to . it is used to implement the solution of critical section problem with multiple processes. counting semaphore – its value can range over an unrestricted domain. it is used to control access to a resource that has multiple instances.
now let us see how it do so.
first, look at two operations which can be used to access and change the value of the semaphore variable.
some point regarding p and v operation
p operation is also called wait, sleep or down operation and v operation is also called signal, wake-up or up operation. both operations are atomic and semaphore(s) is always initialized to one.here atomic means that variable on which read, modify and update happens at the same time/moment with no pre-emption i.e. in between read, modify and update no other operation is performed that may change the variable. a critical section is surrounded by both operations to implement process synchronization.see below image.critical section of process p is in between p and v operation.
now, let us see how it implements mutual exclusion. let there be two processes p and p and a semaphore s is initialized as . now if suppose p enters in its critical section then the value of semaphore s becomes . now if p wants to enter its critical section then it will wait until s > , this can only happen when p finishes its critical section and calls v operation on semaphore s. this way mutual exclusion is achieved. look at the below image for details which is binary semaphore.
implementation of binary semaphores:
the description above is for binary semaphore which can take only two values  and  and ensure the mutual exclusion. there is one other type of semaphore called counting semaphore which can take values greater than one.
now suppose there is a resource whose number of instance is . now we initialize s =  and rest is same as for binary semaphore. whenever process wants that resource it calls p or wait function and when it is done it calls v or signal function. if the value of s becomes zero then a process has to wait until s becomes positive. for example, suppose there are  process p, p, p, p and they all call wait operation on s(initialized with ). if another process p wants the resource then it should wait until one of the four processes calls signal function and value of semaphore becomes positive.
limitations
one of the biggest limitation of semaphore is priority inversion. deadlock, suppose a process is trying to wake up another process which is not in sleep state.therefore a deadlock may block indefinitely. the operating system has to keep track of all calls to wait and to signal the semaphore.
problem in this implementation of semaphore
whenever any process waits then it continuously checks for semaphore value (look at this line while (s==); in p operation) and waste cpu cycle. to avoid this another implementation is provided below.
implementation of counting semaphore
in this implementation whenever process waits it is added to a waiting queue of processes associated with that semaphore. this is done through system call block() on that process. when a process is completed it calls signal function and one process in the queue is resumed. it uses wakeup() system call.
this article is contributed by ashish sharma. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
what are the differences between mutex vs semaphore? when to use mutex and when to use semaphore?
concrete understanding of operating system concepts is required to design/develop smart applications. our objective is to educate the reader on these concepts and learn from other expert geeks.
as per operating system terminology, mutex and semaphore are kernel resources that provide synchronization services (also called as synchronization primitives). why do we need such synchronization primitives? won’t be only one sufficient? to answer these questions, we need to understand few keywords. please read the posts on atomicity and critical section. we will illustrate with examples to understand these concepts well, rather than following usual os textual description.
the producer-consumer problem:
note that the content is generalized explanation. practical details vary with implementation.
consider the standard producer-consumer problem. assume, we have a buffer of  byte length. a producer thread collects the data and writes it to the buffer. a consumer thread processes the collected data from the buffer. objective is, both the threads should not run at the same time.
using mutex:
a mutex provides mutual exclusion, either producer or consumer can have the key (mutex) and proceed with their work. as long as the buffer is filled by producer, the consumer needs to wait, and vice versa.
at any point of time, only one thread can work with the entire buffer. the concept can be generalized using semaphore.
using semaphore:
a semaphore is a generalized mutex. in lieu of single buffer, we can split the  kb buffer into four  kb buffers (identical resources). a semaphore can be associated with these four buffers. the consumer and producer can work on different buffers at the same time.
misconception:
there is an ambiguity between binary semaphore and mutex. we might have come across that a mutex is binary semaphore. but they are not! the purpose of mutex and semaphore are different. may be, due to similarity in their implementation a mutex would be referred as binary semaphore.
strictly speaking, a mutex is locking mechanism used to synchronize access to a resource. only one task (can be a thread or process based on os abstraction) can acquire the mutex. it means there is ownership associated with mutex, and only the owner can release the lock (mutex).
semaphore is signaling mechanism (“i am done, you can carry on” kind of signal). for example, if you are listening songs (assume it as one task) on your mobile and at the same time your friend calls you, an interrupt is triggered upon which an interrupt service routine (isr) signals the call processing task to wakeup.
general questions:
. can a thread acquire more than one lock (mutex)?
yes, it is possible that a thread is in need of more than one resource, hence the locks. if any lock is not available the thread will wait (block) on the lock.
. can a mutex be locked more than once?
a mutex is a lock. only one state (locked/unlocked) is associated with it. however, a recursive mutex can be locked more than once (posix complaint systems), in which a count is associated with it, yet retains only one state (locked/unlocked). the programmer must unlock the mutex as many number times as it was locked.
. what happens if a non-recursive mutex is locked more than once.
deadlock. if a thread which had already locked a mutex, tries to lock the mutex again, it will enter into the waiting list of that mutex, which results in deadlock. it is because no other thread can unlock the mutex. an operating system implementer can exercise care in identifying the owner of mutex and return if it is already locked by same thread to prevent deadlocks.
. are binary semaphore and mutex same?
no. we suggest to treat them separately, as it is explained signalling vs locking mechanisms. but a binary semaphore may experience the same critical issues (e.g. priority inversion) associated with mutex. we will cover these in later article.
a programmer can prefer mutex rather than creating a semaphore with count .
. what is a mutex and critical section?
some operating systems use the same word critical section in the api. usually a mutex is costly operation due to protection protocols associated with it. at last, the objective of mutex is atomic access. there are other ways to achieve atomic access like disabling interrupts which can be much faster but ruins responsiveness. the alternate api makes use of disabling interrupts.
. what are events?
the semantics of mutex, semaphore, event, critical section, etc… are same. all are synchronization primitives. based on their cost in using them they are different. we should consult the os documentation for exact details.
. can we acquire mutex/semaphore in an interrupt service routine?
an isr will run asynchronously in the context of current running thread. it is not recommended to query (blocking call) the availability of synchronization primitives in an isr. the isr are meant be short, the call to mutex/semaphore may block the current running thread. however, an isr can signal a semaphore or unlock a mutex.
. what we mean by “thread blocking on mutex/semaphore” when they are not available?
every synchronization primitive has a waiting list associated with it. when the resource is not available, the requesting thread will be moved from the running list of processor to the waiting list of the synchronization primitive. when the resource is available, the higher priority thread on the waiting list gets the resource (more precisely, it depends on the scheduling policies).
. is it necessary that a thread must block always when resource is not available?
for example posix pthread_mutex_trylock() api. when mutex is not available the function returns immediately whereas the api pthread_mutex_lock() blocks the thread till resource is available.
the monitor is one of the ways to achieve process synchronization. the monitor is supported by programming languages to achieve mutual exclusion between processes. for example java synchronized methods. java provides wait() and notify() constructs.
syntax:
condition variables:
two different operations are performed on the condition variables of the monitor.
wait. signal.
let say we have  condition variables
condition x, y; // declaring variable
wait operation
x.wait() : process performing wait operation on any condition variable are suspended. the suspended processes are placed in block queue of that condition variable.
note: each condition variable has its unique block queue.
signal operation
x.signal(): when a process performs signal operation on condition variable, one of the blocked processes is given chance.
if (x block queue empty) // ignore signal else // resume a process from block queue.
advantages of monitor:
monitors have the advantage of making parallel programming easier and less error prone than using techniques such as semaphore.
disadvantages of monitor:
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : shivanshukumarsingh
problem: given  processes i and j, you need to write a program that can guarantee mutual exclusion between the two without any additional hardware support.
solution: there can be multiple ways to solve this problem, but most of them require additional hardware support. the simplest and the most popular way to do this is by using peterson’s algorithm for mutual exclusion. it was developed by peterson in  though the initial work in this direction was done by theodorus jozef dekker who came up with dekker’s algorithm in , which was later refined by peterson and came to be known as peterson’s algorithm.
basically, peterson’s algorithm provides guaranteed mutual exclusion by using only the shared memory. it uses two ideas in the algorithm:
willingness to acquire lock. turn to acquire lock.
prerequisite: multithreading in c
explanation:
the idea is that first a thread expresses its desire to acquire a lock and sets flag[self] =  and then gives the other thread a chance to acquire the lock. if the thread desires to acquire the lock, then, it gets the lock and passes the chance to the st thread. if it does not desire to get the lock then the while loop breaks and the st thread gets the chance.
implementation in c language
" , self); lock(self); for (i=; i<max; i++) ans++; unlock(self); } int main() { pthread_t p, p; lock_init(); pthread_create(&p, null, func, ( void *)); pthread_create(&p, null, func, ( void *)); pthread_join(p, null); pthread_join(p, null); printf ( "actual count: %d | expected count: %d
output:
thread entered:  thread entered:  actual count:  | expected count: 
the produced output is * where  is incremented by both threads.
this article is contributed by pinkesh badjatiya . if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : pigeonlord
problem: given  process i and j, you need to write a program that can guarantee mutual exclusion between the two without any additional hardware support.
we strongly recommend to refer below basic solution discussed in previous article.
peterson’s algorithm for mutual exclusion | set 
we would be resolving  issues in the previous algorithm.
wastage of cpu clock cycles
in layman terms, when a thread was waiting for its turn, it ended in a long while loop which tested the condition millions of times per second thus doing unnecessary computation. there is a better way to wait, and it is known as “yield”.
to understand what it does, we need to dig deep into how the process scheduler works in linux. the idea mentioned here is a simplified version of the scheduler, the actual implementation has lots of complications.
consider the following example,
this is a complete waste of the  cpu clock cycles. to avoid this, we mutually give up the cpu time slice, i.e. yield, which essentially ends this time slice and the scheduler picks up the next process to run. now, we test our condition once, then we give up the cpu. considering our test takes  clock cycles, we save % of our computation in a time slice. to put this graphically,
considering the processor clock speed as mhz this is a lot of saving!.
different distributions provide different function to achieve this functionality. linux provides sched_yield().
memory fence.
consider this example,
so the order of statements,
flag[self] = ;
turn = -self;
while (turn condition check)
yield();
has to be exactly the same in order for the lock to work, otherwise it will end up in a deadlock condition.
to ensure this, compilers provide a instruction that prevent ordering of statements across this barrier. in case of gcc, its __sync_synchronize().
full implementation in c:
" ,self); lock(self); for (i=; i<max; i++) ans++; unlock(self); } int main() { pthread_t p, p; lock_init(); pthread_create(&p, null, func, ( void *)); pthread_create(&p, null, func, ( void *)); pthread_join(p, null); pthread_join(p, null); printf ( "actual count: %d | expected count:" " %d
" ,ans,max*); return ; }
output:
thread entered:  thread entered:  actual count:  | expected count: 
this article is contributed by pinkesh badjatiya . if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
prerequisite – synchronization, critical section
problem:the producer consumer problem (or bounded buffer problem) describes two processes, the producer and the consumer, which share a common, fixed-size buffer used as a queue. producer produce an item and put it into buffer. if buffer is already full then producer will have to wait for an empty block in buffer. consumer consume an item from buffer. if buffer is already empty then consumer will have to wait for an item in buffer. implement peterson’s algorithm for the two processes using shared memory such that there is mutual exclusion between them. the solution should have free from synchronization problems.
peterson’s algorithm –
explanation of peterson’s algorithm –
peterson’s algorithm is used to synchronize two processes. it uses two variables, a bool array flag of size  and an int variable turn to accomplish it.
in the solution i represents the consumer and j represents the producer. initially the flags are false. when a process wants to execute it’s critical section, it sets it’s flag to true and turn as the index of the other process. this means that the process wants to execute but it will allow the other process to run first. the process performs busy waiting until the other process has finished it’s own critical section.
after this the current process enters it’s critical section and adds or removes a random number from the shared buffer. after completing the critical section, it sets it’s own flag to false, indication it does not wish to execute anymore.
the program runs for a fixed amount of time before exiting. this time can be changed by changing value of the macro rt.
" ); *turn = i; while (flag[i] == true && *turn == i) ; index = ; while (index < bsize) { if (buf[index] == ) { int tempo = myrand(bsize * ); printf ( "job %d has been produced
" , tempo); buf[index] = tempo; break ; } index++; } if (index == bsize) printf ( "buffer is full, nothing can be produced!!!
" ); printf ( "buffer: " ); index = ; while (index < bsize) printf ( "%d " , buf[index++]); printf ( "
" ); flag[j] = false ; if (*state == ) break ; wait_time = myrand(pwt); printf ( "producer will wait for %d seconds
" , wait_time); sleep(wait_time); } exit (); } if (fork() == ) { shm = ( bool *)shmat(shmid, null, ); shm = ( int *)shmat(shmid, null, ); shm = ( int *)shmat(shmid, null, ); if (shm == ( bool *)- || shm == ( int *)- || shm == ( int *)-) { perror ( "consumer shmat error:" ); exit (); } bool * flag = shm; int * turn = shm; int * buf = shm; int index = ; flag[i] = false ; sleep(); while (*state == ) { flag[i] = true ; printf ( "consumer is ready now.
" ); *turn = j; while (flag[j] == true && *turn == j) ; if (buf[] != ) { printf ( "job %d has been consumed
" , buf[]); buf[] = ; index = ; while (index < bsize) { buf[index - ] = buf[index]; index++; } buf[index - ] = ; } else printf ( "buffer is empty, nothing can be consumed!!!
" ); printf ( "buffer: " ); index = ; while (index < bsize) printf ( "%d " , buf[index++]); printf ( "
" ); flag[i] = false ; if (*state == ) break ; wait_time = myrand(cwt); printf ( "consumer will sleep for %d seconds
" , wait_time); sleep(wait_time); } exit (); } while () { gettimeofday(&t, null); t = t.tv_sec; if (t - t > rt) { *state = ; break ; } } wait(); wait(); printf ( "the clock ran out.
" ); return ; }
output:
producer is ready now. job  has been produced buffer:         producer will wait for  seconds producer is ready now. job  has been produced buffer:         producer will wait for  seconds producer is ready now. job  has been produced buffer:         producer will wait for  seconds producer is ready now. job  has been produced buffer:         producer will wait for  seconds consumer is ready now. job  has been consumed buffer:         consumer will sleep for  seconds producer is ready now. job  has been produced buffer:         producer will wait for  seconds producer is ready now. job  has been produced buffer:         producer will wait for  seconds producer is ready now. job  has been produced buffer:         producer will wait for  seconds producer is ready now. job  has been produced buffer:         producer will wait for  seconds producer is ready now. job  has been produced buffer:         producer will wait for  seconds the clock ran out.
this article is contributed by nabaneet roy. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : akanksha_rai
prerequisite – process synchronization, inter process communication
to obtain such a mutual exclusion, bounded waiting, and progress there have been several algorithms implemented, one of which is dekker’s algorithm. to understand the algorithm let’s understand the solution to the critical section problem first.
a process is generally represented as :
do { //entry section critical section //exit section remainder section } while (true);
the solution to critical section problem must ensure following three conditions:
mutual exclusion progress bounded waiting
one of solution for ensuring above all factors is peterson’s solution.
another one is dekker’s solution. dekker’s algorithm was the first provably-correct solution to the critical section problem. it allows two threads to share a single-use resource without conflict, using only shared memory for communication. it avoids the strict alternation of a naïve turn-taking algorithm, and was one of the first mutual exclusion algorithms to be invented.
although there are many versions of dekker’s solution, the final or th version is the one that satisfies all of the above conditions and is the most efficient of them all.
note – dekker’s solution, mentioned here, ensures mutual exclusion between two processes only, it could be extended to more than two processes with the proper use of arrays and variables.
algorithm – it requires both an array of boolean values and an integer variable:
var flag: array [..] of boolean; turn: ..; repeat flag[i] := true; while flag[j] do if turn = j then begin flag[i] := false; while turn = j do no-op; flag[i] := true; end; critical section turn := j; flag[i] := false; remainder section until false;
first version of dekker’s solution – the idea is to use common or shared thread number between processes and stop the other process from entering its critical section if the shared thread indicates the former one already running.
the problem arising in the above implementation is lockstep synchronization, i.e each thread depends on the other for its execution. if one of the processes completes, then the second process runs, gives access to the completed one and waits for its turn, however, the former process is already completed and would never run to return the access back to the latter one. hence, the second process waits infinitely then.
second version of dekker’s solution – to remove lockstep synchronization, it uses two flags to indicate its current status and updates them accordingly at the entry and exit section.
the problem arising in the above version is mutual exclusion itself. if threads are preempted (stopped) during flag updation ( i.e during current_thread = true ) then, both the threads enter their critical section once the preempted thread is restarted, also the same can be observed at the start itself, when both the flags are false.
third version of dekker’s solution – to re-ensure mutual exclusion, it sets the flags before entry section itself.
the problem with this version is deadlock possibility. both threads could set their flag as true simultaneously and both will wait infinitely later on.
fourth version of dekker’s solution – uses small time interval to recheck the condition, eliminates deadlock and ensures mutual exclusion as well.
the problem with this version is the indefinite postponement. also, random amount of time is erratic depending upon the situation in which the algorithm is being implemented, hence not an acceptable solution in business critical systems.
dekker’s algorithm : final and completed solution – -idea is to use favoured thread notion to determine entry to the critical section. favoured thread alternates between the thread providing mutual exclusion and avoiding deadlock, indefinite postponement or lockstep synchronization.
this version guarantees a complete solution to the critical solution problem.
prerequisite – critical section, process synchronization, inter process communication
the bakery algorithm is one of the simplest known solutions to the mutual exclusion problem for the general case of n process. bakery algorithm is a critical section solution for n processes. the algorithm preserves the first come first serve property.
before entering its critical section, the process receives a number. holder of the smallest number enters the critical section.
if processes pi and pj receive the same number, if i < j pi is served first; else pj is served first.
the numbering scheme always generates numbers in increasing order of enumeration; i.e., , , , , , , , , …
notation – lexicographical order (ticket #, process id #) – firstly the ticket number is compared. if same then the process id is compared next, i.e.-
– (a, b) = a[i] for i = , . . ., n - 
shared data – choosing is an array [..n – ] of boolean values; & number is an array [..n – ] of integer values. both are initialized to false & zero respectively.
repeat choosing[i] := true; number[i] := max(number[], number[], ..., number[n - ])+; choosing[i] := false; for j :=  to n -  do begin while choosing[j] do no-op; while number[j] !=  and (number[j], j) < (number[i], i) do no-op; end; critical section number[i] := ; remainder section until false;
explanation –
firstly the process sets its “choosing” variable to be true indicating its intent to enter critical section. then it gets assigned the highest ticket number corresponding to other processes. then the “choosing” variable is set to false indicating that it now has a new ticket number. this is in-fact the most important and confusing part of the algorithm.
it is actually a small critical section in itself ! the very purpose of the first three lines is that if a process is modifying its ticket value then at that time some other process should not be allowed to check its old ticket value which is now obsolete. this is why inside the for loop before checking ticket value we first make sure that all other processes have the “choosing” variable as false.
after that we proceed to check the ticket values of processes where process with least ticket number/process id gets inside the critical section. the exit section just resets the ticket value to zero.
" , thread , resource); } resource = thread ; printf ( "%d using resource...
output:
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
siddhant-bajaj interested in everything cs/it aspire with my acer aspire r to crack gate avid follower of ravindrababu ravula trying my best to keep right up my alley with competitive coding open source and web development projects i am somewhat good at chess and spend loads of time on geeksforgeeks
prerequisite – semaphores in operating system, inter process communication
producer consumer problem is a classical synchronization problem. we can solve this problem by using semaphores.
a semaphore s is an integer variable that can be accessed only through two standard operations : wait() and signal().
the wait() operation reduces the value of semaphore by  and the signal() operation increases its value by .
wait(s){ while(s<=); // busy waiting s--; } signal(s){ s++; }
semaphores are of two types:
binary semaphore – this is similar to mutex lock but not the same thing. it can have only two values –  and . its value is initialized to . it is used to implement the solution of critical section problem with multiple processes. counting semaphore – its value can range over an unrestricted domain. it is used to control access to a resource that has multiple instances.
problem statement – we have a buffer of fixed size. a producer can produce an item and can place in the buffer. a consumer can pick items and can consume them. we need to ensure that when a producer is placing an item in the buffer, then at the same time consumer should not consume any item. in this problem, buffer is the critical section.
to solve this problem, we need two counting semaphores – full and empty. “full” keeps track of number of items in the buffer at any given time and “empty” keeps track of number of unoccupied slots.
initialization of semaphores –
mutex = 
full =  // initially, all slots are empty. thus full slots are 
empty = n // all slots are empty initially
solution for producer –
do{ //produce an item wait(empty); wait(mutex); //place in buffer signal(mutex); signal(full); }while(true)
when producer produces an item then the value of “empty” is reduced by  because one slot will be filled now. the value of mutex is also reduced to prevent consumer to access the buffer. now, the producer has placed the item and thus the value of “full” is increased by . the value of mutex is also increased by  beacuse the task of producer has been completed and consumer can access the buffer.
solution for consumer –
do{ wait(full); wait(mutex); // remove item from buffer signal(mutex); signal(empty); // consumes item }while(true)
as the consumer is removing an item from buffer, therefore the value of “full” is reduced by  and the value is mutex is also reduced so that the producer cannot access the buffer at this moment. now, the consumer has consumed the item, thus increasing the value of “empty” by . the value of mutex is also increased so that producer can access the buffer now.
see for implementation – producer-consumer solution using semaphores in java | set 
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : shikhindahikar
prerequisite – process synchronization, semaphores, dining-philosophers solution using monitors
the dining philosopher problem – the dining philosopher problem states that k philosophers seated around a circular table with one chopstick between each pair of philosophers. there is one chopstick between each philosopher. a philosopher may eat if he can pickup the two chopsticks adjacent to him. one chopstick may be picked up by any one of its adjacent followers but not both.
semaphore solution to dining philosopher –
process p[i] while true do { think; pickup(chopstick[i], chopstick[i+ mod ]); eat; putdown(chopstick[i], chopstick[i+ mod ]) }
there are three states of philosopher : thinking, hungry and eating. here there are two semaphores : mutex and a semaphore array for the philosophers. mutex is used such that no two philosophers may access the pickup or putdown at the same time. the array is used to control the behavior of each philosopher. but, semaphores can result in deadlock due to programming errors.
" , phnum + , left + , phnum + ); printf ( "philosopher %d is eating
" , phnum + ); sem_post(&s[phnum]); } } void take_fork( int phnum) { sem_wait(&mutex); state[phnum] = hungry; printf ( "philosopher %d is hungry
" , phnum + ); test(phnum); sem_post(&mutex); sem_wait(&s[phnum]); sleep(); } void put_fork( int phnum) { sem_wait(&mutex); state[phnum] = thinking; printf ( "philosopher %d putting fork %d and %d down
" , phnum + , left + , phnum + ); printf ( "philosopher %d is thinking
" , phnum + ); test(left); test(right); sem_post(&mutex); } void * philospher( void * num) { while () { int * i = num; sleep(); take_fork(*i); sleep(); put_fork(*i); } } int main() { int i; pthread_t thread_id[n]; sem_init(&mutex, , ); for (i = ; i < n; i++) sem_init(&s[i], , ); for (i = ; i < n; i++) { pthread_create(&thread_id[i], null, philospher, &phil[i]); printf ( "philosopher %d is thinking
note – the below program may compile only with c compilers with semaphore and pthread library.
prerequisite: monitor, process synchronization
dining-philosophers problem – n philosophers seated around a circular table
there is one chopstick between each philosopher
a philosopher must pick up its two nearest chopsticks in order to eat
a philosopher must pick up first one chopstick, then the second one, not both at once
we need an algorithm for allocating these limited resources(chopsticks) among several processes(philosophers) such that solution is free from deadlock and free from starvation.
there exist some algorithm to solve dining – philosopher problem, but they may have deadlock situation. also, a deadlock-free solution is not necessarily starvation-free. semaphores can result in deadlock due to programming errors. monitors alone are not sufficiency to solve this, we need monitors with condition variables
monitor-based solution to dining philosophers
we illustrate monitor concepts by presenting a deadlock-free solution to the dining-philosophers problem. monitor is used to control access to state variables and condition variables. it only tells when to enter and exit the segment. this solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are available.
thinking – when philosopher doesn’t want to gain access to either fork.
hungry – when philosopher wants to enter the critical section.
eating – when philosopher has got both the forks, i.e., he has entered the section.
philosopher i can set the variable state[i] = eating only if her two neighbors are not eating
(state[(i+) % ] != eating) and (state[(i+) % ] != eating).
above program is a monitor solution to the dining-philosopher problem.
we also need to declare
condition self[];
this allows philosopher i to delay herself when she is hungry but is unable to obtain the chopsticks she needs. we are now in a position to describe our solution to the dining-philosophers problem. the distribution of the chopsticks is controlled by the monitor dining philosophers. each philosopher, before starting to eat, must invoke the operation pickup(). this act may result in the suspension of the philosopher process. after the successful completion of the operation, the philosopher may eat. following this, the philosopher invokes the putdown() operation. thus, philosopher i must invoke the operations pickup() and putdown() in the following sequence:
diningphilosophers.pickup(i); ... eat ... diningphilosophers.putdown(i);
it is easy to show that this solution ensures that no two neighbors are eating simultaneously and that no deadlocks will occur. we note, however, that it is possible for a philosopher to starve to death.
this article is contributed by mayank rana. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : palak jain 
consider a situation where we have a file shared between many people.
if one of the people tries editing the file, no other person should be reading or writing at the same time, otherwise changes will not be visible to him/her.
however if some person is reading the file, then others may read it at the same time.
precisely in os we call this situation as the readers-writers problem
problem parameters:
one set of data is shared among a number of processes
once a writer is ready, it performs its write. only one writer may write at a time
if a process is writing, no other process can read it
if at least one reader is reading, no other process can write
readers may not write and only read
solution when reader has the priority over writer
here priority means, no reader should wait if the share is currently opened for reading.
three variables are used: mutex, wrt, readcnt to implement solution
semaphore mutex, wrt; // semaphore mutex is used to ensure mutual exclusion when readcnt is updated i.e. when any reader enters or exit from the critical section and semaphore wrt is used by both readers and writers int readcnt; // readcnt tells the number of processes performing read in the critical section, initially 
functions for sempahore :
– wait() : decrements the semaphore value.
– signal() : increments the semaphore value.
writer process:
writer requests the entry to critical section. if allowed i.e. wait() gives a true value, it enters and performs the write. if not allowed, it keeps on waiting. it exits the critical section.
do { // writer requests for critical section wait(wrt); // performs the write // leaves the critical section signal(wrt); } while(true);
reader process:
reader requests the entry to critical section. if allowed: it increments the count of number of readers inside the critical section. if this reader is the first reader entering, it locks the wrt semaphore to restrict the entry of writers if any reader is inside.
semaphore to restrict the entry of writers if any reader is inside. it then, signals mutex as any other reader is allowed to enter while others are already reading.
after performing reading, it exits the critical section. when exiting, it checks if no more reader is inside, it signals the semaphore “wrt” as now, writer can enter the critical section. if not allowed, it keeps on waiting.
do { // reader wants to enter the critical section wait(mutex); // the number of readers has now increased by  readcnt++; // there is atleast one reader in the critical section // this ensure no writer can enter if there is even one reader // thus we give preference to readers here if (readcnt==) wait(wrt); // other readers can enter while this current reader is inside // the critical section signal(mutex); // current reader performs reading here wait(mutex); // a reader wants to leave readcnt--; // that is, no reader is left in the critical section, if (readcnt == ) signal(wrt); // writers can enter signal(mutex); // reader leaves } while(true);
thus, the semaphore ‘wrt‘ is queued on both readers and writers in a manner such that preference is given to readers if writers are also there. thus, no reader is waiting simply because a writer has requested to enter the critical section.
article contributed by ekta goel. please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : ankit kumar singh 
prerequisite – process synchronization, monitors, readers-writers problem
considering a shared database our objectives are:
readers can access database only when there are no writers.
writers can access database only when there are no readers or writers.
only one thread can manipulate the state variables at a time.
basic structure of a solution –
reader() wait until no writers access database check out – wake up a waiting writer writer() wait until no active readers or writers access database check out – wake up waiting readers or writer
–now let’s suppose that a writer is active and a mixture of readers and writers now show up.
who should get in next?
–or suppose that a writer is waiting and an endless of stream of readers keep showing up.
would it be fair for them to become active?
so we’ll implement a kind of back-and-forth form of fairness:
once a reader is waiting, readers will get in next.
if a writer is waiting, one writer will get in next.
implementation of the solution using monitors:-
the methods should be executed with mutual exclusion i.e. at each point in time, at most one thread may be executing any of its methods. monitors also provide a mechanism for threads to temporarily give up exclusive access, in order to wait for some condition to be met, before regaining exclusive access and resuming their task. monitors also have a mechanism for signaling other threads that such conditions have been met. so in this implementation only mutual exclusion is not enough. threads attempting an operation may need to wait until some assertion p holds true. while a thread is waiting upon a condition variable, that thread is not considered to occupy the monitor, and so other threads may enter the monitor to change the monitor’s state.
understanding the solution:-
it wants to be fair.
if a writer is waiting, readers queue up. if a reader (or another writer) is active or waiting, writers queue up. this is mostly fair, although once it lets a reader in, it lets all waiting readers in all at once, even if some showed up “after” other waiting writers.
it also takes advantage of the fact that signal is a no-op if nobody is waiting.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
siddhant-bajaj interested in everything cs/it aspire with my acer aspire r to crack gate avid follower of ravindrababu ravula trying my best to keep right up my alley with competitive coding open source and web development projects i am somewhat good at chess and spend loads of time on geeksforgeeks
prerequisite – inter process communication
problem : the analogy is based upon a hypothetical barber shop with one barber. there is a barber shop which has one barber, one barber chair, and n chairs for waiting for customers if there are any to sit on the chair.
if there is no customer, then the barber sleeps in his own chair.
when a customer arrives, he has to wake up the barber.
if there are many customers and the barber is cutting a customer’s hair, then the remaining customers either wait if there are empty chairs in the waiting room or they leave if no chairs are empty.
solution : the solution to this problem includes three semaphores.first is for the customer which counts the number of customers present in the waiting room (customer in the barber chair is not included because he is not waiting). second, the barber  or  is used to tell whether the barber is idle or is working, and the third mutex is used to provide the mutual exclusion which is required for the process to execute. in the solution, the customer has the record of the number of customers waiting in the waiting room if the number of customers is equal to the number of chairs in the waiting room then the upcoming customer leaves the barbershop.
when the barber shows up in the morning, he executes the procedure barber, causing him to block on the semaphore customers because it is initially . then the barber goes to sleep until the first customer comes up.
when a customer arrives, he executes customer procedure the customer acquires the mutex for entering the critical region, if another customer enters thereafter, the second one will not be able to anything until the first one has released the mutex. the customer then checks the chairs in the waiting room if waiting customers are less then the number of chairs then he sits otherwise he leaves and releases the mutex.
if the chair is available then customer sits in the waiting room and increments the variable waiting value and also increases the customer’s semaphore this wakes up the barber if he is sleeping.
at this point, customer and barber are both awake and the barber is ready to give that person a haircut. when the haircut is over, the customer exits the procedure and if there are no customers in waiting room barber sleeps.
algorithm for sleeping barber problem:
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : kumar shubham 
prerequisites – process synchronization
a lock variable provides the simplest synchronization mechanism for processes. some noteworthy points regarding lock variables are-
its a software mechanism implemented in user mode, i.e. no support required from the operating system. its a busy waiting solution (keeps the cpu busy even when its technically waiting). it can be used for more than two processes.
when lock =  implies critical section is vacant (initial value ) and lock =  implies critical section occupied.
entry section - while(lock != ); lock = ; //critical section exit section - lock = ;
here we can see a classic implementation of the reader-writer’s problem. the buffer here is the shared memory and many processes are either trying to read or write a character to it. to prevent any ambiguity of data we restrict concurrent access by using a lock variable. we have also applied a constraint on the number of readers/writers that can have access.
now every synchronization mechanism is judged on the basis of three primary parameters :
mutual exclusion. progress. bounded waiting.
. load lock, r ; (store the value of lock in register r.) . cmp r, # ; (compare the value of register r with .) . jnz step  ; (jump to step  if value of r is not .) . store #, lock ; (set new value of lock as .) enter critical section . store #, lock ; (set the value of lock as  again.)
now let’s suppose that processes p and p are competing for critical section and their sequence of execution be as follows (initial alue of lock = ) –
p executes statement  and gets pre-empted. p executes statement , , ,  and enters critical section and gets pre-empted. p executes statement , ,  and also enters critical section.
here initially the r of process p stores lock value as  but fails to update the lock value as . so when p executes it also finds the lock value as  and enters critical section by setting lock value as . but the real problem arises when p executes again it doesn’t check the updated value of lock. it only checks the previous value stored in r which was  and it enters critical section.
this is only one possible sequence of execution among many others. some may even provide mutual exclusion but we cannot dwell on that. according to murphy’s law “anything that can go wrong will go wrong“. so like all easy things the lock variable synchronization method comes with its fair share of demerits but its a good starting point for us to develop better synchronization algorithms to take care of the problems that we face here.
this article is contributed by siddhant bajaj . if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
prerequisite : multithreading in c
thread synchronization is defined as a mechanism which ensures that two or more concurrent processes or threads do not simultaneously execute some particular program segment known as a critical section. processes’ access to critical section is controlled by using synchronization techniques. when one thread starts executing the critical section (a serialized segment of the program) the other thread should wait until the first thread finishes. if proper synchronization techniques are not applied, it may cause a race condition where the values of variables may be unpredictable and vary depending on the timings of context switches of the processes or threads.
thread synchronization problems
job %d has started
" , counter); for (i = ; i < (xffffffff); i++) ; printf ( "
job %d has finished
" , counter); return null; } int main( void ) { int i = ; int error; while (i < ) { error = pthread_create(&(tid[i]), null, &trythis, null); if (error != ) printf ( "
thread can't be created : [%s]" , strerror (error)); i++; } pthread_join(tid[], null); pthread_join(tid[], null); return ; }
how to compile above program?
to compile a multithreaded program using gcc, we need to link it with the pthreads library. following is the command used to compile the program.
gfg@ubuntu:~/$ gcc filename.c -lpthread
in this example, two threads(jobs) are created and in the start function of these threads, a counter is maintained to get the logs about job number which is started and when it is completed.
output :
job  has started job  has started job  has finished job  has finished
problem: from the last two logs, one can see that the log ‘job  has finished’ is repeated twice while no log for ‘job  has finished’ is seen.
why it has occurred ?
the log ‘job  has started’ is printed just after ‘job  has started’ so it can easily be concluded that while thread  was processing the scheduler scheduled the thread .
if we take the above assumption true then the value of the ‘counter’ variable got incremented again before job  got finished.
so, when job  actually got finished, then the wrong value of counter produced the log ‘job  has finished’ followed by the ‘job  has finished’ for the actual job  or vice versa as it is dependent on scheduler.
so we see that its not the repetitive log but the wrong value of the ‘counter’ variable that is the problem.
the actual problem was the usage of the variable ‘counter’ by a second thread when the first thread was using or about to use it.
in other words, we can say that lack of synchronization between the threads while using the shared resource ‘counter’ caused the problems or in one word we can say that this problem happened due to ‘synchronization problem’ between two threads.
how to solve it ? the most popular way of achieving thread synchronization is by using mutexes. mutex a mutex is a lock that we set before using a shared resource and release after using it.
hence, this system ensures synchronization among the threads while working on shared resources.
int pthread_mutex_init(pthread_mutex_t *restrict mutex, const pthread_mutexattr_t *restrict attr) : creates a mutex, referenced by mutex, with attributes specified by attr. if attr is null, the default mutex attribute (nonrecursive) is used. returned value
if successful, pthread_mutex_init() returns , and the state of the mutex becomes initialized and unlocked.
if unsuccessful, pthread_mutex_init() returns -. int pthread_mutex_lock(pthread_mutex_t *mutex) : locks a mutex object, which identifies a mutex. if the mutex is already locked by another thread, the thread waits for the mutex to become available. the thread that has locked a mutex becomes its current owner and remains the owner until the same thread has unlocked it. when the mutex has the attribute of recursive, the use of the lock may be different. when this kind of mutex is locked multiple times by the same thread, then a count is incremented and no waiting thread is posted. the owning thread must call pthread_mutex_unlock() the same number of times to decrement the count to zero. returned value
if successful, pthread_mutex_lock() returns .
if unsuccessful, pthread_mutex_lock() returns -.
the mutex can be unlocked and destroyed by calling following two functions :the first function releases the lock and the second function destroys the lock so that it cannot be used anywhere in future.
int pthread_mutex_unlock(pthread_mutex_t *mutex) : releases a mutex object. if one or more threads are waiting to lock the mutex, pthread_mutex_unlock() causes one of those threads to return from pthread_mutex_lock() with the mutex object acquired. if no threads are waiting for the mutex, the mutex unlocks with no current owner. when the mutex has the attribute of recursive the use of the lock may be different. when this kind of mutex is locked multiple times by the same thread, then unlock will decrement the count and no waiting thread is posted to continue running with the lock. if the count is decremented to zero, then the mutex is released and if any thread is waiting for it is posted. returned value
if successful, pthread_mutex_unlock() returns .
if unsuccessful, pthread_mutex_unlock() returns - int pthread_mutex_destroy(pthread_mutex_t *mutex) : deletes a mutex object, which identifies a mutex. mutexes are used to protect shared resources. mutex is set to an invalid value, but can be reinitialized using pthread_mutex_init(). returned value
if successful, pthread_mutex_destroy() returns .
if unsuccessful, pthread_mutex_destroy() returns -.
an example to show how mutexes are used for thread synchronization
job %d has started
" , counter); for (i = ; i < (xffffffff); i++) ; printf ( "
job %d has finished
" , counter); pthread_mutex_unlock(&lock); return null; } int main( void ) { int i = ; int error; if (pthread_mutex_init(&lock, null) != ) { printf ( "
mutex init has failed
" ); return ; } while (i < ) { error = pthread_create(&(tid[i]), null, &trythis, null); if (error != ) printf ( "
thread can't be created :[%s]" , strerror (error)); i++; } pthread_join(tid[], null); pthread_join(tid[], null); pthread_mutex_destroy(&lock); return ; }
a mutex is initialized in the beginning of the main function.
the same mutex is locked in the ‘trythis()’ function while using the shared resource ‘counter’.
at the end of the function ‘trythis()’ the same mutex is unlocked.
at the end of the main function when both the threads are done, the mutex is destroyed.
output :
job  started job  finished job  started job  finished
so this time the start and finish logs of both the jobs are present. so thread synchronization took place by the use of mutex.
let us first put ‘priority inversion’ in the context of the big picture i.e. where does this come from.
in operating system, one of the important concepts is task scheduling. there are several scheduling methods such as first come first serve, round robin, priority-based scheduling, etc. each scheduling method has its pros and cons. as you might have guessed, priority inversion comes under priority-based scheduling. basically, it’s a problem which arises sometimes when priority-based scheduling is used by os. in priority-based scheduling, different tasks are given different priority so that higher priority tasks can intervene in lower priority tasks if possible.
so, in priority-based scheduling, if lower priority task (l) is running and if a higher priority task (h) also needs to run, the lower priority task (l) would be preempted by higher priority task (h). now, suppose both lower and higher priority tasks need to share a common resource (say access to the same file or device) to achieve their respective work. in this case, since there are resource sharing and task synchronization is needed, several methods/techniques can be used for handling such scenarios. for sake of our topic on priority inversion, let us mention a synchronization method say mutex. just to recap on the mutex, a task acquires mutex before entering critical section (cs) and releases mutex after exiting critical section (cs). while running in cs, task access this common resource. more details on this can be referred here. now, say both l and h shares a common critical section (cs) i.e. the same mutex is needed for this cs.
coming to our discussion of priority inversion, let us examine some scenarios.
) l is running but not in cs; h needs to run; h preempts l; h starts running; h relinquishes or releases control; l resumes and starts running
) l is running in cs; h needs to run but not in cs; h preempts l; h starts running; h relinquishes control; l resumes and starts running.
) l is running in cs; h also needs to run in cs; h waits for l to come out of cs; l comes out of cs; h enters cs and starts running
now let us add another task of middle priority say m. now the task priorities are in the order of l < m < h. in our example, m doesn’t share the same critical section (cs). in this case, the following sequence of task running would result in ‘priority inversion’ problem.
) l is running in cs ; h also needs to run in cs ; h waits for l to come out of cs ; m interrupts l and starts running ; m runs till completion and relinquishes control ; l resumes and starts running till the end of cs ; h enters cs and starts running.
note that neither l nor h share cs with m.
here, we can see that running of m has delayed the running of both l and h. precisely speaking, h is of higher priority and doesn’t share cs with m; but h had to wait for m. this is where priority based scheduling didn’t work as expected because priorities of m and h got inverted in spite of not sharing any cs. this problem is called priority inversion. this is what the heck was priority inversion! in a system with priority based scheduling, higher priority tasks can face this problem and it can result in unexpected behavior/result. in general purpose os, it can result in slower performance. in rtos, it can result in more severe outcomes. the most famous ‘priority inversion’ problem was what happened at mars pathfinder.
if we have a problem, there has to be solution for this. for priority inversion as well, there’re different solutions such as priority inheritance, etc. this is going to be our next article 🙂
but for the inpatients, this can be referred for time being.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : akanksha_rai
both of these concepts come under priority scheduling in operating system. but are they same ?
in one line, priority inversion is a problem while priority inheritance is a solution. literally, priority inversion means that priority of tasks get inverted and priority inheritance means that priority of tasks get inherited. both of these phenomena happen in priority scheduling. basically, in priority inversion, higher priority task (h) ends up waiting for middle priority task (m) when h is sharing critical section with lower priority task (l) and l is already in critical section. effectively, h waiting for m results in inverted priority i.e. priority inversion. one of the solution for this problem is priority inheritance. in priority inheritance, when l is in critical section, l inherits priority of h at the time when h starts pending for critical section. by doing so, m doesn’t interrupt l and h doesn’t wait for m to finish. please note that inheriting of priority is done temporarily i.e. l goes back to its old priority when l comes out of critical section.
more details on these can be found here.
please do like/tweet/g+ if you find the above useful. also, please do leave us comment for further clarification or info. we would love to help and learn 🙂
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
prerequisite – process synchronization | introduction, critical section, semaphores
process synchronization is a technique which is used to coordinate the process that use shared data. there are two types of processes in an operating systems:-
independent process –
the process that does not affect or is affected by the other process while its execution then the process is called independent process. example the process that does not share any shared variable, database, files, etc. cooperating process –
the process that affect or is affected by the other process while execution, is called a cooperating process. example the process that share file, variable, database, etc are the cooperating process.
process synchronization is mainly used for cooperating process that shares the resources.let us consider an example of
//racing condition image
it is the condition where several processes tries to access the resources and modify the shared data concurrently and outcome of the process depends on the particular order of execution that leads to data inconsistency, this condition is called race condition.this condition can be avoided using the technique called synchronization or process synchronization, in which we allow only one process to enter and manipulates the shared data in critical section.
//diagram of the view of cs
this setup can be defined in various regions like:
entry section –
it is part of the process which decide the entry of a particular process in the critical section, out of many other processes.
it is part of the process which decide the entry of a particular process in the critical section, out of many other processes. critical section –
it is the part in which only one process is allowed to enter and modify the shared variable.this part of the process ensures that only no other process can access the resource of shared data.
it is the part in which only one process is allowed to enter and modify the shared variable.this part of the process ensures that only no other process can access the resource of shared data. exit section –
this process allows the other process that are waiting in the entry section, to enter into the critical sections. it checks that a process that after a process has finished execution in critical section can be removed through this exit section.
this process allows the other process that are waiting in the entry section, to enter into the critical sections. it checks that a process that after a process has finished execution in critical section can be removed through this exit section. remainder section –
critical section problems must satisfy these three requirements:
mutual exclusion –
it states that no other process is allowed to execute in the critical section if a process is executing in critical section. progress –
when no process is in the critical section, then any process from outside that request for execution can enter in the critical section without any delay. only those process can enter that have requested and have finite time to enter the process. bounded waiting –
an upper bound must exist on the number of times a process enters so that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted.
process synchronization are handled by two approaches:
software approach –
in software approach, some specific algorithm approach is used to maintain synchronization of the data. like in approach one or approach two, for a number of two process, a temporary variable like (turn) or boolean variable (flag) value is used to store the data. when the condition is true then the process in waiting state, known as busy waiting state. this does not satisfy all the critical section requirements. another software approach known as peterson’s solution is best for synchronization. it uses two variables in the entry section so as to maintain consistency, like flag (boolean variable) and turn variable(storing the process states). it satisfy all the three critical section requirements. //image of peterson’s algorithm hardware approach –
the hardware approach of synchronization can be done through lock & unlock technique.locking part is done in the entry section, so that only one process is allowed to enter into the critical section, after it complete its execution, the process is moved to the exit section, where unlock operation is done so that another process in the lock section can repeat this process of execution.this process is designed in such a way that all the three conditions of the critical sections are satisfied.
//image of lock
using interrupts –
these are easy to implement.when interrupt are disabled then no other process is allowed to perform context switch operation that would allow only one process to enter into the critical state.
//image of interrupts
test_and_set operation –
this allows boolean value (true/false) as a hardware synchronization, which is atomic in nature i.e no other interrupt is allowed to access.this is mainly used in mutual exclusion application. similar type operation can be achieved through compare and swap function. in this process, a variable is allowed to accessed in critical section while its lock operation is on.till then, the other process is in busy waiting state. hence critical section requirements are achieved.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
kartikeya shukla  loves coding##
prerequisite – inter process communication,
inter-process communication (ipc) is set of interfaces, which is usually programmed in order for the programs to communicate between series of processes. this allows running programs concurrently in an operating system. these are the methods in ipc:
pipes (same process) –
this allows flow of data in one direction only. analogous to simplex systems (keyboard). data from the output is usually buffered until input process receives it which must have a common origin. names pipes (different processes) –
this is a pipe with a specific name it can be used in processes that don’t have a shared common process origin. e.g. is fifo where the details written to a pipe is first named. message queuing –
this allows messages to be passed between processes using either a single queue or several message queue. this is managed by system kernel these messages are coordinated using an api. semaphores –
this is used in solving problems associated with synchronization and to avoid race condition. these are integer values which are greater than or equal to . shared memory –
this allows the interchange of data through a defined area of memory. semaphore values have to be obtained before data can get access to shared memory. sockets –
this method is mostly used to communicate over a network between a client and a server. it allows for a standard connection which is computer and os independent.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : shreyashagrawal
a process in operating systems uses different resources and uses resources in the following way.
) requests a resource
) use the resource
) releases the resource
deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process.
consider an example when two trains are coming toward each other on the same track and there is only one track, none of the trains can move once they are in front of each other. a similar situation occurs in operating systems when there are two or more processes that hold some resources and wait for resources held by other(s). for example, in the below diagram, process  is holding resource  and waiting for resource  which is acquired by process , and process  is waiting for resource .
deadlock can arise if the following four conditions hold simultaneously (necessary conditions)
mutual exclusion: one or more than one resource are non-shareable (only one process can use at a time)
hold and wait: a process is holding at least one resource and waiting for resources.
no preemption: a resource cannot be taken from a process unless the process releases the resource.
circular wait: a set of processes are waiting for each other in circular form.
methods for handling deadlock
there are three ways to handle deadlock
) deadlock prevention or avoidance: the idea is to not let the system into a deadlock state.
one can zoom into each category individually, prevention is done by negating one of above mentioned necessary conditions for deadlock.
avoidance is kind of futuristic in nature. by using strategy of “avoidance”, we have to make an assumption. we need to ensure that all information about resources which process will need are known to us prior to execution of the process. we use banker’s algorithm (which is in-turn a gift from dijkstra) in order to avoid deadlock.
) deadlock detection and recovery: let deadlock occur, then do preemption to handle it once occurred.
) ignore the problem altogether: if deadlock is very rare, then let it happen and reboot the system. this is the approach that both windows and unix take.
exercise:
) suppose n processes, p, …. pn share m identical resource units, which can be reserved and released one at a time. the maximum resource requirement of process pi is si, where si > . which one of the following is a sufficient condition for ensuring that deadlock does not occur? (gate cs )
(a) a
(b) b
(c) c
(d) d
for solution, see question  of 
see quiz on deadlock for more questions.
in the previous post, we have discussed deadlock prevention and avoidance. in this post, deadlock detection and recovery technique to handle deadlock is discussed.
deadlock detection
if resources have single instance:
in this case for deadlock detection we can run an algorithm to check for cycle in the resource allocation graph. presence of cycle in the graph is the sufficient condition for deadlock.
in the above diagram, resource  and resource  have single instances. there is a cycle r → p → r → p. so, deadlock is confirmed. if there are multiple instances of resources:
detection of the cycle is necessary but not sufficient condition for deadlock detection, in this case, the system may or may not be in deadlock varies according to different situations.
deadlock recovery
a traditional operating system such as windows doesn’t deal with deadlock recovery as it is time and space consuming process. real-time operating systems use deadlock recovery.
prerequisite – deadlock and starvation
livelock occurs when two or more processes continually repeat the same interaction in response to changes in the other processes without doing any useful work. these processes are not in the waiting state, and they are running concurrently. this is different from a deadlock because in a deadlock all processes are in the waiting state.
example:
imagine a pair of processes using two resources, as shown:
each of the two processes needs the two resources and they use the polling primitive enter_reg to try to acquire the locks necessary for them. in case the attempt fails, the process just tries again.
if process a runs first and acquires resource  and then process b runs and acquires resource , no matter which one runs next, it will make no further progress, but neither of the two processes blocks. what actually happens is that it uses up its cpu quantum over and over again without any progress being made but also without any sort of blocking. thus this situation is not that of a deadlock( as no process is being blocked) but we have something functionally equivalent to deadlock: livelock.
what leads to livelocks?
occurrence of livelocks can occur in the most surprising of ways. the total number of allowed processes in some systems, is determined by the number of entries in the process table. thus process table slots can be referred to as finite resources. if a fork fails because of the table being full, waiting a random time and trying again would be a reasonable approach for the program doing the fork.
consider a unix system having  process slots. ten programs are running, each of which having to create  (sub)processes. after each process has created  processes, the  original processes and the  new processes have exhausted the table. each of the  original processes now sits in an endless loop forking and failing – which is aptly the situation of a deadlock. the probability of this happening is very little but it could happen.
difference between deadlock, starvation, and livelock:
a livelock is similar to a deadlock, except that the states of the processes involved in the livelock constantly change with regard to one another, none progressing. livelock is a special case of resource starvation; the general definition only states that a specific process is not progressing.
livelock:
deadlock:
a deadlock is a state in which each member of a group of actions, is waiting for some other member to release a lock. a livelock on the other hand is almost similar to a deadlock, except that the states of the processes involved in a livelock constantly keep on changing with regard to one another, none progressing. thus livelock is a special case of resource starvation, as stated from the general definition, the process is not progressing.
starvation:
starvation is a problem which is closely related to both, livelock and deadlock. in a dynamic system, requests for resources keep on happening. thereby, some policy is needed to make a decision about who gets the resource when. this process, being reasonable, may lead to a some processes never getting serviced even though they are not deadlocked.
starvation happens when “greedy” threads make shared resources unavailable for long periods. for instance, suppose an object provides a synchronized method that often takes a long time to return. if one thread invokes this method frequently, other threads that also need frequent synchronized access to the same object will often be blocked.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
deadlock characteristics
as discussed in the previous post, deadlock has following characteristics.
mutual exclusion hold and wait no preemption circular wait
deadlock prevention
we can prevent deadlock by eliminating any of the above four conditions.
eliminate mutual exclusion
it is not possible to dis-satisfy the mutual exclusion because some resources, such as the tape drive and printer, are inherently non-shareable.
eliminate hold and wait
allocate all required resources to the process before the start of its execution, this way hold and wait condition is eliminated but it will lead to low device utilization. for example, if a process requires printer at a later time and we have allocated printer before the start of its execution printer will remain blocked till it has completed its execution. the process will make a new request for resources after releasing the current set of resources. this solution may lead to starvation.
eliminate no preemption
preempt resources from the process when resources required by other high priority processes.
eliminate circular wait
each resource will be assigned with a numerical number. a process can request the resources increasing/decreasing. order of numbering.
for example, if p process is allocated r resources, now next time if p ask for r, r lesser than r such request will not be granted, only request for resources more than r will be granted.
deadlock avoidance
deadlock avoidance can be done with banker’s algorithm.
banker’s algorithm
bankers’s algorithm is resource allocation and deadlock avoidance algorithm which test all the request made by processes for resources, it checks for the safe state, if after granting request system remains in the safe state it allows the request and if there is no safe state it doesn’t allow the request made by the process.
inputs to banker’s algorithm:
max need of resources by each process. currently allocated resources by each process. max free available resources in the system.
the request will only be granted under the below condition:
if the request made by the process is less than equal to max need to that process. if the request made by the process is less than equal to the freely available resource in the system.
example:
total resources in system: a b c d    
available system resources are: a b c d    
processes (currently allocated resources): a b c d p     p     p    
processes (maximum resources): a b c d p     p     p    
need = maximum resources - currently allocated resources. processes (need resources): a b c d p     p     p    
note:deadlock prevention is more strict that deadlock avoidance.
following are gate previous year question
the banker’s algorithm is a resource allocation and deadlock avoidance algorithm that tests for safety by simulating the allocation for predetermined maximum possible amounts of all resources, then makes an “s-state” check to test for possible activities, before deciding whether allocation should be allowed to continue.
why banker’s algorithm is named so?
banker’s algorithm is named so because it is used in banking system to check whether loan can be sanctioned to a person or not. suppose there are n number of account holders in a bank and the total sum of their money is s. if a person applies for a loan then the bank first subtracts the loan amount from the total money that bank has and if the remaining amount is greater than s then only the loan is sanctioned. it is done because if all the account holders comes to withdraw their money then the bank can easily do it.
in other words, the bank would never allocate its money in such a way that it can no longer satisfy the needs of all its customers. the bank would try to be in safe state always.
following data structures are used to implement the banker’s algorithm:
let ‘n’ be the number of processes in the system and ‘m’ be the number of resources types.
available :
it is a -d array of size ‘m’ indicating the number of available resources of each type.
indicating the number of available resources of each type. available[ j ] = k means there are ‘k’ instances of resource type r j
max :
it is a -d array of size ‘ n*m’ that defines the maximum demand of each process in a system.
that defines the maximum demand of each process in a system. max[ i, j ] = k means process p i may request at most ‘k’ instances of resource type r j.
allocation :
it is a -d array of size ‘n*m’ that defines the number of resources of each type currently allocated to each process.
that defines the number of resources of each type currently allocated to each process. allocation[ i, j ] = k means process p i is currently allocated ‘k’ instances of resource type r j
need :
it is a -d array of size ‘n*m’ that indicates the remaining resource need of each process.
that indicates the remaining resource need of each process. need [ i, j ] = k means process p i currently need ‘k’ instances of resource type r j
currently need instances of resource type for its execution. need [ i, j ] = max [ i, j ] – allocation [ i, j ]
allocation i specifies the resources currently allocated to process p i and need i specifies the additional resources that process p i may still request to complete its task.
banker’s algorithm consists of safety algorithm and resource request algorithm
safety algorithm
the algorithm for finding out whether or not a system is in a safe state can be described as follows:
) let work and finish be vectors of length ‘m’ and ‘n’ respectively.
initialize: work = available
finish[i] = false; for i=, , , ….n ) find an i such that both
a) finish[i] = false
b) need i <= work
if no such i exists goto step () ) work = work + allocation[i]
finish[i] = true
goto step () ) if finish [i] = true for all i
then the system is in a safe state
resource-request algorithm
let request i be the request array for process p i . request i [j] = k means process p i wants k instances of resource type r j . when a request for resources is made by process p i , the following actions are taken:
considering a system with five processes p  through p  and three resources of type a, b, c. resource type a has  instances, b has  instances and type c has  instances. suppose at time t  following snapshot of the system has been taken:
question. what will be the content of the need matrix?
need [i, j] = max [i, j] – allocation [i, j]
so, the content of need matrix is:
question. is the system in a safe state? if yes, then what is the safe sequence?
applying the safety algorithm on the given system,
question. what will happen if process p  requests one additional instance of resource type a and two instances of resource type c?
we must determine whether this new system state is safe. to do so, we again execute safety algorithm on the above data structures.
hence the new system state is safe, so we can immediately grant the request for process p  .
as banker’s algorithm using some kind of table like allocation, request, available all that thing to understand what is the state of the system. similarly, if you want to understand the state of the system instead of using those table, actually tables are very easy to represent and understand it, but then still you could even represent the same information in the graph. that graph is called resource allocation graph (rag).
so, resource allocation graph is explained to us what is the state of the system in terms of processes and resources. like how many resources are available, how many are allocated and what is the request of each process. everything can be represented in terms of the diagram. one of the advantages of having a diagram is, sometimes it is possible to see a deadlock directly by using rag, but then you might not be able to know that by looking at the table. but the tables are better if the system contains lots of process and resource and graph is better if the system contains less number of process and resource.
we know that any graph contains vertices and edges. so rag also contains vertices and edges. in rag vertices are two type –
. process vertex – every process will be represented as a process vertex.generally, the process will be represented with a circle.
. resource vertex – every resource will be represented as a resource vertex. it is also two type –
single instance type resource – it represents as a box, inside the box, there will be one dot.so the number of dots indicate how many instances are present of each resource type.
it represents as a box, inside the box, there will be one dot.so the number of dots indicate how many instances are present of each resource type. multi-resource instance type resource – it also represents as a box, inside the box, there will be many dots present.
now coming to the edges of rag.there are two types of edges in rag –
. assign edge – if you already assign a resource to a process then it is called assign edge.
. request edge – it means in future the process might want some resource to complete the execution, that is called request edge.
so, if a process is using a resource, an arrow is drawn from the resource node to the process node. if a process is requesting a resource, an arrow is drawn from the process node to the resource node.
example  (single instances rag) –
if there is a cycle in the resource allocation graph and each resource in the cycle provides only one instance, then the processes will be in deadlock. for example, if process p holds resource r, process p holds resource r and process p is waiting for r and process p is waiting for r, then process p and process p will be in deadlock.
here’s another example, that shows processes p and p acquiring resources r and r while process p is waiting to acquire both resources. in this example, there is no deadlock because there is no circular dependency.
so cycle in single-instance resource type is the sufficient condition for deadlock.
example  (multi-instances rag) –
from the above example, it is not possible to say the rag is in a safe state or in an unsafe state.so to see the state of this rag, let’s construct the allocation matrix and request matrix.
the total number of processes are three; p, p & p and the total number of resources are two; r & r.
allocation matrix – for constructing the allocation matrix, just go to the resources and see to which process it is allocated.
r is allocated to p, therefore write  in allocation matrix and similarly, r is allocated to p as well as p and for the remaining element just write .
request matrix – in order to find out the request matrix, you have to go to the process and see the outgoing edges.
p is requesting resource r, so write  in the matrix and similarly, p requesting r and for the remaining element write . so now available resource is = (, ). checking deadlock (safe or not) – so, there is no deadlock in this rag.even though there is a cycle, still there is no deadlock.therefore in multi-instance resource cycle is not sufficient condition for deadlock. above example is the same as the previous example except that, the process p requesting for resource r.
so the table becomes as shown in below.
so,the available resource is = (, ), but requirement are (, ), (, ) and (, ).so you can’t fulfill any one requirement.therefore, it is in deadlock. therefore, every cycle in a multi-instance resource type graph is not a deadlock, if there has to be a deadlock, there has to be a cycle.so, in case of rag with multi-instance resource type, the cycle is a necessary condition for deadlock, but not sufficient.
gate cs corner questions
practicing the following questions will help you test your knowledge. all questions have been asked in gate in previous years or in gate mock tests. it is highly recommended that you practice them.
reference –
a. silberschatz, p. galvin, g. gagne, “operating systems concepts (th edition)”, wiley india pvt. ltd.
this article is contributed by samit mandal. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
the operating system allocates resources when a program need them. when the program terminates, the resources are de-allocated, and allocated to other programs that need them. now the question is, what strategy does the operating system use to allocate these resources to user programs?
there are two resource allocation techniques:
resource partitioning approach –
in this approach, the operating system decides beforehand, that what resources should be allocated to which user program. it divides the resources in the system to many resource partitions, where each partition may include various resources – for example,  mb memory, disk blocks, and a printer. then, it allocates one resource partition to each user program before the program’s initiation. a resource table records the resource partition and its current allocation status (allocated or free). advantages:
easy to implement
less overhead disadvantages: lacks flexibility – if a resource partition contains more resources than what a particular process requires, the additional resources are wasted.
if a resource partition contains more resources than what a particular process requires, the additional resources are wasted. if a program needs more resources than a single resource partition, it cannot execute (though free resources are present in other partitions). an example resource table may look like: pool based approach –
in this approach, there is a common pool of resources. the operating system checks the allocation status in the resource table whenever a program makes a request for a resource. if the resource is free, it allocates the resource to the program. advantages: allocated resources are not wasted.
any resource requirement can be fulfilled if the resource is free (unlike partitioning approach) disadvantages: overhead of allocating and de-allocating the resources on every request and release.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : harshit kalal
prerequisite: banker’s algorithm
the banker’s algorithm is a resource allocation and deadlock avoidance algorithm that tests for safety by simulating the allocation for predetermined maximum possible amounts of all resources, then makes an “s-state” check to test for possible activities, before deciding whether allocation should be allowed to continue.
following data structures are used to implement the banker’s algorithm:
let ‘n’ be the number of processes in the system and ‘m’ be the number of resources types.
available :
it is a -d array of size ‘m’ indicating the number of available resources of each type.
indicating the number of available resources of each type. available[ j ] = k means there are ‘k’ instances of resource type r j
max :
it is a -d array of size ‘ n*m’ that defines the maximum demand of each process in a system.
that defines the maximum demand of each process in a system. max[ i, j ] = k means process p i may request at most ‘k’ instances of resource type r j.
allocation :
it is a -d array of size ‘n*m’ that defines the number of resources of each type currently allocated to each process.
that defines the number of resources of each type currently allocated to each process. allocation[ i, j ] = k means process p i is currently allocated ‘k’ instances of resource type r j
need :
it is a -d array of size ‘n*m’ that indicates the remaining resource need of each process.
that indicates the remaining resource need of each process. need [ i, j ] = k means process p i currently allocated ‘k’ instances of resource type r j
currently allocated instances of resource type need [ i, j ] = max [ i, j ] – allocation [ i, j ]
allocation i specifies the resources currently allocated to process p i and need i specifies the additional resources that process p i may still request to complete its task.
banker’s algorithm consist of safety algorithm and resource request algorithm
safety algorithm
the algorithm for finding out whether or not a system is in a safe state can be described as follows:
let work and finish be vectors of length ‘m’ and ‘n’ respectively.
initialize: work= available
finish [i]=false; for i=,,……,n
find an i such that both
a) finish [i]=false
b) need_i<=work
if no such i exists goto step () work=work + allocation_i
finish[i]= true
goto step() if finish[i]=true for all i,
then the system is in safe state.
safe sequence is the sequence in which the processes can be safely executed.
in this post, implementation of safety algorithm of banker’s algorithm is done.
output:
system is in safe state. safe sequence is:     
illustration :
considering a system with five processes p through p and three resources types a, b, c. resource type a has  instances, b has  instances and type c has  instances. suppose at time t following snapshot of the system has been taken:
we must determine whether the new system state is safe. to do so, we need to execute safety algorithm on the above given allocation chart.
following is the resource allocation graph:
executing safety algorithm shows that sequence  satisfies safety requirement.
time complexity = o(n*n*m) where n = number of processes and m = number of resources.
this article is contributed by sahil chhabra (akku). if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
prerequisite – resource allocation graph (rag), banker’s algorithm, program for banker’s algorithm
banker’s algorithm is a resource allocation and deadlock avoidance algorithm. this algorithm test for safety simulating the allocation for predetermined maximum possible amounts of all resources, then makes an “s-state” check to test for possible activities, before deciding whether allocation should be allowed to continue.
in simple terms, it checks if allocation of any resource will lead to deadlock or not, or is it safe to allocate a resource to a process and if not then resource is not allocated to that process. determining a safe sequence(even if there is only ) will assure that system will not go into deadlock.
banker’s algorithm is generally used to find if a safe sequence exist or not. but here we will determine the total number of safe sequences and print all safe sequences.
the data structure used are:
available vector
max matrix
allocation matrix
need matrix
example:
input:
output: safe sequences are: p--> p--> p--> p p--> p--> p--> p p--> p--> p--> p p--> p--> p--> p there are total  safe-sequences
explanation:
total resources are r = , r = , r =  and allocated resources are r = (+++ =) , r = (+++ =) , r = (+++ =) . therefore, remaining resources are r = ( –  =) , r = ( –  =) , r = ( –  =) .
remaining available = total resources – allocated resources
and
remaining need = max – allocated
so, we can start from either p or p. we can not satisfy remaining need from available resources of either p or p in first or second attempt step of banker’s algorithm. there are only four possible safe sequences. these are :
p–> p–> p–> p
p–> p–> p–> p
p–> p–> p–> p
p–> p–> p–> p
implementation:
output:
safe sequences are: p--> p--> p--> p p--> p--> p--> p p--> p--> p--> p p--> p--> p--> p there are total  safe-sequences
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if a system does not employ either a deadlock prevention or deadlock avoidance algorithm then a deadlock situation may occur. in this case-
apply an algorithm to examine state of system to determine whether deadlock has has occurred or not.
apply an algorithm to recover from the deadlock. for more refer- deadlock recovery
deadlock avoidance algorithm/ bankers algorithm:
the algorithm employs several time varying data structures:
available- a vector of length m indicates the number of available resources of each type.
a vector of length m indicates the number of available resources of each type. allocation- an n*m matrix defines the number of resources of each type currently allocated to a process. column represents resource and resource represent process.
an n*m matrix defines the number of resources of each type currently allocated to a process. column represents resource and resource represent process. request- an n*m matrix indicates the current request of each process. if request[i][j] equals k then process p i is requesting k more instances of resource type r j .
this algorithm has already been discussed here
now, bankers algorithm includes a safety algorithm / deadlock detection algorithm
the algorithm for finding out whether or not a system is in a safe state can be described as follows:
steps of algorithm:
let work and finish be vectors of length m and n respectively. initialize work= available. for i=, , …., n-, if request i = , then finish[i] = true; otherwise, finish[i]= false. find an index i such that both
a) finish[i] == false
b) request i <= work
if no such i exists go to step . work= work+ allocation i
finish[i]= true
go to step . if finish[i]== false for some i, <=i<n, then the system is in a deadlocked state. moreover, if finish[i]==false the process p i is deadlocked.
for example,
in this, work = [, , ] &attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
aastha student at netaji subhas institute of technology
given: a system has r identical resources, p processes competing for them and n is the maximum need of each process. the task is to find the minimum number of resources required so that deadlock will never occur.
consider,  process a, b and c.
let, need of each process is 
therefore, the maximum resources require will be  *  =  i.e, give  resources to each process.
and, the minimum resources required will be  * ( – ) +  = .
i.e, give  resources to each of the process, and we are left out with  resource.
that  resource will be given to any of the process a, b or c.
so that after using that resource by any one of the process, it left the resources and that resources will be used by any other process and thus deadlock will never occur.
output:
r >= 
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
in a distributed system deadlock can neither be prevented nor avoided as the system is so vast that it is impossible to do so. therefore, only deadlock detection can be implemented. the techniques of deadlock detection in the distributed system require the following:
progress – the method should be able to detect all the deadlocks in the system.
the method should be able to detect all the deadlocks in the system. safety – the method should not detect false or phantom deadlocks.
there are three approaches to detect deadlocks in distributed systems. they are as follows:
centralized approach –
in the centralized approach, there is only one responsible resource to detect deadlock. the advantage of this approach is that it is simple and easy to implement, while the drawbacks include excessive workload at one node, single-point failure (that is the whole system is dependent on one node if that node fails the whole system crashes) which in turns makes the system less reliable. distributed approach –
in the distributed approach different nodes work together to detect deadlocks. no single point failure ( that is the whole system is dependent on one node if that node fails the whole system crashes) as the workload is equally divided among all nodes. the speed of deadlock detection also increases. hierarchical approach –
this approach is the most advantageous. it is the combination of both centralized and distributed approaches of deadlock detection in a distributed system. in this approach, some selected nodes or cluster of nodes are responsible for deadlock detection and these selected nodes are controlled by a single node.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : vidit_gupta
prerequisite – deadlock introduction, deadlock detection
in the centralized approach of deadlock detection, two techniques are used namely: completely centralized algorithm and ho ramamurthy algorithm (one phase and two-phase).
completely centralized algorithm –
in a network of n sites, one site is chosen as a control site. this site is responsible for deadlock detection. it has control over all resources of the system. if a site requires a resource it requests the control site, the control site allocates and de-allocates resources and maintains a wait for graph. and at regular interval of time, it checks the wait for graph to detect a cycle. if cycle exits then it will declare system as deadlock otherwise the system will continue working. the major drawbacks of this technique are as follows: a site has to send request even for using its own resource. there is a possibility of phantom deadlock.
in a network of n sites, one site is chosen as a control site. this site is responsible for deadlock detection. it has control over all resources of the system. if a site requires a resource it requests the control site, the control site allocates and de-allocates resources and maintains a wait for graph. and at regular interval of time, it checks the wait for graph to detect a cycle. if cycle exits then it will declare system as deadlock otherwise the system will continue working. the major drawbacks of this technique are as follows: ho ramamurthy (two-phase algorithm) –
in this technique a resource status table is maintained by the central or control site, if a cycle is detected then the system is not declared deadlock at first, the cycle is checked again as the system is distributed some or the other resource is vacant or freed by sites at every instant of time. now, after checking if a cycle is detected again then, the system is declared as deadlock. this technique reduces the possibility of phantom deadlock but on the other hand time consumption is more.
in this technique a resource status table is maintained by the central or control site, if a cycle is detected then the system is not declared deadlock at first, the cycle is checked again as the system is distributed some or the other resource is vacant or freed by sites at every instant of time. now, after checking if a cycle is detected again then, the system is declared as deadlock. this technique reduces the possibility of phantom deadlock but on the other hand time consumption is more. ho ramamurthy (one phase algorithm) –
in this technique a resource status table and a process table is maintained by the central or control site if the cycle is detected in both processes and resource tables then, the system is declared as deadlock. this technique reduces time consumption but space complexity increases.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : rakshithsathish
what is a thread?
a thread is a path of execution within a process. a process can contain multiple threads.
why multithreading?
a thread is also known as lightweight process. the idea is to achieve parallelism by dividing a process into multiple threads. for example, in a browser, multiple tabs can be different threads. ms word uses multiple threads: one thread to format the text, another thread to process inputs, etc. more advantages of multithreading are discussed below
process vs thread?
the primary difference is that threads within the same process run in a shared memory space, while processes run in separate memory spaces.
advantages of thread over process
. responsiveness: if the process is divided into multiple threads, if one thread completes its execution, then its output can be immediately returned.
. faster context switch: context switch time between threads is lower compared to process context switch. process context switching requires more overhead from the cpu.
. effective utilization of multiprocessor system: if we have multiple threads in a single process, then we can schedule multiple threads on multiple processor. this will make process execution faster.
note: stack and registers can’t be shared among the threads. each thread has its own stack and registers.
. communication: communication between multiple threads is easier, as the threads shares common address space. while in process we have to follow some specific communication technique for communication between two process.
. enhanced throughput of the system: if a process is divided into multiple threads, and each thread function is considered as one job, then the number of jobs completed per unit of time is increased, thus increasing the throughput of the system.
types of threads
there are two types of threads.
user level thread
kernel level thread
refer user thread vs kernel thread for more details.
below are previous years’ gate questions on threads:
reference:
multithreading in c
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : chrismaher
thread is a single sequence stream within a process. threads have same properties as of the process so they are called as light weight processes. threads are executed one after another but gives the illusion as if they are executing in parallel. each thread has different states. each thread has
a program counter a register set a stack space
similarity between threads and processes –
only one thread or process is active at a time
within process both execute sequentiall
both can create children
differences between threads and processes –
threads are not independent, processes are.
threads are designed to assist each other, processes may or may not do it
types of threads:
user level thread (ult) –
is implemented in the user level library, they are not created using the system calls. thread switching does not need to call os and to cause interrupt to kernel. kernel doesn’t know about the user level thread and manages them as if they were single-threaded processes. advantages of ult – can be implemented on an os that does’t support multithreading.
simple representation since thread has only program counter, register set, stack space.
simple to create since no intervention of kernel.
thread switching is fast since no os calls need to be made. disadvantages of ult – no or less co-ordination among the threads and kernel.
if one thread causes a page fault, the entire process blocks. kernel level thread (klt) –
kernel knows and manages the threads. instead of thread table in each process, the kernel itself has thread table (a master one) that keeps track of all the threads in the system. in addition kernel also maintains the traditional process table to keep track of the processes. os kernel provides system call to create and manage threads. advantages of klt – since kernel has full knowledge about the threads in the system, scheduler may decide to give more time to processes having large number of threads.
good for applications that frequently block. disadvantages of klt – slow and inefficient.
it requires thread control block so it is an overhead.
summary:
each ult has a process that keeps track of the thread using the thread table. each klt has thread table (tcb) as well as the process table (pcb).
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : wertyu, magbene
user level thread kernel level thread user thread are implemented by users. kernel threads are implemented by os. os doesn’t recognized user level threads. kernel threads are recognized by os. implementation of user threads is easy. implementation of kernel thread is complicated. context switch time is less. context switch time is more. context switch requires no hardware support. hardware support is needed. if one user level thread perform blocking operation then entire process will be blocked. if one kernel thread perform blocking operation then another thread can continue execution. user level threads are designed as dependent threads. kernel level threads are designed as independent threads. example : java thread, posix threads. example : window solaris.
below is the previous year gate question
prerequisite – thread, difference between multitasking, multithreading and multiprocessing
a multitasking operating system is an operating system that gives you the perception of  or more tasks/jobs/processes running at the same time. it does this by dividing system resources amongst these tasks/jobs/processes and switching between the tasks/jobs/processes while they are executing over and over again. usually cpu processes only one task at a time but the switching is so fast that it looks like cpu is executing multiple processes at a time. they can support either preemptive multitasking, where the os doles out time to applications (virtually all modern oses) or cooperative multitasking, where the os waits for the program to give back control (windows .x, mac os  and earlier), leading to hangs and crashes. also known as timesharing, multitasking is a logical extension of multiprogramming.
multitasking programming is of two types –
process-based multitasking thread-based multitasking.
process based multitasking programming –
in process based multitasking two or more processes and programs can be run concurrently.
in process based multitasking a process or a program is the smallest unit.
program is a bigger unit.
process based multitasking requires more overhead.
process requires its own address space.
process to process communication is expensive.
here, it is unable to gain access over idle time of cpu.
it is comparatively heavy weight.
it has slower data rate multi-tasking.
example – we can listen to music and browse internet at the same time. the processes in this example are the music player and browser.
thread based multitasking programming –
in thread based multitasking two or more threads can be run concurrently.
in thread based multitasking a thread is the smallest unit.
thread is a smaller unit.
thread based multitasking requires less overhead.
threads share same address space.
thread to thread communication is not expensive.
it allows taking gain access over idle time taken by cpu.
it is comparatively light weight.
it has faster data rate multi-tasking.
examples – using a browser we can navigate through the webpage and at the same time download a file. in this example, navigation is one thread and downloading is another thread. also in a word-processing application like ms word, we can type text in one thread and spell checker checks for mistakes in another thread.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
many operating systems support kernel thread and user thread in a combined way. example of such system is solaris. multi threading model are of three types.
many to many model. many to one model. one to one model.
many to many model
in this model, we have multiple user threads multiplex to same or lesser number of kernel level threads. number of kernel level threads are specific to the machine, advantage of this model is if a user thread is blocked we can schedule others user thread to other kernel thread. thus, system doesn’t block if a particular thread is blocked.
many to one model
in this model, we have multiple user threads mapped to one kernel thread. in this model when a user thread makes a blocking system call entire process blocks. as we have only one kernel thread and only one user thread can access kernel at a time, so multiple threads are not able access multiprocessor at the same time.
one to one model
in this model, one to one relationship between kernel and user thread. in this model multiple thread can run on multiple processor. problem with this model is that creating a user thread requires the corresponding kernel thread.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
in a non multi threaded environment, a server listens to the port for some request and when the request comes, it processes the request and then resume listening to another request. the time taken while processing of request makes other users wait unnecessarily. instead a better approach would be to pass the request to a worker thread and continue listening to port.
for example, a multi threaded web browser allow user interaction in one thread while an video is being loaded in another thread. so instead of waiting for the whole web-page to load the user can continue viewing some portion of the web-page.
prerequisites: fork() in c, zombie process
zombie state : when a process is created in unix using fork() system call, the address space of the parent process is replicated. if the parent process calls wait() system call, then the execution of parent is suspended until the child is terminated. at the termination of the child, a ‘sigchld’ signal is generated which is delivered to the parent by the kernel. parent, on receipt of ‘sigchld’ reaps the status of the child from the process table. even though, the child is terminated, there is an entry in the process table corresponding to the child where the status is stored. when parent collects the status, this entry is deleted. thus, all the traces of the child process are removed from the system. if the parent decides not to wait for the child’s termination and it executes its subsequent task, then at the termination of the child, the exit status is not read. hence, there remains an entry in the process table even after the termination of the child. this state of the child process is known as the zombie state.
" ); } else { printf ( "i am parent
" ); while (); } }
output :
now check the process table using the following command in the terminal
$ ps -eaf
here the entry [a.out] defunct shows the zombie process.
why do we need to prevent the creation of zombie process?
there is one process table per system. the size of the process table is finite. if too many zombie processes are generated, then the process table will be full. that is, the system will not be able to generate any new process, then the system will come to a standstill. hence, we need to prevent the creation of zombie processes.
different ways in which the creation of zombie can be prevented
. using wait() system call : when the parent process calls wait(), after the creation of a child, it indicates that, it will wait for the child to complete and it will reap the exit status of the child. the parent process is suspended(waits in a waiting queue) until the child is terminated. it must be understood that during this period, the parent process does nothing just waits.
" ); } else { wait(null); printf ( "i am parent
" ); while (); } }
. by ignoring the sigchld signal : when a child is terminated, a corresponding sigchld signal is delivered to the parent, if we call the ‘signal(sigchld,sig_ign)’, then the sigchld signal is ignored by the system, and the child process entry is deleted from the process table. thus, no zombie is created. however, in this case, the parent cannot know about the exit status of the child.
" ); else { signal (sigchld,sig_ign); printf ( "i am parent
" ); while (); } }
. by using a signal handler : the parent process installs a signal handler for the sigchld signal. the signal handler calls wait() system call within it. in this senario, when the child terminated, the sigchld is delivered to the parent. on receipt of sigchld, the corresponding handler is activated, which in turn calls the wait() system call. hence, the parent collects the exit status almost immediately and the child entry in the process table is cleared. thus no zombie is created.
" ); else { signal (sigchld, func); printf ( "i am parent
" ); while (); } }
output:
here no any [a.out] defunct i.e. no any zombie process is created.
this article is contributed by kishlay verma. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
prerequisites: fork() in c, zombie process
zombie state : when a process is created in unix using fork() system call, the address space of the parent process is replicated. if the parent process calls wait() system call, then the execution of parent is suspended until the child is terminated. at the termination of the child, a ‘sigchld’ signal is generated which is delivered to the parent by the kernel. parent, on receipt of ‘sigchld’ reaps the status of the child from the process table. even though, the child is terminated, there is an entry in the process table corresponding to the child where the status is stored. when parent collects the status, this entry is deleted. thus, all the traces of the child process are removed from the system. if the parent decides not to wait for the child’s termination and it executes its subsequent task, then at the termination of the child, the exit status is not read. hence, there remains an entry in the process table even after the termination of the child. this state of the child process is known as the zombie state.
" ); } else { printf ( "i am parent
" ); while (); } }
output :
now check the process table using the following command in the terminal
$ ps -eaf
here the entry [a.out] defunct shows the zombie process.
why do we need to prevent the creation of zombie process?
there is one process table per system. the size of the process table is finite. if too many zombie processes are generated, then the process table will be full. that is, the system will not be able to generate any new process, then the system will come to a standstill. hence, we need to prevent the creation of zombie processes.
different ways in which the creation of zombie can be prevented
. using wait() system call : when the parent process calls wait(), after the creation of a child, it indicates that, it will wait for the child to complete and it will reap the exit status of the child. the parent process is suspended(waits in a waiting queue) until the child is terminated. it must be understood that during this period, the parent process does nothing just waits.
here no any [a.out] defunct i.e. no any zombie process is created.
this article is contributed by kishlay verma. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
remote procedure call (rpc) is a powerful technique for constructing distributed, client-server based applications. it is based on extending the conventional local procedure calling so that the called procedure need not exist in the same address space as the calling procedure. the two processes may be on the same system, or they may be on different systems with a network connecting them.
when making a remote procedure call:
. the calling environment is suspended, procedure parameters are transferred across the network to the environment where the procedure is to execute, and the procedure is executed there.
. when the procedure finishes and produces its results, its results are transferred back to the calling environment, where execution resumes as if returning from a regular procedure call.
note: rpc is especially well suited for client-server (e.g. query-response) interaction in which the flow of control alternates between the caller and callee. conceptually, the client and server do not both execute at the same time. instead, the thread of execution jumps from the caller to the callee and then back again.
working of rpc
the following steps take place during a rpc:
. a client invokes a client stub procedure, passing parameters in the usual way. the client stub resides within the client’s own address space.
. the client stub marshalls(pack) the parameters into a message. marshalling includes converting the representation of the parameters into a standard format, and copying each parameter into the message.
. the client stub passes the message to the transport layer, which sends it to the remote server machine.
. on the server, the transport layer passes the message to a server stub, which demarshalls(unpack) the parameters and calls the desired server routine using the regular procedure call mechanism.
. when the server procedure completes, it returns to the server stub (e.g., via a normal procedure call return), which marshalls the return values into a message. the server stub then hands the message to the transport layer.
. the transport layer sends the result message back to the client transport layer, which hands the message back to the client stub.
. the client stub demarshalls the return parameters and execution returns to the caller.
rpc issues
issues that must be addressed:
on the client side, the stub handles the interface between the client’s local procedure call and the run-time system, marshaling and unmarshaling data, invoking the rpc run-time protocol, and if requested, carrying out some of the binding steps.
on the server side, the stub provides a similar interface between the run-time system and the local manager procedures that are executed by the server.
. binding: how does the client know who to call, and where the service resides?
the most flexible solution is to use dynamic binding and find the server at run time when the rpc is first made. the first time the client stub is invoked, it contacts a name server to determine the transport address at which the server resides.
binding consists of two parts:
naming:
remote procedures are named through interfaces. an interface uniquely identifies a particular service, describing the types and numbers of its arguments. it is similar in purpose to a type definition in programming languauges. locating: finding the transport address at which the server actually resides. once we have the transport address of the service, we can send messages directly to the server.
a server having a service to offer exports an interface for it. exporting an interface registers it with the system so that clients can use it.
a client must import an (exported) interface before communication can begin.
advantages
. rpc provides abstraction i.e message-passing nature of network communication is hidden from the user.
. rpc often omits many of the protocol layers to improve performance. even a small performance improvement is important because a program may invoke rpcs often.
. rpc enables the usage of the applications in the distributed environment, not only in the local environment.
. process-oriented and thread oriented models supported by rpc.
in the computer system design, memory hierarchy is an enhancement to organize the memory such that it can minimize the access time. the memory hierarchy was developed based on a program behavior known as locality of 
memories are made up of registers. each register in the memory is one storage location. storage location is also called as memory location. memory locations are identified using address. the total number of bit a memory can store is its capacity.
a storage element is called a cell. each register is made up of storage element in which one bit of data is stored. the data in a memory are stored and retrieved by the process called writing and reading respectively.
a word is a group of bits where a memory unit stores binary information. a word with group of  bits is called a byte.
a memory unit consists of data lines, address selection lines, and control lines that specify the direction of transfer. the block diagram of a memory unit is shown below:
data lines provide the information to be stored in memory. the control inputs specify the direction transfer. the k-address lines specify the word chosen.
when there are k address lines, k memory word can be accessed.
refer for ram and rom, different types of ram, cache memory, and secondary memory
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
ram(random access memory) is a part of computer’s main memory which is directly accessible by cpu. ram is used to read and write data into it which is accessed by cpu randomly. ram is volatile in nature, it means if the power goes off, the stored information is lost. ram is used to store the data that is currently processed by the cpu. most of the programs and data that are modifiable are stored in ram.
integrated ram chips are available in two form:
sram(static ram) dram(dynamic ram)
the block diagram of ram chip is given below.
sram
the sram memories consist of circuits capable of retaining the stored information as long as the power is applied. that means this type of memory requires constant power. sram memories are used to build cache memory.
dram
dram stores the binary information in the form of electric charges that applied to capacitors. the stored information on the capacitors tend to lose over a period of time and thus the capacitors must be periodically recharged to retain their usage. the main memory is generally made up of dram chips.
dram memory cell: though sram is very fast, but it is expensive because of its every cell requires several transistors. relatively less expensive ram is dram, due to the use of one transistor and one capacitor in each cell, as shown in the below figure., where c is the capacitor and t is the transistor. information is stored in a dram cell in the form of a charge on a capacitor and this charge needs to be periodically recharged.
for storing information in this cell, transistor t is turned on and an appropriate voltage is applied to the bit line. this causes a known amount of charge to be stored in the capacitor. after the transistor is turned off, due to the property of the capacitor, it starts to discharge. hence, the information stored in the cell can be read correctly only if it is read before the charge on the capacitors drops below some threshold value.
types of dram
there are mainly  types of dram:
asynchronous dram (adram): the dram described above is the asynchronous type dram. the timing of the memory device is controlled asynchronously. a specialized memory controller circuit generates the necessary control signals to control the timing. the cpu must take into account the delay in the response of the memory. synchronous dram (sdram): these ram chips’ access speed is directly synchronized with the cpu’s clock. for this, the memory chips remain ready for operation when the cpu expects them to be ready. these memories operate at the cpu-memory bus without imposing wait states. sdram is commercially available as modules incorporating multiple sdram chips and forming the required capacity for the modules. double-data-rate sdram (ddr sdram): this faster version of sdram performs its operations on both edges of the clock signal; whereas a standard sdram performs its operations on the rising edge of the clock signal. since they transfer data on both edges of the clock, the data transfer rate is doubled. to access the data at high rate, the memory cells are organized into two groups. each group is accessed separately. rambus dram (rdram): the rdram provides a very high data transfer rate over a narrow cpu-memory bus. it uses various speedup mechanisms, like synchronous memory interface, caching inside the dram chips and very fast signal timing. the rambus data bus width is  or  bits. cache dram (cdram): this memory is a special type dram memory with an on-chip cache memory (sram) that acts as a high-speed buffer for the main dram.
difference between sram and dram
below table lists some of the differences between sram and dram:
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
prerequisite – partition allocation methods
static partition schemes suffer from the limitation of having the fixed number of active processes and the usage of space may also not be optimal. the buddy system is a memory allocation and management algorithm that manages memory in power of two increments. assume the memory size is u, suppose a size of s is required.
if  u- <s<= u : allocate the whole block
allocate the whole block else: recursively divide the block equally and test the condition at each time, when it satisfies, allocate the block and get out the loop.
system also keep the record of all the unallocated blocks each and can merge these different size blocks to make one big chunk.
advantage –
easy to implement a buddy system
allocates block of correct size
it is easy to merge adjacent holes
fast to allocate memory and de-allocating memory
disadvantage –
it requires all allocation unit to be powers of two
it leads to internal fragmentation
example –
consider a system having buddy system with physical address space  kb.calculate the size of partition for  kb process.
solution –
so, size of partition for  kb process =  kb. it divides by , till possible to get minimum block to fit  kb.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : soumya, rakshithsathish
in the operating system, the following are four common memory management techniques.
single contiguous allocation: simplest allocation method used by ms-dos. all memory (except some reserved for os) is available to a process.
partitioned allocation: memory is divided into different blocks or partitions. each process is allocated according to the requirement.
paged memory management: memory is divided into fixed-sized units called page frames, used in a virtual memory environment.
most of the operating systems (for example windows and linux) use segmentation with paging. a process is divided into segments and individual segments have pages.
in partition allocation, when there is more than one partition freely available to accommodate a process’s request, a partition must be selected. to choose a particular partition, a partition allocation method is needed. a partition allocation method is considered better if it avoids internal fragmentation.
when it is time to load a process into the main memory and if there is more than one free block of memory of sufficient size then the os decides which free block to allocate.
there are different placement algorithm:
a. first fit
b. best fit
c. worst fit
d. next fit
. first fit: in the first fit, the partition is allocated which is the first sufficient block from the top of main memory. it scans memory from the beginning and chooses the first available block that is large enough. thus it allocates the first hole that is large enough.
. best fit allocate the process to the partition which is the first smallest sufficient partition among the free available partition. it searches the entire list of holes to find the smallest hole whose size is greater than or equal to the size of the process.
. worst fit allocate the process to the partition which is the largest sufficient among the freely available partitions available in the main memory. it is opposite to the best-fit algorithm. it searches the entire list of holes to find the largest hole and allocate it to process.
. next fit: next fit is similar to the first fit but it will search for the first sufficient partition from the last allocation point.
is best-fit really best?
although best fit minimizes the wastage space, it consumes a lot of processor time for searching the block which is close to the required size. also, best-fit may perform poorer than other algorithms in some cases. for example, see the below exercise.
exercise: consider the requests from processes in given order k, k, k, and k. let there be two blocks of memory available of size k followed by a block size k.
which of the following partition allocation schemes can satisfy the above requests?
a) best fit but not first fit.
b) first fit but not best fit.
c) both first fit & best fit.
d) neither first fit nor best fit.
solution: let us try all options.
best fit:
k is allocated from a block of size k.  is left in the block.
k is allocated from the remaining k block. k is left in the block.
k is allocated from  k block. k is left in this block also.
k can’t be allocated even if there is k + k space available.
first fit:
k request is allocated from k block, k is left out.
k is be allocated from the k block, k is left out.
then k and k are allocated to the remaining left out partitions.
so, the first fit can handle requests.
so option b is the correct choice.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : vaibhavrai, harleenk_, deepakmkoshy
in operating systems, memory management is the function responsible for allocating and managing computer’s main memory. memory management function keeps track of the status of each memory location, either allocated or free to ensure effective and efficient use of primary memory.
there are two memory management techniques: contiguous, and non-contiguous. in contiguous technique, executing process must be loaded entirely in main-memory. contiguous technique can be divided into:
fixed (or static) partitioning variable (or dynamic) partitioning
fixed partitioning:
this is the oldest and simplest technique used to put more than one processes in the main memory. in this partitioning, number of partitions (non-overlapping) in ram are fixed but size of each partition may or may not be same. as it is contiguous allocation, hence no spanning is allowed. here partition are made before execution or during system configure.
as illustrated in above figure, first process is only consuming mb out of mb in the main memory.
hence, internal fragmentation in first block is (-) = mb.
sum of internal fragmentation in every block = (-)+(-)+(-)+(-)= +++ = mb.
suppose process p of size mb comes. but this process cannot be accommodated inspite of available free space because of contiguous allocation (as spanning is not allowed). hence, mb becomes part of external fragmentation.
there are some advantages and disadvantages of fixed partitioning.
advantages of fixed partitioning –
easy to implement:
algorithms needed to implement fixed partitioning are easy to implement. it simply requires putting a process into certain partition without focussing on the emergence of internal and external fragmentation. little os overhead:
processing of fixed partitioning require lesser excess and indirect computational power.
disadvantages of fixed partitioning –
internal fragmentation:
main memory use is inefficient. any program, no matter how small, occupies an entire partition. this can cause internal fragmentation. external fragmentation:
the total unused space (as stated above) of various partitions cannot be used to load the processes even though there is space available but not in the contiguous form (as spanning is not allowed). limit process size:
process of size greater than size of partition in main memory cannot be accommodated. partition size cannot be varied according to the size of incoming process’s size. hence, process size of mb in above stated example is invalid. limitation on degree of multiprogramming:
partition in main memory are made before execution or during system configure. main memory is divided into fixed number of partition. suppose if there are partitions in ram and are the number of processes, then condition must be fulfilled. number of processes greater than number of partitions in ram is invalid in fixed partitioning.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : ayushnigam
in operating systems, memory management is the function responsible for allocating and managing computer’s main memory. memory management function keeps track of the status of each memory location, either allocated or free to ensure effective and efficient use of primary memory.
there are two memory management techniques: contiguous, and non-contiguous. in contiguous technique, executing process must be loaded entirely in main-memory. contiguous technique can be divided into:
variable partitioning –
it is a part of contiguous allocation technique. it is used to alleviate the problem faced by fixed partitioning. in contrast with fixed partitioning, partitions are not made before the execution or during system configure. various features associated with variable partitioning-
initially ram is empty and partitions are made during the run-time according to process’s need instead of partitioning during system configure. the size of partition will be equal to incoming process. the partition size varies according to the need of the process so that the internal fragmentation can be avoided to ensure efficient utilisation of ram. number of partitions in ram is not fixed and depends on the number of incoming process and main memory’s size.
there are some advantages and disadvantages of variable partitioning over fixed partitioning as given below.
advantages of variable partitioning –
no internal fragmentation:
in variable partitioning, space in main memory is allocated strictly according to the need of process, hence there is no case of internal fragmentation. there will be no unused space left in the partition. no restriction on degree of multiprogramming:
more number of processes can be accommodated due to absence of internal fragmentation. a process can be loaded until the memory is empty. no limitation on the size of the process:
in fixed partitioning, the process with the size greater than the size of the largest partition could not be loaded and process can not be divided as it is invalid in contiguous allocation technique. here, in variable partitioning, the process size can’t be restricted since the partition size is decided according to the process size.
disadvantages of variable partitioning –
difficult implementation:
implementing variable partitioning is difficult as compared to fixed partitioning as it involves allocation of memory during run-time rather than during system configure. external fragmentation:
there will be external fragmentation inspite of absence of internal fragmentation. for example, suppose in above example- process p(mb) and process p(mb) completed their execution. hence two spaces are left i.e. mb and mb. let’s suppose process p of size mb comes. the empty space in memory cannot be allocated as no spanning is allowed in contiguous allocation. the rule says that process must be contiguously present in main memory to get executed. hence it results in external fragmentation. now p of size  mb cannot be accommodated in spite of required available space because in contiguous no spanning is allowed.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : ayushnigam
prerequisite – variable partitioning, fixed partitioning
paging and segmentation are the two ways which allow a process’s physical address space to be non-contiguous. it has advantage of reducing memory wastage but it increases the overheads due to address translation. it slows the execution of the memory because time is consumed in address translation.
in non-contiguous allocation, operating system needs to maintain the table which is called page table for each process which contains the base address of the each block which is acquired by the process in memory space. in non-contiguous memory allocation, different parts of a process is allocated different places in main memory. spanning is allowed which is not possible in other techniques like dynamic or static contiguous memory allocation. that’s why paging is needed to ensure effective memory allocation. paging is done to remove external fragmentation.
working:
here a process can be spanned across different spaces in main memory in non-consecutive manner. suppose process p of size kb. consider main memory have two empty slots each of size kb. hence total free space is, *=  kb. in contiguous memory allocation, process p cannot be accommodated as spanning is not allowed.
in contiguous allocation, space in memory should be allocated to whole process. if not, then that space remains unallocated. but in non-contiguous allocation, process can be divided into different parts and hence filling the space in main memory. in this example, process p can be divided into two parts of equal size – kb. hence one part of process p can be allocated to first kb space of main memory and other part of processp can be allocated to second kb space of main memory. below diagram will explain in better way:
but, in what manner we divide a process to allocate them into main memory is very important to understand. process is divided after analysing the number of empty spaces and their size in main memory. then only we divide our process. it is very time consuming process. their number as well as their sizes changing every time due to execution of already present processes in main memory.
in order to avoid this time consuming process, we divide our process in secondary memory in advance before reaching the main memory for its execution. every process is divided into various parts of equal size called pages. we also divide our main memory into different parts of equal size called frames. it is important to understand that:
size of page in process = size of frame in memory
although their numbers can be different. below diagram will make you understand in better way: consider empty main memory having size of each frame is  kb, and two processes p and p are  kb each.
resolvent main memory,
in conclusion we can say that, paging allows memory address space of a process to be non-contiguous. paging is more flexible as only pages of a process are moved. it allows more processes to reside in main memory than contiguous memory allocation.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : pulkitjoshi
logical address is generated by cpu while a program is running. the logical address is virtual address as it does not exist physically, therefore, it is also known as virtual address. this address is used as a reference to access the physical memory location by cpu. the term logical address space is used for the set of all logical addresses generated by a program’s perspective.
the hardware device called memory-management unit is used for mapping logical address to its corresponding physical address.
physical address identifies a physical location of required data in a memory. the user never directly deals with the physical address but can access by its corresponding logical address. the user program generates the logical address and thinks that the program is running in this logical address but the program needs physical memory for its execution, therefore, the logical address must be mapped to the physical address by mmu before they are used. the term physical address space is used for all physical addresses corresponding to the logical addresses in a logical address space.
mapping virtual-address to physical-addresses
differences between logical and physical address in operating system
the basic difference between logical and physical address is that logical address is generated by cpu in perspective of a program whereas the physical address is a location that exists in the memory unit. logical address space is the set of all logical addresses generated by cpu for a program whereas the set of all physical address mapped to corresponding logical addresses is called physical address space. the logical address does not exist physically in the memory whereas physical address is a location in the memory that can be accessed physically. identical logical addresses are generated by compile-time and load time address binding methods whereas they differs from each other in run-time address binding method. please refer this for details. the logical address is generated by the cpu while the program is running whereas the physical address is computed by the memory management unit (mmu).
comparison chart:
paramenter logical address physical address basic generated by cpu location in a memory unit address space logical address space is set of all logical addresses generated by cpu in reference to a program. physical address is set of all physical addresses mapped to the corresponding logical addresses. visibility user can view the logical address of a program. user can never view physical address of program. generation generated by the cpu computed by mmu access the user can use the logical address to access the physical address. the user can indirectly access physical address but not directly.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : vaibhavrai
paging is a memory management scheme that eliminates the need for contiguous allocation of physical memory. this scheme permits the physical address space of a process to be non – contiguous.
logical address or virtual address (represented in bits): an address generated by the cpu
logical address space or virtual address space( represented in words or bytes): the set of all logical addresses generated by a program
physical address (represented in bits): an address actually available on memory unit
physical address space (represented in words or bytes): the set of all physical addresses corresponding to the logical addresses
example:
if logical address =  bit, then logical address space =   words =  g words ( g =   )
words =  g words ( g =  ) if logical address space =  m words =   *   words, then logical address = log    =  bits
*  words, then logical address = log  =  bits if physical address =  bit, then physical address space =   words =  m words ( m =   )
words =  m words ( m =  ) if physical address space =  m words =  *  words, then physical address = log   =  bits
the mapping from virtual to physical address is done by the memory management unit (mmu) which is a hardware device and this mapping is known as paging technique.
the physical address space is conceptually divided into a number of fixed-size blocks, called frames .
. the logical address space is also splitted into fixed-size blocks, called pages .
. page size = frame size
let us consider an example:
physical address =  bits, then physical address space =  k words
logical address =  bits, then logical address space =  k words
page size = frame size =  k words (assumption)
address generated by cpu is divided into
page number(p): number of bits required to represent the pages in logical address space or page number
number of bits required to represent the pages in logical address space or page number page offset(d): number of bits required to represent particular word in a page or page size of logical address space or word number of a page or page offset.
physical address is divided into
frame number(f): number of bits required to represent the frame of physical address space or frame number.
number of bits required to represent the frame of physical address space or frame number. frame offset(d): number of bits required to represent particular word in a frame or frame size of physical address space or word number of a frame or frame offset.
the hardware implementation of page table can be done by using dedicated registers. but the usage of register for the page table is satisfactory only if page table is small. if page table contain large number of entries then we can use tlb(translation look-aside buffer), a special, small, fast look up hardware cache.
memory management keeps track of the status of each memory location, whether it is allocated or free. it allocates the memory dynamically to the programs at their request and frees it for reuse when it is no longer needed. memory management meant to satisfy some requirements that we should keep in mind.
these requirements of memory management are:
relocation – the available memory is generally shared among a number of processes in a multiprogramming system, so it is not possible to know in advance which other programs will be resident in main memory at the time of execution of his program. swapping the active processes in and out of the main memory enables the operating system to have a larger pool of ready-to-execute process.
memory consists of large array of words or arrays, each of which has address associated with it. now the work of cpu is to fetch instructions from the memory based program counter. now further these instruction may cause loading or storing to specific memory address.
an address binding can be done in three different ways:
compile time – if you know that during compile time where process will reside in memory then absolute address is generated i.e physical address is embedded to the executable of the program during compilation. loading the executable as a process in memory is very fast. but if the generated address space is preoccupied by other process, then the program crashes and it becomes necessary to recompile the program to change the address space.
load time – if it is not known at the compile time where process will reside then relocatable address will be generated. loader translates the relocatable address to absolute address. the base address of the process in main memory is added to all logical addresses by the loader to generate absolute address. in this if the base address of the process changes then we need to reload the process again.
execution time- the instructions are in memory and are being processed by the cpu. additional memory may be allocated and/or deallocated at this time. this is used if process can be moved from one memory to another during execution(dynamic linking-linking that is done during load or run time). e.g – compaction.
mmu(memory management unit)-
the run time mapping between virtual address and physical address is done by hardware device known as mmu.
in memory management, operating system will handle the processes and moves the processes between disk and memory for execution . it keeps the track of available and used memory.
instruction-execution cycle follows steps:
basic hardware
as main memory and registers are built into processor and cpu can access these only.so every instructions should be written in direct access storage
devices.
if cpu access instruction from register then it can be done in one cpu clock cycle as registers are built into cpu. if instruction resides in main memory then it will be accessed via memory bus that will take lot of time. so remedy to this add fast memory in between cpu and main memory i.e. adding cache for transaction. now we should insure that process resides in legal address. legal address consists of base register(holds smallest physical address) and limit register(size of range).
for example:
base register =  limit register =  then legal address = (+)= (inclusive). legal address = base register+ limit register
how processes are mapped from disk to memory
usually process resides in disk in form of binary executable file. so to execute process it should reside in main memory. process is moved from disk to memory based on memory management in use. the processes waits in disk in form of ready queue to acquire memory.
procedure of mapping of disk and memory
normal procedure is that process is selected from input queue and loaded in memory. as process executes it accesses data and instructions from memory and as soon as it completes it will release memory and now memory will be available for other processes.
mmu scheme –
cpu------- mmu------memory
cpu will generate logical address for eg:  mmu will generate relocation register(base register) for eg: in memory physical address is located eg:(+= )
reference and image source:
this article is contributed by vaishali bhatia.if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : neeraj negi
prerequisite – paging
page table has page table entries where each page table entry stores a frame number and optional status (like protection) bits. many of status bits used in the virtual memory system. the most important thing in pte is frame number.
page table entry has the following information –
virtual memory is a storage allocation scheme in which secondary memory can be addressed as though it were part of main memory. the addresses a program may use to reference memory are distinguished from the addresses the memory system uses to identify physical storage sites, and program generated addresses are translated automatically to the corresponding machine addresses.
the size of virtual storage is limited by the addressing scheme of the computer system and amount of secondary memory is available not by the actual number of the main storage locations.
it is a technique that is implemented using both hardware and software. it maps memory addresses used by a program, called virtual addresses, into physical addresses in computer memory.
all memory 
prerequisite – virtual memory
abstraction is one the most important aspect of computing. it is widely implemented practice in the computational field.
memory interleaving is less or more an abstraction technique. though its a bit different from abstraction. it is a technique which divides memory into a number of modules such that successive words in the address space are placed in the different module.
consecutive word in a module:
figure-: consecutive word in a module
let us assume  data’s to be transferred to the four module. where module  be module , module  be module , module  be module  & module  be module . also , , …. are the data to be transferred.
from the figure above in module ,  [data] is transferred then ,  & finally,  which are the data. that means the data are added consecutively in the module till its max capacity.
most significant bit (msb) provides the address of the module & least significant bit (lsb) provides the address of the data in the module.
for example, to get  (data)  will be provided by the processor. in this  will indicate that the data is in module  (module ) &  is the address of  in module  (module ). so,
module  contains data : , , ,  module  contains data : , , ,  module  contains data : , , ,  module  contains data : , , , 
consecutive word in consecutive module:
figure-: consecutive word in consecutive module
now again we assume  data’s to be transferred to the four module. but now the consecutive data are added in consecutive module. that is,  [data] is added in module ,  [data] in module  and so on.
least significant bit (lsb) provides the address of the module & most significant bit (msb) provides the address of the data in the module.
for example, to get  (data)  will be provided by the processor. in this  will indicate that the data is in module  (module ) &  is the address of  in module  (module ). that is,
module  contains data : , , ,  module  contains data : , , ,  module  contains data : , , ,  module  contains data : , , , 
why we use memory interleaving? [advantages]:
whenever, processor request data from the main memory. a block (chunk) of data is transferred to the cache and then to processor. so whenever a cache miss occurs the data is to be fetched from main memory. but main memory is relatively slower than the cache. so to improve the access time of the main memory interleaving is used.
we can access all four module at the same time thus achieving parallelism. from figure  the data can be acquired from the module using the higher bits. this method uses memory effectively.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
advantages
large virtual memory.
more efficient use of memory.
unconstrained multiprogramming. there is no limit on degree of multiprogramming.
disadvantages
number of tables and amount of processor overhead for handling page interrupts are greater than in the case of the simple paged management techniques.
due to lack of an explicit constraint on a job’s address space size.
a way to control thrashing
set the lower and upper bounds of page fault rate for each process. using the above step, establish ‘acceptable’ page fault rate.
if actual rate is lower than lower bound, decrease the number of frames
if actual rate is larger than upper bound, increase the number of frames.
q. virtual memory is
(a) large secondary memory
(b) large main memory
(c) illusion of large main memory
(d) none of the above
answer: (c)
explanation: virtual memory is illusion of large main memory.
q. thrashing occurs when
(a)when a page fault occurs
(b) processes on system frequently access pages not memory
(c) processes on system are in running state
(d) processes on system are in waiting state
answer: (b)
explanation: thrashing occurs when processes on system require more memory than it has. if processes do not have “enough” pages, the page fault rate is very high. this leads to:
– low cpu utilization
– operating system spends most of its time swapping to disk
the above situation is called thrashing
q. a computer system supports -bit virtual addresses as well as -bit physical addresses. since the virtual address space is of the same size as the physical address space, the operating system designers decide to get rid of the virtual memory entirely. which one of the following is true?
(a) efficient implementation of multi-user support is no longer possible
(b) the processor cache organization can be made more efficient now
(c) hardware support for memory management is no longer needed
(d) cpu scheduling can be made more efficient now
answer: (c)
explanation: for supporting virtual memory, special hardware support is needed from memory management unit. since operating system designers decide to get rid of the virtual memory entirely, hardware support for memory management is no longer needed.
this article is contributed by mithlesh upadhyay
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
prerequisites – types of server virtualization, hardware based virtualization
operating system based virtualization refers to an operating system feature in which the kernel enables the existence of various isolated user-space instances. the installation of virtualization software also refers to operating system-based virtualization. it is installed over a pre-existing operating system and that operating system is called the host operating system.
in this virtualization, a user installs the virtualization software in the operating system of his system like any other program and utilize this application to operate and generate various virtual machines. here, the virtualization software allows direct access to any of the created virtual machine to the user. as the host os can provide hardware devices with the mandatory support, operating system virtualization may affect compatibility issues of hardware even when the hardware driver is not allocated to the virtualization software.
virtualization software is able to convert hardware it resources which require unique software for operation into virtualized it resources. as the host os is a complete operating system in itself, many os based services are available as organizational management and administration tools can be utilized for the virtualization host management.
some major operating system-based services are mentioned below:
backup and recovery. security management. integration to directory services.
various major operations of operating system based virtualization are described below:
hardware capabilities which can be employed, such as the network connection and cpu. connected peripherals with which it can interact with, such as webcam, printer, keyboard, or scanners. data which can be read or written, such as files, folders and network shares.
the operating system may have the capability to allow or deny access to such resources based on which program requests them and the user account in the context of which it runs. os may also hide these resources, which leads that when computer program computes them, they do not appear in the enumeration results. nevertheless, from a programming perspective, the computer program has interacted with those resources and the operating system has managed an act of interaction.
with operating-system-virtualization, or containerization, it is probable to run programs within containers, to which only parts of these resources are allocated. a program which is expected to perceive the whole computer, once run inside a container, can only see the allocated resources and believes them to be all that is available. several containers can be formed on each operating system, to each of which a subset of the computer’s resources is allocated. each container may include many computer programs. these programs may run parallel or distinctly, even interrelate with each other.
operating system-based virtualization can raise demands and problems related to performance overhead, such as:
the host operating system employs cpu, memory, and other hardware it resources. hardware-related calls from guest operating systems need to navigate numerous layers to and from the hardware, which shrinkage overall performance. licenses are frequently essential for host operating systems, in addition to individual licenses for each of their guest operating systems.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
prerequisite – paging, page table entries, segmentation
most of the operating systems implement a separate pagetable for each process, i.e. for ‘n’ number of processes running on a multiprocessing/ timesharing operating system, there are ‘n’ number of pagetables stored in the memory. sometimes when a process is very large in size and it occupies virtual memory then with the size of the process, it’s pagetable size also increases substantially.
example: a process of size  gb with: page size =  bytes size of page table entry =  bytes, then number of pages in the process =  gb /  b =  pagetable size =  *  =  bytes
through this example, it can be concluded that for multiple processes running simultaneously in an os, a considerable part of memory is occupied by page tables only.
operating systems also incorporate multilevel paging schemes which further increase the space required for storing the page tables and a large amount of memory is invested in storing them. the amount of memory occupied by the page tables can turn out to be a huge overhead and is always unacceptable as main memory is always a scarce resource. various efforts are made to utilize the memory efficiently and to maintain a good balance in the level of multiprogramming and efficient cpu utilization.
inverted page table –
an alternate approach is to use the inverted page table structure that consists of one-page table entry for every frame of the main memory. so the number of page table entries in the inverted page table reduces to the number of frames in physical memory and a single page table is used to represent the paging information of all the processes.
through the inverted page table, the overhead of storing an individual page table for every process gets eliminated and only a fixed portion of memory is required to store the paging information of all the processes together. this technique is called as inverted paging as the indexing is done with respect to the frame number instead of the logical page number. each entry in the page table contains the following fields.
page number – it specifies the page number range of the logical address.
it specifies the page number range of the logical address. process id – an inverted page table contains the address space information of all the processes in execution. since two different processes can have similar set of virtual addresses, it becomes necessary in inverted page table to store a process id of each process to identify it’s address space uniquely. this is done by using the combination of pid and page number. so this process id acts as an address space identifier and ensures that a virtual page for a particular process is mapped correctly to the corresponding physical frame.
an inverted page table contains the address space information of all the processes in execution. since two different processes can have similar set of virtual addresses, it becomes necessary in inverted page table to store a process id of each process to identify it’s address space uniquely. this is done by using the combination of pid and page number. so this process id acts as an address space identifier and ensures that a virtual page for a particular process is mapped correctly to the corresponding physical frame. control bits – these bits are used to store extra paging-related information. these include the valid bit, dirty bit, reference bits, protection and locking information bits.
these bits are used to store extra paging-related information. these include the valid bit, dirty bit, reference bits, protection and locking information bits. chained pointer – it may be possible sometime that two or more processes share a part of main memory. in this case, two or more logical pages map to same page table entry then a chaining pointer is used to map the details of these logical pages to the root page table.
working – the operation of an inverted page table is shown below.
the virtual address generated by the cpu contains the fields and each page table entry contains and the other relevant information required in paging related mechanism. when a memory reference takes place, this virtual address is matched by the memory-mapping unit and the inverted page table is searched to match the and the corresponding frame number is obtained. if the match is found at the ith entry then the physical address of the process, , is sent as the real address otherwise if no match is found then segmentation fault is generated.
note: number of entries in inverted page table = number of frames in physical address space(pas)
examples – the inverted page table and its variations are implemented in various systems like powerpc, ultrasparc and the ia- architecture. an implementation of the mach operating system on the rt-pc also uses this technique.
advantages and disadvantages:
reduced memory space –
inverted pagetables typically reduces the amount of memory required to store the page tables to a size bound of physical memory. the maximum number of entries could be the number of page frames in the physical memory.
inverted pagetables typically reduces the amount of memory required to store the page tables to a size bound of physical memory. the maximum number of entries could be the number of page frames in the physical memory. longer lookup time –
inverted page tables are sorted in order of frame number but the memory look-up takes place with respect to the virtual address, so, it usually takes a longer time to find the appropriate entry but often these page tables are implemented using hash data structures for a faster lookup.
inverted page tables are sorted in order of frame number but the memory look-up takes place with respect to the virtual address, so, it usually takes a longer time to find the appropriate entry but often these page tables are implemented using hash data structures for a faster lookup. difficult shared memory implementation –
as the inverted page table stores a single entry for each frame, it becomes difficult to implement the shared memory in the page tables. chaining techniques are used to map more than one virtual address to the entry specified in order of frame number.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : vaibhavrai
a computer has sufficient amount of physical memory but most of times we need more so we swap some memory on disk. swap space is a space on hard disk which is a substitute of physical memory. it is used as virtual memory which contains process memory image. whenever our computer run short of physical memory it uses it’s virtual memory and stores information in memory on disk. swap space helps the computer’s operating system in pretending that it have more ram than it actually has. it is also called as swap file.this interchange of data between virtual memory and real memory is called as swapping and space on disk as “swap space”.
virtual memory is a combination of ram and disk space that running processes can use. swap space is the portion of virtual memory that is on the hard disk, used when ram is full.
swap space can be useful to computer in various ways:
it can be used as a single contiguous memory which reduces i/o operations to read or write a file.
applications which are not used or are used less can be kept in swap file.
having sufficient swap file helps the system keep some physical memory free all the time.
the space in physical memory which has been freed due to swap space can be used by os for some other important tasks.
in operating systems such as windows, linux, etc systems provide a certain amount of swap space by default which can be changed by users according to their needs. if you don’t want to use virtual memory you can easily disable it all together but in case if you run out of memory then kernel will kill some of the processes in order to create a sufficient amount of space in physical memory. so it totally depends upon user whether he wants to use swap space or not.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
the computer hardware traps to the kernel and program counter (pc) is saved on the stack. current instruction state information is saved in cpu registers.
an assembly program is started to save the general registers and other volatile information to keep the os from destroying it.
operating system finds that a page fault has occurred and tries to find out which virtual page is needed. some times hardware register contains this required information. if not, the operating system must retrieve pc, fetch instruction and find out what it was doing when the fault occurred.
once virtual address caused page fault is known, system checks to see if address is valid and checks if there is no protection access problem.
if the virtual address is valid, the system checks to see if a page frame is free. if no frames are free, the page replacement algorithm is run to remove a page.
if frame selected is dirty, page is scheduled for transfer to disk, context switch takes place, fault process is suspended and another process is made to run until disk transfer is completed.
as soon as page frame is clean, operating system looks up disk address where needed page is, schedules disk operation to bring it in.
when disk interrupt indicates page has arrived, page tables are updated to reflect its position, and frame marked as being in normal state.
faulting instruction is backed up to state it had when it began and pc is reset. faulting is scheduled, operating system returns to routine that called it.
assembly routine reloads register and other state information, returns to user space to continue execution.
in operating systems, memory management is the function responsible for allocating and managing computer’s main memory. memory management function keeps track of the status of each memory location, either allocated or free to ensure effective and efficient use of primary memory.
there are two memory management techniques: contiguous, and non-contiguous. in contiguous technique, executing process must be loaded entirely in main-memory. contiguous technique can be divided into:
fixed (or static) partitioning variable (or dynamic) partitioning
fixed partitioning:
this is the oldest and simplest technique used to put more than one processes in the main memory. in this partitioning, number of partitions (non-overlapping) in ram are fixed but size of each partition may or may not be same. as it is contiguous allocation, hence no spanning is allowed. here partition are made before execution or during system configure.
as illustrated in above figure, first process is only consuming mb out of mb in the main memory.
hence, internal fragmentation in first block is (-) = mb.
sum of internal fragmentation in every block = (-)+(-)+(-)+(-)= +++ = mb.
suppose process p of size mb comes. but this process cannot be accommodated inspite of available free space because of contiguous allocation (as spanning is not allowed). hence, mb becomes part of external fragmentation.
there are some advantages and disadvantages of fixed partitioning.
advantages of fixed partitioning –
easy to implement:
algorithms needed to implement fixed partitioning are easy to implement. it simply requires putting a process into certain partition without focussing on the emergence of internal and external fragmentation. little os overhead:
processing of fixed partitioning require lesser excess and indirect computational power.
disadvantages of fixed partitioning –
internal fragmentation:
main memory use is inefficient. any program, no matter how small, occupies an entire partition. this can cause internal fragmentation. external fragmentation:
the total unused space (as stated above) of various partitions cannot be used to load the processes even though there is space available but not in the contiguous form (as spanning is not allowed). limit process size:
process of size greater than size of partition in main memory cannot be accommodated. partition size cannot be varied according to the size of incoming process’s size. hence, process size of mb in above stated example is invalid. limitation on degree of multiprogramming:
partition in main memory are made before execution or during system configure. main memory is divided into fixed number of partition. suppose if there are partitions in ram and are the number of processes, then condition must be fulfilled. number of processes greater than number of partitions in ram is invalid in fixed partitioning.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : ayushnigam
a process is divided into segments. the chunks that a program is divided into which are not necessarily all of the same sizes are called segments. segmentation gives user’s view of the process which paging does not give. here the user’s view is mapped to physical memory.
there are types of segmentation:
virtual memory segmentation –
each process is divided into a number of segments, not all of which are resident at any one point in time. simple segmentation –
each process is divided into a number of segments, all of which are loaded into memory at run time, though not necessarily contiguously.
there is no simple relationship between logical addresses and physical addresses in segmentation. a table stores the information about all such segments and is called segment table.
segment table – it maps two-dimensional logical address into one-dimensional physical address. it’s each table entry has:
base address: it contains the starting physical address where the segments reside in memory.
it contains the starting physical address where the segments reside in memory. limit: it specifies the length of the segment.
translation of two dimensional logical address to one dimensional physical address.
address generated by the cpu is divided into:
segment number (s): number of bits required to represent the segment.
number of bits required to represent the segment. segment offset (d): number of bits required to represent the size of the segment.
advantages of segmentation –
no internal fragmentation.
segment table consumes less space in comparison to page table in paging.
disadvantage of segmentation –
as processes are loaded and removed from the memory, the free memory space is broken into little pieces, causing external fragmentation.
this article has been contributed by vikash kumar. please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : vaibhavrai
prerequisite – segmentation
segmentation is the process in which the main memory of the computer is logically divided into different segments and each segment has its own base address. it is basically used to enhance the speed of execution of the computer system, so that the processor is able to fetch and execute the data from the memory easily and fast.
need for segmentation –
the bus interface unit (biu) contains four  bit special purpose registers (mentioned below) called as segment registers.
points to the data segment of the memory where the data is stored. extra segment register (es): also refers to a segment in the memory which is another data segment in the memory.
also refers to a segment in the memory which is another data segment in the memory. stack segment register (ss): is used for addressing stack segment of the memory. the stack segment is that segment of memory which is used to store stack data.
the number of address lines in  is ,  biu will send bit address, so as to access one of the mb memory locations. the four segment registers actually contain the upper  bits of the starting addresses of the four memory segments of  kb each with which the  is working at that instant of time. a segment is a logical unit of memory that may be up to  kilobytes long. each segment is made up of contiguous memory locations. it is an independent, separately addressable unit. starting address will always be changing. it will not be fixed.
note that the  does not work the whole mb memory at any given time. however, it works only with four kb segments within the whole mb memory.
below is the one way of positioning four  kilobyte segments within the m byte memory space of an .
types of segmentation –
overlapping segment – a segment starts at a particular address and its maximum size can go up to kilobytes. but if another segment starts along with this kilobytes location of the first segment, then the two are said to be overlapping segment. non-overlapped segment – a segment starts at a particular address and its maximum size can go up to kilobytes. but if another segment starts before this kilobytes location of the first segment, then the two segments are said to be non-overlapped segment.
rules of segmentation segmentation process follows some rules as follows:
prerequisite: partition allocation methods
what is next fit ?
next fit is a modified version of ‘first fit’. it begins as the first fit to find a free partition but when called next time it starts searching from where it left off, not from the beginning. this policy makes use of a roving pointer. the pointer moves along the memory chain to search for a next fit. this helps in, to avoid the usage of memory always from the head (beginning) of the free block chain.
what are its advantage over first fit ?
first fit is a straight and fast algorithm, but tends to cut large portion of free parts into small pieces due to which, processes that need a large portion of memory block would not get anything even if the sum of all small pieces is greater than it required which is so-called external fragmentation problem.
another problem of the first fit is that it tends to allocate memory parts at the beginning of the memory, which may lead to more internal fragments at the beginning. next fit tries to address this problem by starting the search for the free portion of parts not from the start of the memory, but from where it ends last time.
next fit is a very fast searching algorithm and is also comparatively faster than first fit and best fit memory management algorithms.
example: input : blocksize[] = {, , }; processsize[] = {, , }; output: process no. process size block no.         not allocated
algorithm:
input the number of memory blocks and their sizes and initializes all the blocks as free. input the number of processes and their sizes. start by picking each process and check if it can be assigned to the current block, if yes, allocate it the required memory and check for next process but from the block where we left not from starting. if the current block size is smaller then keep checking the further blocks.
process no.\tprocess size\tblock no.
process no.\tprocess size\tblock no.
process no.\tprocess size\tblock no.
process no.\tprocess size\tblock no.
" ; for ( $i = ; $i < $n ; $i ++) { echo " " .( $i + ). "\t\t" . $processsize [ $i ]. "\t\t" ; if ( $allocation [ $i ] != -) echo ( $allocation [ $i ] + ); else echo "not allocated" ; echo "
output:
process no. process size block no.         
this article is contributed by akash gupta. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
the main problem in fixed partitioning is the size of a process has to be limited by the maximum size of the partition, which means a process can never be span over another.in order to solve this problem, earlier people have used some solution which is called as overlays.
the concept of overlays is that whenever a process is running it will not use the complete program at the same time, it will use only some part of it.then overlays concept says that whatever part you required, you load it an once the part is done, then you just unload it, means just pull it back and get the new part you required and run it.
formally,
sometimes it happens that compare to the size of the biggest partition, the size of the program will be even more, then, in that case, you should go with overlays.
so overlay is a technique to run a program that is bigger than the size of the physical memory by keeping only those instructions and data that are needed at any given time.divide the program into modules in such a way that not all modules need to be in the memory at the same time.
advantage –
reduce memory requirement
reduce time requirement
disadvantage –
overlap map must be specified by programmer
programmer must know memory requirement
overlaped module must be completely disjoint
programmming design of overlays structure is complex and not possible in all cases
example –
pass .......................kb pass .......................kb symbol table.................kb common routine...............kb
question –
the overlay tree for a program is as shown below:
what will be the size of the partition (in physical memory) required to load (and
run) this program?
(a)  kb (b)  kb (c)  kb (d)  kb
explanation –
using the overlay concept we need not actually have the entire program inside the main memory.only we need to have the part which are required at that instance of time, either we need root-a-d or root-a-e or root-b-f or root-c-g part.
root+a+d = kb + kb + kb = kb root+a+e = kb + kb + kb = kb root+b+f = kb + kb + kb = kb root+c+g = kb + kb + kb = kb
so if we have kb size of partition then we can run any of them.
answer -(b) kb
this article is contributed by samit mandal. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
in an operating system that uses paging for memory management, a page replacement algorithm is needed to decide which page needs to be replaced when new page comes in.
page fault – a page fault happens when a running program accesses a memory page that is mapped into the virtual address space, but not loaded in physical memory.
since actual physical memory is much smaller than virtual memory, page faults happen. in case of page fault, operating system might have to replace one of the existing pages with the newly needed page. different page replacement algorithms suggest different ways to decide which page to replace. the target for all algorithms is to reduce the number of page faults.
page replacement algorithms :
first in first out (fifo) –
this is the simplest page replacement algorithm. in this algorithm, the operating system keeps track of all pages in the memory in a queue, the oldest page is in the front of the queue. when a page needs to be replaced page in the front of the queue is selected for removal.
example- consider page reference string , , , , ,  with  page frames.find number of page faults.
initially all slots are empty, so when , ,  came they are allocated to the empty slots —>  page faults.
when  comes, it is already in memory so —>  page faults.
then  comes, it is not available in memory so it replaces the oldest page slot i.e . —>  page fault.
 comes, it is also not available in memory so it replaces the oldest page slot i.e  —>  page fault.
finally when  come it is not avilable so it replaces   page fault belady’s anomaly – belady’s anomaly proves that it is possible to have more page faults when increasing the number of page frames while using the first in first out (fifo) page replacement algorithm. for example, if we consider reference string , , , , , , , , , , ,  and  slots, we get  total page faults, but if we increase slots to , we get  page faults.
this is the simplest page replacement algorithm. in this algorithm, the operating system keeps track of all pages in the memory in a queue, the oldest page is in the front of the queue. when a page needs to be replaced page in the front of the queue is selected for removal. optimal page replacement –
in this algorithm, pages are replaced which would not be used for the longest duration of time in the future. example-: consider the page 
prerequisite: page replacement algorithms
in operating systems that use paging for memory management, page replacement algorithm are needed to decide which page needed to be replaced when new page comes in. whenever a new page is referred and not present in memory, page fault occurs and operating system replaces one of the existing pages with newly needed page. different page replacement algorithms suggest different ways to decide which page to replace. the target for all algorithms is to reduce number of page faults.
in least recently used (lru) algorithm is a greedy algorithm where the page to be replaced is least recently used. the idea is based on locality of reference, the least recently used page is not likely
let say the page reference string              . initially we have  page slots empty.
initially all slots are empty, so when     are allocated to the empty slots —>  page faults
 is already their so —>  page fault.
when  came it will take the place of  because it is least recently used —> page fault
 is already in memory so —>  page fault.
 will takes place of  —>  page fault
now for the further page reference string —>  page fault because they are already available in the memory.
given memory capacity (as number of pages it can hold) and a string representing pages to be referred, write a function to find number of page faults.
let capacity be the number of pages that memory can hold. let set be the current set of pages in memory. - start traversing the pages. i) if set holds less pages than capacity. a) insert page into the set one by one until the size of set reaches capacity or all page requests are processed. b) simultaneously maintain the recent occurred index of each page in a map called indexes. c) increment page fault ii) else if current page is present in set, do nothing. else a) find the page in the set that was least recently used. we find it using index array. we basically need to replace the page with minimum index. b) replace the found page with current page. c) increment page faults. d) update index of current page. . return page faults.
below is implementation of above steps.
another approach: (without using hashmap)
output:
note : we can also find the number of page hits. just have to maintain a separate count.
if the current page is already in the memory then that must be count as page-hit.
we will discuss other page-replacement algorithms in further sets.
this article is contributed by sahil chhabra. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
prerequisite: page replacement algorithms
in operating systems, whenever a new page is referred and not present in memory, page fault occurs and operating system replaces one of the existing pages with newly needed page. different page replacement algorithms suggest different ways to decide which page to replace. the target for all algorithms is to reduce number of page faults.
in this algorithm, os replaces the page that will not be used for the longest period of time in future.
examples :
input : number of frames, fn =  reference string, pg[] = {, , , , , , , , , , , , , , , , , , , }; output : no. of hits =  no. of misses =  input : number of frames, fn =  reference string, pg[] = {, , , , , , , , , , , , }; output : no. of hits =  no. of misses = 
the idea is simple, for every reference we do following :
if referred page is already present, increment hit count. if not present, find if a page that is never referenced in future. if such a page exists, replace this page with new page. if no such page exists, find a page that is referenced farthest in future. replace this page with new page.
output:
no. of hits =  no. of misses = 
the above implementation can optimized using hashing. we can use an unordered_set in place of vector so that search operation can be done in o() time.
note that optimal page replacement algorithm is not practical as we cannot predict future. however it is used as a reference for other page replacement algorithms.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
least frequently used (lfu) is a caching algorithm in which the least frequently used cache block is removed whenever the cache is overflowed. in lfu we check the old page as well as the frequency of that page and if the frequency of the page is larger than the old page we cannot remove it and if all the old pages are having same frequency then take last i.e fifo method for that and remove that page.
min-heap data structure is a good option to implement this algorithm, as it handles insertion, deletion, and update in logarithmic time complexity. a tie can be resolved by removing the least recently used cache block. the following two containers have been used to solve the problem:
a vector of integer pairs has been used to represent the cache, where each pair consists of the block number and the number of times it has been used. the vector is ordered in the form of a min-heap, which allows us to access the least frequently used block in constant time.
a hashmap has been used to store the indices of the cache blocks which allows searching in constant time.
below is the implementation of the above approach:
" ; v[] = v[--n]; heapify(v, m, , n); } v[n++] = make_pair(value, ); m.insert(make_pair(value, n - )); int i = n - ; while (i && v[parent(i)].second > v[i].second) { m[v[i].first] = parent(i); m[v[parent(i)].first] = i; swap(v[i], v[parent(i)]); i = parent(i); } cout << "cache block " << value << " inserted.
" ; } void refer(vector >& cache, unordered_map& indices, int value, int & cache_size) { if (indices.find(value) == indices.end()) insert(cache, indices, value, cache_size); else increment(cache, indices, indices[value], cache_size); } int main() { int cache_max_size = , cache_size = ; vector > cache(cache_max_size); unordered_map indices; refer(cache, indices, , cache_size); refer(cache, indices, , cache_size); refer(cache, indices, , cache_size); refer(cache, indices, , cache_size); refer(cache, indices, , cache_size); refer(cache, indices, , cache_size); refer(cache, indices, , cache_size); return ; }
output: cache block  inserted. cache block  inserted. cache block  inserted. cache block  inserted. cache block  removed. cache block  inserted.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : vaibhav
prerequisite – page replacement algorithms
apart from lru, opt and fifo page replacement policies, we also have the second chance/clock page replacement policy. in the second chance page replacement policy, the candidate pages for removal are considered in a round robin matter, and a page that has been accessed between consecutive considerations will not be replaced. the page replaced is the one that, when considered in a round robin matter, has not been accessed since its last consideration.
it can be implemented by adding a “second chance” bit to each memory frame-every time the frame is considered (due to a reference made to the page inside it), this bit is set to , which gives the page a second chance, as when we consider the candidate page for replacement, we replace the first one with this bit set to  (while zeroing out bits of the other pages we see in the process). thus, a page with the “second chance” bit set to  is never replaced during the first consideration and will only be replaced if all the other pages deserve a second chance too!
example –
let’s say the reference string is                   and we have  frames. let’s see how the algorithm proceeds by tracking the second chance bit and the pointer.
initially, all frames are empty so after first  passes they will be filled with {, , } and the second chance array will be {, , } as none has been referenced yet. also, the pointer will cycle back to .
create an array frames to track the pages currently in memory and another boolean array second_chance to track whether that page has been accessed since it’s last replacement (that is if it deserves a second chance or not) and a variable pointer to track the target for replacement.
start traversing the array arr. if the page already exists, simply set its corresponding element in second_chance to true and return. if the page doesn’t exist, check whether the space pointed to by pointer is empty (indicating cache isn’t full yet) – if so, we will put the element there and return, else we’ll traverse the array arr one by one (cyclically using the value of pointer), marking all corresponding second_chance elements as false, till we find a one that’s already false. that is the most suitable page for replacement, so we do so and return. finally, we report the page fault count.
total page faults were  total page faults were 
note:
the arrays arr and second_chance can be replaced and combined together via a hashmap (with element as key, true/false as value) to speed up search. time complexity of this method is o(number_of_frames*reference_string_length) or o(mn) but since number of frames will be a constant in an operating system (as main memory size is fixed), it is simply o(n) [same as hashmap approach, but that will have lower constants] second chance algorithm may suffer from belady’s anomaly.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
uni_omni cse b tech th year at iit bhubaneswar||summer intern  iit roorkee||summer intern  goldman sachs
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : ajaykumar, nikhilrathor
prerequisite – virtual memory
thrashing is a condition or a situation when the system is spending a major portion of its time in servicing the page faults, but the actual processing done is very negligible.
the basic concept involved is that if a process is allocated too few frames, then there will be too many and too frequent page faults. as a result, no useful work would be done by the cpu and the cpu utilisation would fall drastically. the long-term scheduler would then try to improve the cpu utilisation by loading some more processes into the memory thereby increasing the degree of multiprogramming. this would result in a further decrease in the cpu utilization triggering a chained reaction of higher page faults followed by an increase in the degree of multiprogramming, called thrashing.
locality model –
a locality is a set of pages that are actively used together. the locality model states that as a process executes, it moves from one locality to another. a program is generally composed of several different localities which may overlap.
for example when a function is called, it defines a new locality where memory 
prerequisite – buddy system
two strategies for managing free memory that is assigned to kernel processes:
. buddy system –
buddy allocation system is an algorithm in which a larger memory block is divided into small parts to satisfy the request. this algorithm is used to give best fit. the two smaller parts of block are of equal size and called as buddies. in the same manner one of the two buddies will further divide into smaller parts until the request is fulfilled. benefit of this technique is that the two buddies can combine to form the block of larger size according to the memory request.
example – if the request of kb is made then block of size kb is allocated.
four types of buddy system –
binary buddy system fibonacci buddy system weighted buddy system tertiary buddy system
why buddy system?
if the partition size and procees size are different then poor match occurs and may use space inefficiently.
it is easy to implement and efficient then dynamic allocation.
binary buddy system –
the buddy system maintains a list of the free blocks of each size (called a free list), so that it is easy to find ablock of the desired size, if one is available. if no block of the requested size is available, allocate searches for the first nonempty list for blocks of atleast the size requested. in either case, a block is removed from the free list.
example – assume the size of memory segment is initially kb and the kernel rquests kb of memory. the segment is initially divided into two buddies. let we call a and a each kb in size. one of these buddies is further divided into two kb buddies let say b and b. but the next highest power of kb is kb so, either b or b is further divided into two kb buddies(c and c) and finally one of these buddies is used to satisfy the kb request. a split block can only be merged with its unique buddy block, which then reforms the larger block they were split from.
fibonacci buddy system –
this is the system in which blocks are divided into sizes which are fibonacci numbers. it satisfy the following relation:
z i = z (i-) +z (i-)
, , , , , , , , , , , , , , . the address calculation for the binary and weighted buddy systems is straight forward, but the original procedure for the fibonacci buddy system was either limited to a small, fixed number of block sizes or a time consuming computation.
advantages –
in comparison to other simpler techniques such as dynamic allocation, the buddy memory system has little external fragmentation.
the buddy memory allocation system is implemented with the use of a binary tree to represent used or unused split memory blocks.
the buddy system is very fast to allocate or deallocate memory.
in buddy systems, the cost to allocate and free a block of memory is low compared to that of best-fit or first-fit algorithms.
other advantage is coalescing.
address calculation is easy.
what is coalescing?
it is defined as how quickly adjacent buddies can be combined to form larger segments this is known as coalescing.
for example, when the kernel releases the c unit it was allocated, the system can coalesce c and c into a kb segment. this segment b can in turn be coalesced with its buddy b to form a kb segment. ultimately we can end up with the original kb segment.
drawback –
the main drawback in buddy system is internal fragmentation as larger block of memory is acquired then required. for example if a  kb request is made then it can only be satisfied by  kb segment and reamining memory is wasted.
. slab allocation –
a second strategy for allocating kernel memory is known as slab allocation. it eliminates fragmentation caused by allocations and deallocations. this method is used to retain allocated memory that contains a data object of a certain type for reuse upon subsequent allocations of objects of the same type. in slab allocation memory chunks suitable to fit data objects of certain type or size are preallocated. cache does not free the space immediately after use although it keeps track of data which are required frequently so that whenever request is made the data will reach very fast. two terms required are:
slab – a slab is made up of one or more physically contiguous pages. the slab is the actual container of data associated with objects of the specific kind of the containing cache.
a slab is made up of one or more physically contiguous pages. the slab is the actual container of data associated with objects of the specific kind of the containing cache. cache – cache represents a small amount of very fast memory. a cache consists of one or more slabs. there is a single cache for each unique kernel data structure.
example –
a separate cache for a data structure representing processes descriptors
separate cache for file objects
separate cache for semaphores etc.
each cache is populated with objects that are instantiations of the kernel data structure the cache represents. for example the cache representing semaphores stores instances of semaphore objects, the cache representing process descriptors stores instances of process descriptor objects.
implementation –
the slab allocation algorithm uses caches to store kernel objects. when a cache is created a number of objects which are initially marked as free are allocated to the cache. the number of objects in the cache depends on size of the associated slab.
example – a  kb slab (made up of three contiguous  kb pages) could store six  kb objects. initially all objects in the cache are marked as free. when a new object for a kernel data structure is needed, the allocator can assign any free object from the cache to satisfy the request. the object assigned from the cache is marked as used.
in linux, a slab may in one of three possible states:
full – all objects in the slab are marked as used empty – all objects in the slab are marked as free partial – the slab consists of both
the slab allocator first attempts to satisfy the request with a free object in a partial slab. if none exists, a free object is assigned from an empty slab. if no empty slabs are available, a new slab is allocated from contiguous physical pages and assigned to a cache.
benefits of slab allocator –
no memory is wasted due to fragmentation because each unique kernel data structure has an associated cache.
memory request can be satisfied quickly.
the slab allocating scheme is particularly effective for managing when objects are frequently allocated or deallocated. the act of allocating and releasing memory can be a time consuming process. however, objects are created in advance and thus can be quickly allocated from the cache. when the kernel has finished with an object and releases it, it is marked as free and return to its cache, thus making it immediately available for subsequent request from the kernel.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
prerequisite – buddy system
question: write a program to implement the buddy system of memory allocation in operating systems.
explanation –
the buddy system is implemented as follows- a list of free nodes, of all the different possible powers of , is maintained at all times (so if total memory size is  mb, we’d have  free lists to track-one for blocks of size  byte,  for  bytes, next for  bytes and so on).
when a request for allocation comes, we look for the smallest block bigger than it. if such a block is found on the free list, the allocation is done (say, the request is of  kb and the free list tracking  kb blocks has at least one element in it), else we traverse the free list upwards till we find a big enough block. then we keep splitting it in two blocks-one for adding to the next free list (of smaller size), one to traverse down the tree till we reach the target and return the requested memory block to the user. if no such allocation is possible, we simply return null.
example:
let us see how the algorithm proceeds by tracking a memory block of size  kb. initially, the free list is: {}, {}, {}, {}, {}, {}, {}, { (, ) }
request:  bytes
no such block found, so we traverse up and split the - block into -, -; we add - to list tracking  byte blocks and pass - downwards; again it is split into - and -; since we have found the required block size, we add - to list tracking  byte blocks and return - to user.
list is: {}, {}, {}, {}, {}, { (, ) }, { (, ) }, {}
 bytes no such block found, so we traverse up and split the - block into -, -; we add - to list tracking  byte blocks and pass - downwards; again it is split into - and -; since we have found the required block size, we add - to list tracking  byte blocks and return - to user. list is: {}, {}, {}, {}, {}, { (, ) }, { (, ) }, {} request:  bytes
no such block found-split block - into two blocks, namely - and -; then split - into - and -; finally, return - to user (internal fragmentation of  byte occurs)
list is: {}, {}, {}, { (, ) }, { (, ) }, {}, { (, ) }, {}
 bytes no such block found-split block - into two blocks, namely - and -; then split - into - and -; finally, return - to user (internal fragmentation of  byte occurs) list is: {}, {}, {}, { (, ) }, { (, ) }, {}, { (, ) }, {} request:  bytes
straight up memory segment - will be allocated as it already exists.
list is: {}, {}, {}, { (, ) }, { (, ) }, {}, {}, {}
 bytes straight up memory segment - will be allocated as it already exists. list is: {}, {}, {}, { (, ) }, { (, ) }, {}, {}, {} request:  bytes
result: not allocated
the result will be as follows:
figure – buddy allocation- shows the starting address of next possible block (if main memory size ever increases)
implementation –
" ; mp[temp.first] = temp.second - temp.first + ; } else { int i; for (i = n + ; i < size; i++) { if (free_list[i].size() != ) break ; } if (i == size) { cout << "sorry, failed to allocate memory
" ; } else { pair temp; temp = free_list[i][]; free_list[i].erase(free_list[i].begin()); i--; for (; i >= n; i--) { pair pair, pair; pair = make_pair(temp.first, temp.first + (temp.second - temp.first) / ); pair = make_pair(temp.first + (temp.second - temp.first + ) / , temp.second); free_list[i].push_back(pair); free_list[i].push_back(pair); temp = free_list[i][]; free_list[i].erase(free_list[i].begin()); } cout << "memory from " << temp.first << " to " << temp.second << " allocated" << "
output: memory from  to  allocated memory from  to  allocated memory from  to  allocated sorry, failed to allocate memory
time complexity –
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
uni_omni cse b tech th year at iit bhubaneswar||summer intern  iit roorkee||summer intern  goldman sachs
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : ak, ajaykumar, sarthak_eddy
prerequisite – buddy allocation | set 
question: write a program to implement the buddy system of memory allocation and deallocation in operating systems.
explanation –
as we already know from set , the allocation is done via the usage of free lists. now, for deallocation, we will maintain an extra data structure-a map (unordered_set in c++, hashmap in java) with the starting address of segment as key and size of the segment as value and update it whenever an allocation request comes. now, when deallocation request comes, we will first check the map to see if it is a valid request. if so, we will then add the block to the free list tracking blocks of its sizes. then, we will search the free list to see if it’s buddy is free-if so, we will merge the blocks and place them on the free list above them (which tracks blocks of double the size), else we will not coalesce and simply return after that.
how to know which block is a given block’s buddy?
let us define two terms-buddynumber and buddyaddress. the buddynumber of a block is calculated by the formula:
(base_address-starting_address_of_main_memory)/block_size
we note that this is always an integer, as both numerator and denominator are powers of . now, a block will be another block’s buddy if both of them were formed by the splitting of the same bigger block. for example, if  consecutive allocation requests of  bytes come, we will end up with blocks -, -, -, - where blocks - and - are buddies (as they were formed by splitting block -) but - and - aren’t. the buddyaddress of a block is the starting index of its buddy block, given by the formula:
block_starting_address+block_size (if buddynumber is even) block_starting_address-block_size (if buddynumber is odd)
thus, all we have to do is find this buddyaddress in the free list (by comparing with all the starting addresses in that particular list), and if present, coalescing can be done.
examples:
let us see how the algorithm proceeds by tracking a memory block of size  kb. initially, the free list is: {}, {}, {}, {}, {}, {}, {}, { (, ) }
allocation request:  bytes
no such block found, so we traverse up and split the - block into -, -; we add - to list tracking  byte blocks and pass - downwards; again it is split into - and -; we add - to list tracking  byte blocks, passing - downwards; one more spli is done and - is returned to user while - is added to free list tracking  byte blocks.
list is: {}, {}, {}, {}, { (, ) }, { (, ) }, { (, ) }, {}
 bytes no such block found, so we traverse up and split the - block into -, -; we add - to list tracking  byte blocks and pass - downwards; again it is split into - and -; we add - to list tracking  byte blocks, passing - downwards; one more spli is done and - is returned to user while - is added to free list tracking  byte blocks. list is: {}, {}, {}, {}, { (, ) }, { (, ) }, { (, ) }, {} allocation request:  bytes
straight up memory segment - will be allocated as it already exists.
list is: {}, {}, {}, {}, {}, { (, ) }, { (, ) }, {}
 bytes straight up memory segment - will be allocated as it already exists. list is: {}, {}, {}, {}, {}, { (, ) }, { (, ) }, {} allocation request:  bytes
no such block found, so we will traverse up to block - and split it to blocks - and -; we will add - to list tracking  byte blocks and return - to user.
list is: {}, {}, {}, {}, { (, ) }, {}, { (, ) }, {}
 bytes no such block found, so we will traverse up to block - and split it to blocks - and -; we will add - to list tracking  byte blocks and return - to user. list is: {}, {}, {}, {}, { (, ) }, {}, { (, ) }, {} allocation request:  bytes
straight up memory segment - will be allocated as it already exists.
list is: {}, {}, {}, {}, {}, {}, { (, ) }, {}
 bytes straight up memory segment - will be allocated as it already exists. list is: {}, {}, {}, {}, {}, {}, { (, ) }, {} deallocation request: startindex = 
deallocation will be done but no coalesscing is possible as it’s buddynumber is  and buddyaddress is  (via the formula), none of which is in the free list.
list is: {}, {}, {}, {}, { (, ) }, {}, { (, ) }, {}
startindex =  deallocation will be done but no coalesscing is possible as it’s is  and is  (via the formula), none of which is in the free list. list is: {}, {}, {}, {}, { (, ) }, {}, { (, ) }, {} deallocation request: startindex = 
result: invalid request, as this segmeent was never allocated.
list is: {}, {}, {}, {}, { (, ) }, {}, { (, ) }, {}
startindex =  result: invalid request, as this segmeent was never allocated. list is: {}, {}, {}, {}, { (, ) }, {}, { (, ) }, {} deallocation request: startindex = 
deallocation will be done but no coalesscing is possible as the buddynumber of the blocks are  and  buddyaddress of the blocks are  and , repectively, none of which is in the free list.
list is: {}, {}, {}, {}, { (, ), (-) }, {}, { (, ) }, {}
startindex =  deallocation will be done but no coalesscing is possible as the of the blocks are  and  of the blocks are  and , repectively, none of which is in the free list. list is: {}, {}, {}, {}, { (, ), (-) }, {}, { (, ) }, {} deallocation request: startindex = 
deallocation will be done and coealsecing of the blocks - and - will also be done as the buddyaddress of block - is , which is present in the free list tracking  byte blocks.
list is: {}, {}, {}, {}, { (-) }, { (, ) }, { (, ) }, {}
figure – buddy algorithm-allocation and deallocation
implementation –
below is the complete program.
" ; mp[temp.first] = temp.second - temp.first + ; } else { int i; for (i = x + ; i < size; i++) { if (arr[i].size() != ) break ; } if (i == size) { cout << "sorry, failed to allocate memory
" ; } else { pair temp; temp = arr[i][]; arr[i].erase(arr[i].begin()); i--; for (;i >= x; i--) { pair pair, pair; pair = make_pair(temp.first, temp.first + (temp.second - temp.first) / ); pair = make_pair(temp.first + (temp.second - temp.first + ) / , temp.second); arr[i].push_back(pair); arr[i].push_back(pair); temp = arr[i][]; arr[i].erase(arr[i].begin()); } cout << "memory from " << temp.first << " to " << temp.second << " allocate" << "
" ; mp[temp.first] = temp.second - temp.first + ; } } } void deallocate( int id) { if (mp.find(id) == mp.end()) { cout << "sorry, invalid free request
" ; return ; } int n = ceil ( log (mp[id]) / log ()); int i, buddynumber, buddyaddress; arr[n].push_back(make_pair(id, id + pow (, n) - )); cout << "memory block from " << id << " to " << id + pow (, n) -  << " freed
" ; buddynumber = id / mp[id]; if (buddynumber %  != ) buddyaddress = id - pow (, n); else buddyaddress = id + pow (, n); for (i = ; i < arr[n].size(); i++) { if (arr[n][i].first == buddyaddress) { if (buddynumber %  == ) { arr[n + ].push_back(make_pair(id, id +  * ( pow (, n) - ))); cout << "coalescing of blocks starting at " << id << " and " << buddyaddress << " was done" << "
" ; } else { arr[n + ].push_back(make_pair( buddyaddress, buddyaddress +  * ( pow (, n)))); cout << "coalescing of blocks starting at " << buddyaddress << " and " << id << " was done" << "
output: memory from  to  allocated memory from  to  allocated memory from  to  allocated memory from  to  allocated memory block from  to  freed sorry, invalid free request memory block from  to  freed memory block from  to  freed coalescing of blocks starting at  and  was done
time complexity –
as already discussed in set , time complexity of allocation is o(log(n)). for deallocation, in the worst case, all the allocated blocks can be of size  unit, which will then require o(n) time to scan the list for coalescing. however, in practice, it is highly unlikely that such an allocation will happen so it is generally much faster than linear time.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
uni_omni cse b tech th year at iit bhubaneswar||summer intern  iit roorkee||summer intern  goldman sachs
static linking and static libraries is the result of the linker making copy of all used library functions to the executable file. static linking creates larger binary files, and need more space on disk and main memory. examples of static libraries (libraries which are statically linked) are, .a files in linux and .lib files in windows.
steps to create a static library let us create and use a static library in unix or unix like os.
. create a c file that contains functions in your library.
we have created only one file for simplicity. we can also create multiple files in a library.
. create a header file for the library
. compile library files.
gcc -c lib_mylib.c -o lib_mylib.o
. create static library. this step is to bundle multiple object files in one static library (see ar for details). the output of this step is static library.
ar rcs lib_mylib.a lib_mylib.o
. now our static library is ready to use. at this point we could just copy lib_mylib.a somewhere else to use it. for demo purposes, let us keep the library in the current directory.
let us create a driver program that uses above created static library.
. create a c file with main function
. compile the driver program.
gcc -c driver.c -o driver.o
. link the compiled driver program to the static library. note that -l. is used to tell that the static library is in current folder (see this for details of -l and -l options).
gcc -o driver driver.o -l. -l_mylib
. run the driver program
./driver fun() called from a static library
following are some important points about static libraries.
. in static libraries, once everything is bundled into your application, you don’t have to worry that the client will have the right library (and version) available on their system.
. one drawback of static libraries is, for any change(up-gradation) in the static libraries, you have to recompile the main program every time.
. one major advantage of static libraries being preferred even now “is speed”. there will be no dynamic querying of symbols in static libraries. many production line software use static libraries even today.
we will soon be covering more points on dynamic libraries and steps to create them.
this article is compiled by abhijit saha and reviewed by geeksforgeeks team. please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
this article is not for those algo geeks. if you are interested in systems related stuff, just read on…
every operating system has its own representation and tool-set to create shared libraries. more or less the concepts are same. on windows every object file (*.obj, *.dll, *.ocx, *.sys, *.exe etc…) follow a format called portalbe executable. even shared libraries (called as dynamic linked libraries or dll in short) are also represented in pe format. the tool-set that is used to create these libraries need to understand the binary format. linux variants follow a format called executable and linkable format (elf). the elf files are position independent (pic) format. shared libraries in linux are referred as shared objects (generally with extension *.so). these are similar to dlls in windows platform. even shared object files follow the elf binary format.
remember, the file extensions (*.dll, *.so, *.a, *.lib, etc…) are just for programmer convenience. they don’t have any significance. all these are binary files. you can name them as you wish. yet ensure you provide absolute paths in building applications.
in general, when we compile an application the steps are simple. compile, link and load. however, it is not simple. these steps are more versatile on modern operating systems.
when we link an application against a shared library, the linker leaves some stubs (unresolved symbols) to be filled at application loading time. these stubs need to be filled by a tool called, dynamic linker at run time or at application loading time. again loading of a library is of two types, static loading and dynamic loading. don’t confuse between static loading vs static linking and dynamic loading vs dynamic linking.
for example, you have built an application that depends on libstdc++.so which is a shared object (dynamic libary). how does the application become aware of required shared libraries? (if you are interested, explore the tools tdump from borland tool set, objdump or nm or readelf tools on linux).
static loading:
in static loading, all of those dependent shared libraries are loaded into memory even before the application starts execution. if loading of any shared library fails, the application won’t run.
a dynamic loader examines application’s dependency on shared libraries. if these libraries are already loaded into the memory, the library address space is mapped to application virtual address space (vas) and the dynamic linker does relocation of unresolved symbols.
if these libraries are not loaded into memory (perhaps your application might be first to invoke the shared library), the loader searches in standard library paths and loads them into memory, then maps and resolves symbols. again loading is big process, if you are interested write your own loader :).
while resolving the symbols, if the dynamic linker not able to find any symbol (may be due to older version of shared library), the application can’t be started.
dynamic loading:
as the name indicates, dynamic loading is about loading of library on demand.
for example, if you want a small functionality from a shared library. why should it be loaded at the application load time and sit in the memory? you can invoke loading of these shared libraries dynamically when you need their functionality. this is called dynamic loading. in this case, the programmer aware of situation ‘when should the library be loaded’. the tool-set and relevant kernel provides api to support dynamic loading, and querying of symbols in the shared library.
more details in later articles.
note: if you come across terms like loadable modules or equivalent terms, don’t mix them with shared libraries. they are different from shared libraries the kernels provide framework to support loadable modules.
working with shared libraries | set 
exercise:
. assuming you have understood the concepts, how do you design an application (e.g. banking) which can upgrade to new shared libraries without re-running the application.
— venki. please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
we have covered basic information about shared libraries in the previous post. in the current article we will learn how to create shared libraries on linux.
prior to that we need to understand how a program is loaded into memory, various (basic) steps involved in the process.
let us see a typical “hello world” program in c. simple hello world program screen image is given below.
we are using ldd to list dependencies of our program binary image. in the screen image, we can see our sample program depends on three binary files namely, linux-vdso.so., libc.so. and /lib/ld-linux-x-.so..
the file vdso is fast implementation of system call interface and some other stuff, it is not our focus (on some older systems you may see different file name in liue of *.vsdo.*). ignore this file. we have interest in the other two files.
the file libc.so. is c implementation of various standard functions. it is the file where we see printf definition needed for our hello world. it is the shared library needed to be loaded into memory to run our hello world program.
creating our own shared library:
let us work with simple shared library on linux. create a file library.c with the following content.
gcc -shared -fpic -o liblibrary.so library.c
create another file application.c with the following content.
in the file application.c we are invoking the function signum which was defined in a shared library. compile the application.c file using the following command.
gcc application.c -l /home/geetanjali/coding/ -llibrary -o sample
if you invoke the executable, the dynamic linker will not be able to find the required shared library. by default it won’t look into current working directory. you have to explicitly instruct the tool chain to provide proper paths. the dynamic linker searches standard paths available in the ld_library_path and also searches in system cache (for details explore the command ldconfig). we have to add our working directory to the ld_library_path environment variable. the following command does the same.
export ld_library_path=/home/geetanjali/coding/:$ld_library_path
you can now invoke our executable as shown.
./sample
sample output on my system is shown below.
note: the path /home/geetanjali/coding/ is working directory path on my machine. you need to use your working directory path where ever it is being used in the above commands.
stay tuned, we haven’t even explored /rd of shared library concepts. more advanced concepts in the later articles.
exercise:
it is workbook like article. you won’t gain much unless you practice and do some research.
. create similar example and write your won function in the shared library. invoke the function in another application.
. is (are) there any other tool(s) which can list dependent libraries?
. what is system cache in the current context? how does the directory /etc/ld.so.conf.d/* related in the current context?
— venki. please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
in computing, a named pipe (also known as a fifo) is one of the methods for intern-process communication.
it is an extension to the traditional pipe concept on unix. a traditional pipe is “unnamed” and lasts only as long as the process.
a named pipe, however, can last as long as the system is up, beyond the life of the process. it can be deleted if no longer used.
usually a named pipe appears as a file and generally processes attach to it for inter-process communication. a fifo file is a special kind of file on the local storage which allows two or more processes to communicate with each other by reading/writing to/from this file.
a fifo special file is entered into the filesystem by calling mkfifo() in c. once we have created a fifo special file in this way, any process can open it for reading or writing, in the same way as an ordinary file. however, it has to be open at both ends simultaneously before you can proceed to do any input or output operations on it.
creating a fifo file: in order to create a fifo file, a function calls i.e. mkfifo is used.
mkfifo() makes a fifo special file with name pathname. here mode specifies the fifo’s permissions. it is modified by the process’s umask in the usual way: the permissions of the created file are (mode & ~umask).
using fifo: as named pipe(fifo) is a kind of file, we can use all the system calls associated with it i.e. open, read, write, close.
example programs to illustrate the named pipe: there are two programs that use the same fifo. program  writes first, then reads. the program  reads first, then writes. they both keep doing it until terminated.
output: run the two programs simultaneously on two terminals.
this article is contributed by kishlay verma. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : paarmitabhargava
often it’s necessary to trace memory usage of the system in order to determine the program that consumes all cpu resources or the program that is responsible to slowing down the activities of the cpu. tracing memory usage also becomes necessary to determine the load on the server. parsing the usage data enables the servers to be able to balance the load and serve the user’s request without slowing down the system.
free displays the amount of memory which is currently available and used by the system(both physical and swapped). free command gathers this data by parsing /proc/meminfo. by default, the amount of memory is display in kilobytes. free command in unix watch -n  free -m watch command is used to execute a program periodically. according to the image above, there is a total of  mb of ram and  mb of swap space allotted to linux system. out of this  mb of ram,  mb is currently used where as  mb is free. similarly for swap space, out of  mb,  mb is use and  mb is free currently in the system.
vmstat vmstat command is used to display virtual memory statistics of the system. this command reports data about the memory, paging, disk and cpu activities, etc. the first use of this command returns the data averages since the last reboot. further uses returns the data based on sampling periods of length delays. vmstat -d reports disk statistics vmstat -s displays the amount of memory used and available top top command displays all the currently running process in the system. this command displays the list of processes and thread currently being handled by the kernel. top command can also be used to monitor the total amount of memory usage. top -h threads-mode operation displays individual thread that are currently in the system. without this command option, a summation of all thread in each process is displayed. /proc/meminfo this file contains all the data about the memory usage. it provides the current memory usage details rather than old stored values. htop htop is an interactive process viewer. this command is similar to top command except that it allows to scroll vertically and horizontally to allows users to view all processes running on the system, along with their full command line as well as viewing them as a process tree, selecting multiple processes and acting on them all at once. working of htop command in unix:
reference:
this article is contributed by mayank kumar. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
a file is a collection of related information that is recorded on secondary storage. or file is a collection of logically related entities. from user’s perspective a file is the smallest allotment of logical secondary storage.
attributes types operations name doc create type exe open size jpg read creation data xis write author c append last modified java truncate protection class delete close
file directories:
collection of files is a file directory. the directory contains information about the files, including attributes, location and ownership. much of this information, especially that is concerned with storage, is managed by the operating system. the directory is itself a file, accessible by various file management routines.
information contained in a device directory are:
name
type
address
current length
maximum length
date last accessed
date last updated
owner id
protection information
operation performed on directory are:
search for a file
create a file
delete a file
list a directory
rename a file
traverse the file system
advantages of maintaining directories are:
efficiency: a file can be located more quickly.
a file can be located more quickly. naming: it becomes convenient for users as two users can have same name for different files or may have different name for same file.
it becomes convenient for users as two users can have same name for different files or may have different name for same file. grouping: logical grouping of files can be done by properties e.g. all java programs, all games etc.
single-level directory
in this a single directory is maintained for all the users.
naming problem: users cannot have same name for two files.
users cannot have same name for two files. grouping problem: users cannot group files according to their need.
two-level directory
in this separate directories for each user is maintained.
path name:due to two levels there is a path name for every file to locate that file.
now,we can have same file name for different user.
searching is efficient in this method.
tree-structured directory :
directory is maintained in the form of a tree. searching is efficient and also there is grouping capability. we have absolute or relative path name for a file.
file allocation methods
. continuous allocation: a single continuous set of blocks is allocated to a file at the time of file creation. thus, this is a pre-allocation strategy, using variable size portions. the file allocation table needs just a single entry for each file, showing the starting block and the length of the file. this method is best from the point of view of the individual sequential file. multiple blocks can be read in at a time to improve i/o performance for sequential processing. it is also easy to retrieve a single block. for example, if a file starts at block b, and the ith block of the file is wanted, its location on secondary storage is simply b+i-.
disadvantage
external fragmentation will occur, making it difficult to find contiguous blocks of space of sufficient length. compaction algorithm will be necessary to free up additional space on disk.
also, with pre-allocation, it is necessary to declare the size of the file at the time of creation.
. linked allocation(non-contiguous allocation) : allocation is on an individual block basis. each block contains a pointer to the next block in the chain. again the file table needs just a single entry for each file, showing the starting block and the length of the file. although pre-allocation is possible, it is more common simply to allocate blocks as needed. any free block can be added to the chain. the blocks need not be continuous. increase in file size is always possible if free disk block is available. there is no external fragmentation because only one block at a time is needed but there can be internal fragmentation but it exists only in the last disk block of file.
disadvantage:
internal fragmentation exists in last disk block of file.
there is an overhead of maintaining the pointer in every disk block.
if the pointer of any disk block is lost, the file will be truncated.
it supports only the sequencial access of files.
. indexed allocation:
it addresses many of the problems of contiguous and chained allocation. in this case, the file allocation table contains a separate one-level index for each file: the index has one entry for each block allocated to the file. allocation may be on the basis of fixed-size blocks or variable-sized blocks. allocation by blocks eliminates external fragmentation, whereas allocation by variable-size blocks improves locality. this allocation technique supports both sequential and direct access to the file and thus is the most popular form of file allocation.
disk free space management
just as the space that is allocated to files must be managed ,so the space that is not currently allocated to any file must be managed. to perform any of the file allocation techniques,it is necessary to know what blocks on the disk are available. thus we need a disk allocation table in addition to a file allocation table.the following are the approaches used for free space management.
bit tables : this method uses a vector containing one bit for each block on the disk. each entry for a  corresponds to a free block and each  corresponds to a block in use.
for example:  in this vector every bit correspond to a particular block and  implies that, that particular block is free and  implies that the block is already occupied. a bit table has the advantage that it is relatively easy to find one or a contiguous group of free blocks. thus, a bit table works well with any of the file allocation methods. another advantage is that it is as small as possible. free block list : in this method, each block is assigned a number sequentially and the list of the numbers of all free blocks is maintained in a reserved block of the disk.
this article is contributed by aakansha yadav
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : raghvanimahesh
unix file system is a logical method of organizing and storing large amounts of information in a way that makes it easy to manage. a file is a smallest unit in which the information is stored. unix file system has several important features. all data in unix is organized into files. all files are organized into directories. these directories are organized into a tree-like structure called the file system.
files in unix system are organized into multi-level hierarchy structure known as a directory tree. at the very top of the file system is a directory called “root” which is represented by a “/”. all other files are “descendants” of root.
directories or files and their description –
/ : the slash / character alone denotes the root of the filesystem tree.
the slash / character alone denotes the root of the filesystem tree. /bin : stands for “binaries” and contains certain fundamental utilities, such as ls or cp, which are generally needed by all users.
stands for “binaries” and contains certain fundamental utilities, such as ls or cp, which are generally needed by all users. /boot : contains all the files that are required for successful booting process.
contains all the files that are required for successful booting process. /dev : stands for “devices”. contains file representations of peripheral devices and pseudo-devices.
stands for “devices”. contains file representations of peripheral devices and pseudo-devices. /etc : contains system-wide configuration files and system databases. originally also contained “dangerous maintenance utilities” such as init,but these have typically been moved to /sbin or elsewhere.
contains system-wide configuration files and system databases. originally also contained “dangerous maintenance utilities” such as init,but these have typically been moved to /sbin or elsewhere. /home : contains the home directories for the users.
contains the home directories for the users. /lib : contains system libraries, and some critical files such as kernel modules or device drivers.
contains system libraries, and some critical files such as kernel modules or device drivers. /media : default mount point for removable devices, such as usb sticks, media players, etc.
default mount point for removable devices, such as usb sticks, media players, etc. /mnt : stands for “mount”. contains filesystem mount points. these are used, for example, if the system uses multiple hard disks or hard disk partitions. it is also often used for remote (network) filesystems, cd-rom/dvd drives, and so on.
stands for “mount”. contains filesystem mount points. these are used, for example, if the system uses multiple hard disks or hard disk partitions. it is also often used for remote (network) filesystems, cd-rom/dvd drives, and so on. /proc : procfs virtual filesystem showing information about processes as files.
procfs virtual filesystem showing information about processes as files. /root : the home directory for the superuser “root” – that is, the system administrator. this account’s home directory is usually on the initial filesystem, and hence not in /home (which may be a mount point for another filesystem) in case specific maintenance needs to be performed, during which other filesystems are not available. such a case could occur, for example, if a hard disk drive suffers physical failures and cannot be properly mounted.
the home directory for the superuser “root” – that is, the system administrator. this account’s home directory is usually on the initial filesystem, and hence not in /home (which may be a mount point for another filesystem) in case specific maintenance needs to be performed, during which other filesystems are not available. such a case could occur, for example, if a hard disk drive suffers physical failures and cannot be properly mounted. /tmp : a place for temporary files. many systems clear this directory upon startup; it might have tmpfs mounted atop it, in which case its contents do not survive a reboot, or it might be explicitly cleared by a startup script at boot time.
a place for temporary files. many systems clear this directory upon startup; it might have tmpfs mounted atop it, in which case its contents do not survive a reboot, or it might be explicitly cleared by a startup script at boot time. /usr : originally the directory holding user home directories,its use has changed. it now holds executables, libraries, and shared resources that are not system critical, like the x window system, kde, perl, etc. however, on some unix systems, some user accounts may still have a home directory that is a direct subdirectory of /usr, such as the default as in minix. (on modern systems, these user accounts are often related to server or system use, and not directly used by a person).
originally the directory holding user home directories,its use has changed. it now holds executables, libraries, and shared resources that are not system critical, like the x window system, kde, perl, etc. however, on some unix systems, some user accounts may still have a home directory that is a direct subdirectory of /usr, such as the default as in minix. (on modern systems, these user accounts are often related to server or system use, and not directly used by a person). /usr/bin : this directory stores all binary programs distributed with the operating system not residing in /bin, /sbin or (rarely) /etc.
this directory stores all binary programs distributed with the operating system not residing in /bin, /sbin or (rarely) /etc. /usr/include : stores the development headers used throughout the system. header files are mostly used by the #include directive in c/c++ programming language.
stores the development headers used throughout the system. header files are mostly used by the directive in c/c++ programming language. /usr/lib : stores the required libraries and data files for programs stored within /usr or elsewhere.
stores the required libraries and data files for programs stored within /usr or elsewhere. /var : a short for “variable.” a place for files that may change often – especially in size, for example e-mail sent to users on the system, or process-id lock files.
a short for “variable.” a place for files that may change often – especially in size, for example e-mail sent to users on the system, or process-id lock files. /var/log : contains system log files.
contains system log files. /var/mail : the place where all the incoming mails are stored. users (other than root) can access their own mail only. often, this directory is a symbolic link to /var/spool/mail.
the place where all the incoming mails are stored. users (other than root) can access their own mail only. often, this directory is a symbolic link to /var/spool/mail. /var/spool : spool directory. contains print jobs, mail spools and other queued tasks.
spool directory. contains print jobs, mail spools and other queued tasks. /var/tmp : a place for temporary files which should be preserved between system reboots.
types of unix files – the unix files system contains several different types of files :
. ordinary files – an ordinary file is a file on the system that contains data, text, or program instructions.
used to store your information, such as some text you have written or an image you have drawn. this is the type of file that you usually work with.
always located within/under a directory file.
do not contain other files.
in long-format output of ls -l, this type of file is specified by the “-” symbol.
. directories – directories store both special and ordinary files. for users familiar with windows or mac os, unix directories are equivalent to folders. a directory file contains an entry for every file and subdirectory that it houses. if you have  files in a directory, there will be  entries in the directory. each entry has two components.
() the filename
() a unique identification number for the file or directory (called the inode number)
branching points in the hierarchical tree.
used to organize groups of files.
may contain ordinary files, special files or other directories.
never contain “real” information which you would work with (such as text). basically, just used for organizing files.
all files are descendants of the root directory, ( named / ) located at the top of the tree.
in long-format output of ls –l , this type of file is specified by the “d” symbol.
. special files – used to represent a real physical device such as a printer, tape drive or terminal, used for input/output (i/o) operations. device or special files are used for device input/output(i/o) on unix and linux systems. they appear in a file system just like an ordinary file or a directory.
on unix systems there are two flavors of special files for each device, character special files and block special files :
when a character special file is used for device input/output(i/o), data is transferred one character at a time. this type of access is called raw device access.
when a block special file is used for device input/output(i/o), data is transferred in large fixed-size blocks. this type of access is called block device access.
for terminal devices, it’s one character at a time. for disk devices though, raw access means reading or writing in whole chunks of data – blocks, which are native to your disk.
in long-format output of ls -l, character special files are marked by the “c” symbol.
in long-format output of ls -l, block special files are marked by the “b” symbol.
. pipes – unix allows you to link commands together using a pipe. the pipe acts a temporary file which only exists to hold data from one command until it is read by another.a unix pipe provides a one-way flow of data.the output or result of the first command sequence is used as the input to the second command sequence. to make a pipe, put a vertical bar (|) on the command line between two commands.for example: who | wc -l
in long-format output of ls –l , named pipes are marked by the “p” symbol.
. sockets – a unix socket (or inter-process communication socket) is a special file which allows for advanced inter-process communication. a unix socket is used in a client-server application framework. in essence, it is a stream of data, very similar to network stream (and network sockets), but all the transactions are local to the filesystem.
in long-format output of ls -l, unix sockets are marked by “s” symbol.
. symbolic link – symbolic link is used for referencing some other file of the file system.symbolic link is also known as soft link. it contains a text form of the path to the file it 
in any operating system, it is necessary to have dual mode operation to ensure protection and security of the system from unauthorized or errant users . this dual mode separates the user mode from the system mode or kernel mode.
what are privileged instructions?
the instructions that can run only in kernel mode are called privileged instructions .
privileged instructions possess the following characteristics :
(i) if any attempt is made to execute a privileged instruction in user mode, then it will not be executed and treated as an illegal instruction. the hardware traps it to the operating system.
(ii) before transferring the control to any user program, it is the responsibility of the operating system to ensure that the timer is set to interrupt. thus, if the timer interrupts then the operating system regains the control.
thus, any instruction which can modify the contents of the timer is a privileged instruction.
(iii) privileged instructions are used by the operating system in order to achieve correct operation.
(iv) various examples of privileged instructions include:
i/o instructions and halt instructions
turn off all interrupts
set the timer
context switching
clear the memory or remove a process from the memory
modify entries in device-status table
what are non-privileged instructions?
the instructions that can run only in user mode are called non-privileged instructions .
various examples of non-privileged instructions include:
reading the status of processor
reading the system time
generate any trap instruction
sending the final prinout of printer
also, it is important to note that in order to change the mode from privileged to non-privileged, we require a non-privileged instruction that does not generate any interrupt.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : vaibhavrai
prerequisite – file systems
hierarchical directory systems –
directory is maintained in the form of a tree. each user can have as many directories as are needed so, that files can be grouped together in a natural way.
advantages of this structure:
searching is efficient
groping capability of files increase
when the file system is organized as a directory tree, some way is needed for specifying file names.
two different methods are commonly used:
absolute path name – in this method, each file is given an absolute path name consisting of the path from the root directory to the file. as an example, the path /usr/ast/mailbox means that the root directory contains a subdirectory usr, which in turn contains a subdirectory ast, which contains the file mailbox. absolute path names always start at the root directory and are unique. in unix the components of the path are separated by ‘/’. in windows, the separator is ‘\’.
windows \usr\ast\mailbox
unix /usr/ast/mailbox relative path name – this is used in conjunction with the concept of the working directory (also called the current directory).a user can designate one directory as the current working directory, in which case all path names not beginning at the root directory are taken relative to the working directory. for example, if the current working directory is /usr/ast, then the file whose absolute path is /usr/ast/mailbox can be referenced simply as mailbox.
in other words, the unix
command : cp /usr/ast/mailbox /usr/ast/mailbox.bak
and the command : cp mailbox mailbox.bak
do exactly the same thing if the working directory is /usr/ast.
when to use which approach?
some programs need to access a specific file without regard to what the working directory is. in that case, they should always use absolute path names. for example, a spelling checker might need to read /usr/lib/dictionary to do its work. it should use the full, absolute path name in this case because it does not know what the working directory will be when it is called. the absolute path name will always work, no matter what the working directory is.
of course, if the spelling checker needs a large number of files from /usr/lib, an alternative approach is for it to issue a system call to change its working directory to /usr/lib, and then use just dictionary as the first parameter to open. by explicitly changing the working directory, it knows for sure where it is in the directory tree, so it can then use relative paths.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : magbene
a directory is a container that is used to contain folders and file. it organizes files and folders into a hierarchical manner.
there are several logical structures of a directory, these are given below.
single-level directory –
single level directory is simplest directory structure.in it all files are contained in same directory which make it easy to support and understand. a single level directory has a significant limitation, however, when the number of files increases or when the system has more than one user. since all the files are in the same directory, they must have the unique name . if two users call their dataset test, then the unique name rule violated.
advantages: since it is a single directory, so its implementation is very easy.
if the files are smaller in size, searching will become faster.
the operations like file creation, searching, deletion, updating are very easy in such a directory structure. disadvantages: there may chance of name collision because two files can not have the same name.
searching will become time taking if the directory is large.
in this can not group the same type of files together. two-level directory –
as we have seen, a single level directory often leads to confusion of files names among different users. the solution to this problem is to create a separate directory for each user. in the two-level directory structure, each user has there own user files directory (ufd). the ufds has similar structures, but each lists only the files of a single user. system’s master file directory (mfd) is searches whenever a new user id=s logged in. the mfd is indexed by username or account number, and each entry points to the ufd for that user. advantages: we can give full path like /user-name/directory-name/.
different users can have same directory as well as file name.
searching of files become more easy due to path name and user-grouping. disadvantages:
a user is not allowed to share files with other users.
still it not very scalable, two files of the same type cannot be grouped together in the same user. tree-structured directory –
once we have seen a two-level directory as a tree of height , the natural generalization is to extend the directory structure to a tree of arbitrary height.
this generalization allows the user to create there own subdirectories and to organize on their files accordingly. a tree structure is the most common directory structure. the tree has a root directory, and every file in the system have a unique path. advantages: very generalize, since full path name can be given.
very scalable, the probability of name collision is less.
searching becomes very easy, we can use both absolute path as well as relative. disadvantages: every file does not fit into the hierarchical model, files may be saved into multiple directories.
we can not share files.
it is inefficient, because accessing a file may go under multiple directories. acyclic graph directory –
an acyclic graph is a graph with no cycle and allows to share subdirectories and files. the same file or subdirectories may be in two different directories. it is a natural generalization of the tree-structured directory. it is used in the situation like when two programmers are working on a joint project and they need to access files. the associated files are stored in a subdirectory, separating them from other projects and files of other programmers, since they are working on a joint project so they want the subdirectories to be into their own directories. the common subdirectories should be shared. so here we use acyclic directories. it is the point to note that shared file is not the same as copy file . if any programmer makes some changes in the subdirectory it will reflect in both subdirectories. advantages: we can share files.
searching is easy due to different-different paths. disadvantages: we share the files via linking, in case of deleting it may create the problem,
if the link is softlink then after deleting the file we left with a dangling pointer.
in case of hardlink, to delete a file we have to delete all the reference associated with it. general graph directory structure –
in general graph directory structure, cycles are allowed within a directory structure where multiple directories can be derived from more than one parent directory.
the main problem with this kind of directory structure is to calculate total size or space that has been taken by the files and directories. advantages: it allows cycles.
it is more flexible than other directories structure. disadvantages: it is more costly than others.
it needs garbage collection.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : shubham_singh, nehasharma, miranjunaidi
the allocation methods define how the files are stored in the disk blocks. there are three main disk space or file allocation methods.
contiguous allocation
linked allocation
indexed allocation
the main idea behind these methods is to provide:
efficient disk space utilization.
fast access to the file blocks.
all the three methods have their own advantages and disadvantages as discussed below:
. contiguous allocation
in this scheme, each file occupies a contiguous set of blocks on the disk. for example, if a file requires n blocks and is given a block b as the starting location, then the blocks assigned to the file will be: b, b+, b+,……b+n-. this means that given the starting block address and the length of the file (in terms of blocks required), we can determine the blocks occupied by the file.
the directory entry for a file with contiguous allocation contains
address of starting block
length of the allocated portion.
the file ‘mail’ in the following figure starts from the block  with length =  blocks. therefore, it occupies , , , , ,  blocks.
advantages:
both the sequential and direct accesses are supported by this. for direct access, the address of the kth block of the file which starts at block b can easily be obtained as (b+k).
this is extremely fast since the number of seeks are minimal because of contiguous allocation of file blocks.
disadvantages:
this method suffers from both internal and external fragmentation. this makes it inefficient in terms of memory utilization.
increasing file size is difficult because it depends on the availability of contiguous memory at a particular instance.
. linked list allocation
in this scheme, each file is a linked list of disk blocks which need not be contiguous. the disk blocks can be scattered anywhere on the disk.
the directory entry contains a pointer to the starting and the ending file block. each block contains a pointer to the next block occupied by the file.
the file ‘jeep’ in following image shows how the blocks are randomly distributed. the last block () contains - indicating a null pointer and does not point to any other block.
advantages:
this is very flexible in terms of file size. file size can be increased easily since the system does not have to look for a contiguous chunk of memory.
this method does not suffer from external fragmentation. this makes it relatively better in terms of memory utilization.
disadvantages:
because the file blocks are distributed randomly on the disk, a large number of seeks are needed to access every block individually. this makes linked allocation slower.
it does not support random or direct access. we can not directly access the blocks of a file. a block k of a file can be accessed by traversing k blocks sequentially (sequential access ) from the starting block of the file via block pointers.
pointers required in the linked allocation incur some extra overhead.
. indexed allocation
in this scheme, a special block known as the index block contains the pointers to all the blocks occupied by a file. each file has its own index block. the ith entry in the index block contains the disk address of the ith file block. the directory entry contains the address of the index block as shown in the image:
advantages:
this supports direct access to the blocks occupied by the file and therefore provides fast access to the file blocks.
it overcomes the problem of external fragmentation.
disadvantages:
the pointer overhead for indexed allocation is greater than linked allocation.
for very small files, say files that expand only - blocks, the indexed allocation would keep one entire block (index block) for the pointers which is inefficient in terms of memory utilization. however, in linked allocation we lose the space of only  pointer per block.
for files that are very large, single index block may not be able to hold all the pointers.
following mechanisms can be used to resolve this:
linked scheme: this scheme links two or more index blocks together for holding the pointers. every index block would then contain a pointer or the address to the next index block. multilevel index: in this policy, a first level index block is used to point to the second level index blocks which inturn points to the disk blocks occupied by the file. this can be extended to  or more levels depending on the maximum file size. combined scheme: in this scheme, a special block called the inode (information node) contains all the information about the file such as the name, size, authority, etc and the remaining space of inode is used to store the disk block addresses which contain the actual file as shown in the image below. the first few of these pointers in inode point to the direct blocks i.e the pointers contain the addresses of the disk blocks that contain data of the file. the next few pointers point to indirect blocks. indirect blocks may be single indirect, double indirect or triple indirect. single indirect block is the disk block that does not contain the file data but the disk address of the blocks that contain the file data. similarly, double indirect blocks do not contain the file data but the disk address of the blocks that contain the address of the blocks containing the file data.
this article is contributed by saloni baweja. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
prerequisite – file systems
when a file is used, information is read and accessed into computer memory and there are several ways to access this information of the file. some systems provide only one access method for files. other systems, such as those of ibm, support many access methods, and choosing the right one for a particular application is a major design problem.
there are three ways to access a file into a computer system: sequential-access, direct access, index sequential method.
sequential access –
it is the simplest access method. information in the file is processed in order, one record after the other. this mode of access is by far the most common; for example, editor and compiler usually access the file in this fashion. read and write make up the bulk of the operation on a file. a read operation -read next- read the next position of the file and automatically advance a file pointer, which keeps track i/o location. similarly, for the writewrite next append to the end of the file and advance to the newly written material. key points:
data is accessed one record right after another record in an order.
when we use read command, it move ahead pointer by one
when we use write command, it will allocate memory and move the pointer to the end of the file
such a method is reasonable for tape.
direct access –
another method is direct access method also known as relative access method. a filed-length logical record that allows the program to read and write record rapidly. in no particular order. the direct access is based on the disk model of a file since disk allows random access to any file block. for direct access, the file is viewed as a numbered sequence of block or record. thus, we may read block  then block  and then we can write block . there is no restriction on the order of reading and writing for a direct access file. a block number provided by the user to the operating system is normally a relative block number, the first relative block of the file is  and then  and so on. index sequential method –
it is the other method of accessing a file which is built on the top of the sequential access method. these methods construct an index for the file. the index, like an index in the back of a book, contains the pointer to the various blocks. to find a record in the file, we first search the index and then by the help of pointer we access the file directly. key points: it is built on top of sequential access.
it control the pointer by using index.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : gokohan, devanshgandhi, yac
primary memory has limited storage capacity and is volatile. secondary memory overcome this limitation by providing permanent storage of data and in bulk quantity. secondary memory is also termed as external memory and refers to the various storage media on which a computer can store data and programs. the secondary storage media can be fixed or removable. fixed storage media is an internal storage medium like hard disk that is fixed inside the computer. storage medium that are portable and can be taken outside the computer are termed as removable storage media.
difference between primary memory and secondary memory:
primary memory secondary memory primary memory is directly accessed by the central processing unit(cpu). secondary memory is not accessed directly by the central processing unit(cpu). instead, data accessed from a secondary memory is first loaded into random access memory(ram) and is then sent to the processing unit. ram provides much faster accessing speed to data than secondary memory. by loading software programs and required files into primary memory(ram), computer can process data much more quickly. secondary memory is slower in data accessing. typically primary memory is six times faster than the secondary memory. primary memory, i.e. random access memory(ram) is volatile and gets completely erased when a computer is shut down. secondary memory provides a feature of being non-volatile, which means it can hold on to its data with or without electrical power supply.
uses of secondary media:
permanent storage: primary memory (ram) is volatile, i.e. it loses all information when the electricity is turned off, so in order to secure the data permanently in the device, secondary storage devices are needed.
primary memory (ram) is volatile, i.e. it loses all information when the electricity is turned off, so in order to secure the data permanently in the device, secondary storage devices are needed. portability: storage medium, like the cds, flash drives can be used to transfer the data from one devive to another.
fixed and removable storage
fixed storage-
a fixed storage is an internal media device that is used by a computer system to store data, and usually these are referred to as the fixed disks drives or the hard drives.
fixed storage devices are literally not fixed, obviously these can be removed from the system for repairing work, maintenance purpose, and also for upgrade etc. but in general, this can’t be done without a proper toolkit to open up the computer system to provide physical access, and that needs to be done by an engineer.
technically, almost all of the data i.e. being processed on a computer system is stored on some type of a built-in fixed storage device.
types of fixed storage:
internal flash memory (rare)
ssd (solid-state disk) units
hard disk drives (hdd)
removable storage-
a removable storage is an external media device that is used by a computer system to store data, and usually these are referred to as the removable disks drives or the external drives.
removable storage is any type of storage device that can be removed/ejected from a computer system while the system is running. examples of external devices include cds, dvds and blu-ray disk drives, as well as diskettes and usb drives. removable storage makes it easier for a user to transfer data from one computer system to another.
in a storage factors, the main benefit of removable disks is that they can provide the fast data transfer rates associated with storage area networks (sans)
types of removable storage:
optical discs (cds, dvds, blu-ray discs)
memory cards
floppy disks
magnetic tapes
disk packs
paper storage (punched tapes , punched cards)
secondary storage media
there are the following main types of storage media:
. magnetic storage media:
magnetic media is coated with a magnetic layer which is magnetized in clockwise or anticlockwise directions. when the disk moves, the head interprets the data stored at a specific location in binary s and s at reading.
examples: hard disks, floppy disks and magnetic tapes.
floppy disk: a floppy disk is a flexible disk with a magnetic coating on it. it is packaged inside a protective plastic envelope. these are one of the oldest type of portable storage devices that could store up to . mb of data but now they are not used due to very less memory storage.
a floppy disk is a flexible disk with a magnetic coating on it. it is packaged inside a protective plastic envelope. these are one of the oldest type of portable storage devices that could store up to . mb of data but now they are not used due to very less memory storage. hard disk: a hard disk consists of one or more circular disks called platters which are mounted on a common spindle. each surface of a platter is coated with a magnetic material. both surfaces of each disk are capable of storing data except the top and bottom disk where only the inner surface is used. the information is recorded on the surface of the rotating disk by magnetic read/write heads. these heads are joined to a common arm known as access arm. hard disk drive components:
most of the basic types of hard drives contains a number of disk platters that are placed around a spindle which is placed inside a sealed chamber. the chamber also includes read/write head and motors. data is stored on each of these disks in the arrangement of concentric circles called tracks which are divided further into sectors. though internal hard drives are not very portable and used internally in a computer system, external hard disks can be used as a substitute for portable storage. hard disks can store data upto several terabytes.
. optical storage media
in optical storage media information is stored and read using a laser beam. the data is stored as a spiral pattern of pits and ridges denoting binary  and binary .
examples: cds and dvds
it stands for digital versatile disk or digital video disk. it looks just like a cd and use a similar technology as that of the cds but allows tracks to be spaced closely enough to store data that is more than six times the cd’s capacity. it is a significant advancement in portable storage technology. a dvd holds . gb to  gb of data.
it stands for digital versatile disk or digital video disk. it looks just like a cd and use a similar technology as that of the cds but allows tracks to be spaced closely enough to store data that is more than six times the cd’s capacity. it is a significant advancement in portable storage technology. a dvd holds . gb to  gb of data. blue ray disk:
this is the latest optical storage media to store high definition audio and video. it is similar to a cd or dvd but can store up to  gb of data on a single layer disk and up to  gb of data on a dual layer disk. while cds or dvds use red laser beam, the blue ray disk uses a blue laser to read/write data on a disk.
. solid state memories
solid-state storage devices are based on electronic circuits with no moving parts like the reels of tape, spinning discs etc. solid-state storage devices use special memories called flash memory to store data. solid state drive (or flash memory) is used mainly in digital cameras, pen drives or usb flash drives.
pen drives:
pen drives or thumb drives or flash drives are the recently emerged portable storage media. it is an eeprom based flash memory which can be repeatedly erased and written using electric signals. this memory is accompanied with a usb connector which enables the pendrive to connect to the computer. they have a capacity smaller than a hard disk but greater than a cd. pendrive has following advantages:
transfer files:
a pen drive being plugged into a usb port of the system can be used as a device to transfer files, documents and photos to a pc and also vice versa. similarly, selected files can be transferred between a pen drive and any type of workstation.
a pen drive being plugged into a usb port of the system can be used as a device to transfer files, documents and photos to a pc and also vice versa. similarly, selected files can be transferred between a pen drive and any type of workstation. portability:
the lightweight nature and smaller size of a pen drive make it possible to carry it from place to place which makes data transportation an easier task.
the lightweight nature and smaller size of a pen drive make it possible to carry it from place to place which makes data transportation an easier task. backup storage:
most of the pen drives now come with a feature of having password encryption, important information related to family, medical records and photos can be stored on them as a backup.
most of the pen drives now come with a feature of having password encryption, important information related to family, medical records and photos can be stored on them as a backup. transport data:
professionals/students can now easily transport large data files and video/audio lectures on a pen drive and gain access to them from anywhere. independent pc technicians can store work-related utility tools, various programs and files on a high-speed  gb pen drive and move from one site to another.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
a hard disk is a memory storage device which looks like this:
the disk is divided into tracks. each track is further divided into sectors. the point to be noted here is that outer tracks are bigger in size than the inner tracks but they contain the same number of sectors and have equal storage capacity. this is because the storage density is high in sectors of the inner tracks where as the bits are sparsely arranged in sectors of the outer tracks. some space of every sector is used for formatting. so, the actual capacity of a sector is less than the given capacity.
read-write(r-w) head moves over the rotating hard disk. it is this read-write head that performs all the read and write operations on the disk and hence, position of the r-w head is a major concern. to perform a read or write operation on a memory location, we need to place the r-w head over that position. some important terms must be noted here:
seek time – the time taken by the r-w head to reach the desired track from it’s current position. rotational latency – time taken by the sector to come under the r-w head. data transfer time – time taken to transfer the required amount of data. it depends upon the rotational speed. controller time – the processing time taken by the controller. average access time – seek time + average rotational latency + data transfer time + controller time.
note:average rotational latency is mostly /*(rotetional latency).
in questions, if the seek time and controller time is not mentioned, take them to be zero.
if the amount of data to be transferred is not given, assume that no data is being transferred. otherwise, calculate the time taken to transfer the given amount of data.
the average of rotational latency is taken when the current position of r-w head is not given. because, the r-w may be already present at the desired position or it might take a whole rotation to get the desired sector under the r-w head. but, if the current position of the r-w head is given then the rotational latency must be calculated.
example –
consider a hard disk with:
 surfaces
 tracks/surface
 sectors/track
 bytes/sector
what is the capacity of the hard disk?
disk capacity = surfaces * tracks/surface * sectors/track * bytes/sector
disk capacity =  *  *  * 
disk capacity =  mb the disk is rotating at  rpm, what is the data transfer rate?
 sec ->  rotations
 sec ->  rotations
data transfer rate = number of rotations per second * track capacity * number of surfaces (since  r-w head is used for each surface)
data transfer rate =  *  *  * 
data transfer rate = . mb/sec the disk is rotating at  rpm, what is the average access time?
since, seek time, controller time and the amount of data to be transferred is not given, we consider all the three terms as .
therefore, average access time = average rotational delay
rotational latency =>  sec ->  rotations
 sec ->  rotations
rotational latency = (/) sec = . msec.
average rotational latency = (.)/
= . msec.
average access time = . msec. another example: gate it  | question 
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : vaibhavrai
disk scheduling is done by operating systems to schedule i/o requests arriving for the disk. disk scheduling is also known as i/o scheduling.
disk scheduling is important because:
multiple i/o requests may arrive by different processes and only one i/o request can be served at a time by the disk controller. thus other i/o requests need to wait in the waiting queue and need to be scheduled.
two or more request may be far from each other so can result in greater disk arm movement.
hard drives are one of the slowest parts of the computer system and thus need to be accessed in an efficient manner.
there are many disk scheduling algorithms but before discussing them let’s have a quick look at some of the important terms:
seek time : seek time is the time taken to locate the disk arm to a specified track where the data is to be read or write. so the disk scheduling algorithm that gives minimum average seek time is better.
seek time is the time taken to locate the disk arm to a specified track where the data is to be read or write. so the disk scheduling algorithm that gives minimum average seek time is better. rotational latency: rotational latency is the time taken by the desired sector of disk to rotate into a position so that it can access the read/write heads. so the disk scheduling algorithm that gives minimum rotational latency is better.
rotational latency is the time taken by the desired sector of disk to rotate into a position so that it can access the read/write heads. so the disk scheduling algorithm that gives minimum rotational latency is better. transfer time: transfer time is the time to transfer the data. it depends on the rotating speed of the disk and number of bytes to be transferred.
transfer time is the time to transfer the data. it depends on the rotating speed of the disk and number of bytes to be transferred. disk access time: disk access time is:
disk access time = seek time + rotational latency + transfer time
disk response time: response time is the average of time spent by a request waiting to perform its i/o operation. average response time is the response time of the all requests. variance response time is measure of how individual request are serviced with respect to average response time. so the disk scheduling algorithm that gives minimum variance response time is better.
disk scheduling algorithms
fcfs: fcfs is the simplest of all the disk scheduling algorithms. in fcfs, the requests are addressed in the order they arrive in the disk queue.let us understand this with the help of an example.
example: suppose the order of request is- (,,,,,,)
and current position of read/write head is : 
so, total seek time:
=(-)+(-)+(-)+(-)+(-)+(-)+(-)
=
advantages:
every request gets a fair chance
no indefinite postponement
disadvantages:
does not try to optimize seek time
may not provide the best possible service
sstf: in sstf (shortest seek time first), requests having shortest seek time are executed first. so, the seek time of every request is calculated in advance in the queue and then they are scheduled according to their calculated seek time. as a result, the request near the disk arm will get executed first. sstf is certainly an improvement over fcfs as it decreases the average response time and increases the throughput of system.let us understand this with the help of an example.
example: suppose the order of request is- (,,,,,,)
and current position of read/write head is : 
so, total seek time: =(-)+(-)+(-)+(-)+(-)+(-)+(-)
=
advantages:
average response time decreases
throughput increases
disadvantages:
overhead to calculate seek time in advance
can cause starvation for a request if it has higher seek time as compared to incoming requests
high variance of response time as sstf favours only some requests
scan: in scan algorithm the disk arm moves into a particular direction and services the requests coming in its path and after reaching the end of disk, it reverses its direction and again services the request arriving in its path. so, this algorithm works as an elevator and hence also known as elevator algorithm. as a result, the requests at the midrange are serviced more and those arriving behind the disk arm will have to wait.
example: suppose the requests to be addressed are-,,,,,,. and the read/write arm is at , and it is also given that the disk arm should move “towards the larger value”.
therefore, the seek time is calculated as:
=(-)+(-)
=
advantages:
high throughput
low variance of response time
average response time
disadvantages:
long waiting time for requests for locations just visited by disk arm
cscan : in scan algorithm, the disk arm again scans the path that has been scanned, after reversing its direction. so, it may be possible that too many requests are waiting at the other end or there may be zero or few requests pending at the scanned area.
these situations are avoided in cscan algorithm in which the disk arm instead of reversing its direction goes to the other end of the disk and starts servicing the requests from there. so, the disk arm moves in a circular fashion and this algorithm is also similar to scan algorithm and hence it is known as c-scan (circular scan).
example:
suppose the requests to be addressed are-,,,,,,. and the read/write arm is at , and it is also given that the disk arm should move “towards the larger value”.
seek time is calculated as:
=(-)+(-)+(-)
=
advantages:
provides more uniform wait time compared to scan
look: it is similar to the scan disk scheduling algorithm except for the difference that the disk arm in spite of going to the end of the disk goes only to the last request to be serviced in front of the head and then reverses its direction from there only. thus it prevents the extra delay which occurred due to unnecessary traversal to the end of the disk.
example: suppose the requests to be addressed are-,,,,,,. and the read/write arm is at , and it is also given that the disk arm should move “towards the larger value”.
so, the seek time is calculated as: =(-)+(-)
=
clook: as look is similar to scan algorithm, in similar way, clook is similar to cscan disk scheduling algorithm. in clook, the disk arm in spite of going to the end goes only to the last request to be serviced in front of the head and then from there goes to the other end’s last request. thus, it also prevents the extra delay which occurred due to unnecessary traversal to the end of the disk.
example: suppose the requests to be addressed are-,,,,,,. and the read/write arm is at , and it is also given that the disk arm should move “towards the larger value”
so, the seek time is calculated as: =(-)+(-)+(-)
= rss– it stands for random scheduling and just like its name it is nature. it is used in situations where scheduling involves random attributes such as random processing time, random due dates, random weights, and stochastic machine breakdowns this algorithm sits perfect. which is why it is usually used for and analysis and simulation. lifo– in lifo (last in, first out) algorithm, newest jobs are serviced before the existing ones i.e. in order of requests that get serviced the job that is newest or last entered is serviced first and then the rest in the same order. advantages maximizes locality and resource utilization disadvantages can seem a little unfair to other requests and if new requests keep coming in, it cause starvation to the old and existing ones. example
suppose the order of request is- (,,,,,,)
and current position of read/write head is :  n-step scan – it is also known as n-step look algorithm. in this a buffer is created for n requests. all requests belonging to a buffer will be serviced in one go. also once the buffer is full no new requests are kept in this buffer and are sent to another one. now, when these n requests are serviced, the time comes for another top n requests and this way all get requests get a guaranteed service advantages it eliminates starvation of requests completely fscan– this algorithm uses two sub-queues. during the scan all requests in the first queue are serviced and the new incoming requests are added to the second queue. all new requests are kept on halt until the existing requests in the first queue are serviced.
advantages fscan along with n-step-scan prevents “arm stickiness” (phenomena in i/o scheduling where the scheduling algorithm continues to service requests at or near the current sector and thus prevents any seeking)
each algorithm is unique in its own way. overall performance depends on the number and type of requests.
note:average rotational latency is generally taken as /(rotational latency).
exercise
) suppose a disk has  cylinders, numbered from  to . at some time the disk arm is at cylinder , and there is a queue of disk access requests for cylinders , , , , , ,  and . if shortest-seek time first (sstf) is being used for scheduling the disk access, the request for cylinder  is serviced after servicing ____________ number of requests. (gate cs 
(a) 
(b) 
(c) 
(d) 
see this for solution.
) consider an operating system capable of loading and executing a single sequential user process at a time. the disk head scheduling algorithm used is first come first served (fcfs). if fcfs is replaced by shortest seek time first (sstf), claimed by the vendor to give % better benchmark results, what is the expected improvement in the i/o performance of user programs? (gate cs )
(a) %
(b) %
(c) %
(d) %
see this for solution.
) suppose the following disk request sequence (track numbers) for a disk with  tracks is given: , , , , , , , , . assume that the initial position of the r/w head is on track . the additional distance that will be traversed by the r/w head when the shortest seek time first (sstf) algorithm is used compared to the scan (elevator) algorithm (assuming that scan algorithm moves towards  when it starts execution) is _________ tracks
(a) 
(b) 
(c) 
(d) 
see this for solution.
) consider a typical disk that rotates at  rotations per minute (rpm) and has a transfer rate of  × ^ bytes/sec. if the average seek time of the disk is twice the average rotational delay and the controller’s transfer time is  times the disk transfer time, the average time (in milliseconds) to read or write a  byte sector of the disk is _____________
see this for solution.
this article is contributed by ankit mittal. please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : vaibhavrai, majorssn, singhkomal, vanshikagoyal
prerequisite – disk scheduling algorithms
given an array of disk track numbers and initial head position, our task is to find the total number of seek operations done to access all the requested tracks if shortest seek time first (sstf) is a disk scheduling algorithm is used.
shortest seek time first (sstf) –
basic idea is the tracks which are closer to current disk head position should be serviced first in order to minimise the seek operations.
advantages of shortest seek time first (sstf) –
better performance than fcfs scheduling algorithm. it provides better throughput. this algorithm is used in batch processing system where throughput is more important. it has less average response and waiting time.
disadvantages of shortest seek time first (sstf) –
starvation is possible for some requests as it favours easy to reach request and ignores the far away processes. their is lack of predictability because of high variance of response time. switching direction slows things down.
let request array represents an array storing indexes of tracks that have been requested. ‘head’ is the position of disk head. find the positive distance of all tracks in the request array from head. find a track from requested array which has not been accessed/serviced yet and has minimum distance from head. increment the total seek count with this distance. currently serviced track position now becomes the new head position. go to step  until all tracks in request array have not been serviced.
initial head position = 
the following chart shows the sequence in which requested tracks are serviced using sstf.
therefore, total seek count is calculates as:
implementation –
implementation of sstf is given below. note that we have made a node class having  members. ‘distance’ is used to store the distance between head and the track position. ‘accessed’ is a boolean variable which tells whether the track has been accessed/serviced before by disk head or not.
output:
total number of seek operations =  seek sequence is         
attention reader! don’t stop learning now. get hold of all the important dsa concepts with the dsa self paced course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : sonuyadavaffriya, shubhamsingh, rajput-ji, itskawal
spool is an acronym for simultaneous peripheral operations on-line. it is a kind of buffering mechanism or a process in which data is temporarily held to be used and executed by a device, program or the system. data is sent to and stored in memory or other volatile storage until the program or computer requests it for execution.
in a computer system peripheral equipments, such as printers and punch card readers etc (batch processing), are very slow relative to the performance of the rest of the system. getting input and output from the system was quickly seen to be a bottleneck. here comes the need for spool.
spooling works like a typical request queue where data, instructions and processes from multiple sources are accumulated for execution later on. generally, it is maintained on computer’s physical memory, buffers or the i/o device-specific interrupts. the spool is processed in fifo manner i.e. whatever first instruction is there in the queue will be popped and executed.
applications/implementations of spool:
) the most common can be found in i/o devices like keyboard printers and mouse. for example, in printer, the documents/files that are sent to the printer are first stored in the memory or the printer spooler. once the printer is ready, it fetches the data from the spool and prints it.
even experienced a situation when suddenly for some seconds your mouse or keyboard stops working? meanwhile, we usually click again and again here and there on the screen to check if its working or not. when it actually starts working, what and wherever we pressed during its hang state gets executed very fast because all the instructions got stored in the respective device’s spool.
) a batch processing system uses spooling to maintain a queue of ready-to-run jobs which can be started as soon as the system has the resources to process them.
) spooling is capable of overlapping i/o operation for one job with processor operations for another job. i.e. multiple processes can write documents to a print queue without waiting and resume with their work.
) e-mail: an email is delivered by a mta (mail transfer agent) to a temporary storage area where it waits to be picked up by the ma (mail user agent)
) can also be used for generating banner pages (these are the pages used in computerized printing in order to separate documents from each other and to identify e.g. the originator of the print request by username, an account number or a bin for pickup. such pages are used in office environments where many people share the small number of available resources).
about the author:
ekta is a very active contributor on geeksforgeeks. currently studying at delhi technological university.she has also made a chrome extention for www.geeksquiz.com to practice mcqs randomly.she can be reached at github.com/ekta
if you also wish to showcase your blog here, please see gblog for guest blog writing on geeksforgeeks.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
there are two ways by which input/output subsystems can improve the performance and efficiency of the computer by using a memory space in the main memory or on the disk and these two are spooling and buffering.
spooling –
spooling stands for simultaneous peripheral operation online. a spool is similar to buffer as it holds the jobs for a device until the device is ready to accept the job. it considers disk as a huge buffer that can store as many jobs for the device till the output devices are ready to accept them.
buffering –
the main memory has an area called buffer that is used to store or hold the data temporarily that is being transmitted either between two devices or between a device or an application. buffering is an act of storing data temporarily in the buffer. it helps in matching the speed of the data stream between the sender and the receiver. if the speed of the sender’s transmission is slower than the receiver, then a buffer is created in the main memory of the receiver, and it accumulates the bytes received from the sender and vice versa.
the basic difference between spooling and buffering is that spooling overlaps the input/output of one job with the execution of another job while the buffering overlaps the input/output of one job with the execution of the same job.
differences between spooling and buffering –
the key difference between spooling and buffering is that spooling can handle the input/output of one job along with the computation of another job at the same time while buffering handles input/output of one job along with its computation.
spooling stands for simultaneous peripheral operation online. whereas buffering is not an acronym.
spooling is more efficient than buffering, as spooling can overlap processing two jobs at a time.
buffering uses limited area in main memory while spooling uses the disk as a huge buffer.
comparison chart –
spooling buffering basic difference it overlap the input/output of one job with the execution of another job. it overlaps the input/output of one job with the execution of the same job. full form (stands for) simultaneous peripheral operation online no full form efficiency spooling is more efficient than buffering. buffering is less efficient than spooling. consider size it considers disk as a huge spool or buffer. buffer is a limited area in main memory.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : amansachan
advantages –
simple to understand.
finding the first free block is efficient. it requires scanning the words (a group of  bits) in a bitmap for a non-zero word. (a -valued word has all bits ). the first free block is then found by scanning for the first  bit in the non-zero word.
the block number can be calculated as:
(number of bits per word) *(number of -values words) + offset of bit first bit  in the non-zero word .
for the figure-, we scan the bitmap sequentially for the first non-zero word.
the first group of  bits () constitute a non-zero word since all bits are not . after the non- word is found, we look for the first  bit. this is the th bit of the non-zero word. so, offset = .
therefore, the first free block number = *+ = .
unix is an operating system which is truly the base of all operating systems like ubuntu, solaris, posix, etc. it was developed in the s by ken thompson, dennis ritchie, and others in the at&t laboratories. it was originally meant for programmers developing software rather than non-programmers.
unix and the c were found by at&t and distributed to government and academic institutions, which led to both being ported to a wider variety of machine families than any other operating system. the main focus that was brought by the developers in this operating system was the kernel. unix was considered to be the heart of the operating system. system structure of unix os are as follows:
figure – system structure
layer-: hardware –
it consists of all hardware related information.
it consists of all hardware related information. layer-: kernel –
it interacts with hardware and most of the tasks like memory management, task scheduling, and management are done by the kernel.
it interacts with hardware and most of the tasks like memory management, task scheduling, and management are done by the kernel. layer-: shell commands –
shell is the utility that processes your requests. when you type in a command at the terminal, the shell interprets the command and calls the program that you want. there are various commands like cp, mv, cat, grep, id, wc, nroff, a.out and more.
shell is the utility that processes your requests. when you type in a command at the terminal, the shell interprets the command and calls the program that you want. layer-: application layer –
it is the outermost layer that executes the given external applications.
figure – kernel and its block diagram
this diagram shows three levels: user, kernel, and hardware.
the system call and library interface represent the border between user programs and the kernel. system calls look like ordinary function calls in c programs. assembly language programs may invoke system calls directly without a system call library. the libraries are linked with the programs at compile time.
the set of system calls into those that interact with the file subsystem and some system calls interact with the process control subsystem. the file subsystem manages files, allocating file space, administering free space, controlling access to files, and retrieving data for users.
processes interact with the file subsystem via a specific set of system calls, such as open (to open a file for reading or writing), close, read, write, stat (query the attributes of a file), chown (change the record of who owns the file), and chmod (change the access permissions of a file).
the file subsystem accesses file data using a buffering mechanism that regulates data flow between the kernel and secondary storage devices. the buffering mechanism interacts with block i/o device drivers to initiate data transfer to and from the kernel.
device drivers are the kernel modules that control the operator of peripheral devices. the file subsystem also interacts directly with “raw” i/o device drivers without the intervention of the buffering mechanism. finally, the hardware control is responsible for handling interrupts and for communicating with the machine. devices such as disks or terminals may interrupt the cpu while a process is executing. if so, the kernel may resume execution of the interrupted process after servicing the interrupt.
interrupts are not serviced by special processes but by special functions in the kernel, called in the context of the currently running process.
difference between unix and linux –
linux is essentially a clone of unix. but, basic differences are shown below:
my personal notes arrow_drop_up save
if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks. please improve this article if you find anything incorrect by clicking on the "improve article" button below.
improved by : erusnika
linux provides some important tricks. here are a few and important one’s:
leave — remind you when you have to leave
syntax: leave +hhmm leave waits until the specified time (within the next  hours), then reminds you that you have to leave by writing to the tty that you executed leave on. you are reminded  minutes and  minute before the actual time, at the time, and every minute thereafter.
options: hhmm the time of day is in the form hhmm where hh is a time in hours (on a  or  hour clock), and mm are minutes. diff – compare files line by line
syntax: diff file file compare files line by line.
diff -q file file report only when files differ cal, ncal — displays a calendar and the date of easter
syntax: cal the cal utility displays a simple calendar in traditional format and ncal offers an alternative layout, more options and the date of easter. the new format is a little cramped but it makes a year fit on a × terminal. if arguments are not specified, the current month is displayed.
locate – find files by name
syntax: locate file_name locate reads one or more databases prepared by updatedb() and writes file names matching at least one of the patterns to standard output, one per line. passwd – change user password
syntax: passwd the passwd command changes passwords for user accounts. a normal user may only change the password for his/her own account, while the superuser may change the password for any account. passwd also changes the account or associated password validity period. ln – make links between files
syntax: ln existing_file_name file_name create a link to target with the name specified
this article is contributed by mayank kumar. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
improved by : manav
process
a process is an instance of a program in execution. a set of processes combined together make a complete program.
there are two categories of processes in unix, namely
user processes : they are operated in user mode.
: they are operated in user mode. kernel processes: they are operated in kernel mode.
process states
the states that a process enters in working from start till end are known as process states. these are listed below as:
created -process is newly created by system call, is not ready to run
-process is newly created by system call, is not ready to run user running -process is running in user mode which means it is a user process.
-process is running in user mode which means it is a user process. kernel running -indicates process is a kernel process running in kernel mode.
-indicates process is a kernel process running in kernel mode. zombie- process does not exist/ is terminated.
process does not exist/ is terminated. preempted- when process runs from kernel to user mode, it is said to be preempted.
when process runs from kernel to user mode, it is said to be preempted. ready to run in memory- it indicated that process has reached a state where it is ready to run in memory and is waiting for kernel to schedule it.
it indicated that process has reached a state where it is ready to run in memory and is waiting for kernel to schedule it. ready to run, swapped – process is ready to run but no empty main memory is present
– process is ready to run but no empty main memory is present sleep, swapped- process has been swapped to secondary storage and is at a blocked state.
process has been swapped to secondary storage and is at a blocked state. asleep in memory- process is in memory(not swapped to secondary storage) but is in blocked state.
process transitions
the working of process is explained in following steps:
user-running: process is in user-running. kernel-running: process is allocated to kernel and hence, is in kernel mode. ready to run in memory: further, after processing in main memory process is rescheduled to the kernel.i.e.the process is not executing but is ready to run as soon as the kernel schedules it. asleep in memory: process is sleeping but resides in main memory. it is waiting for the task to begin. ready to run, swapped: process is ready to run and be swapped by the processor into main memory, thereby allowing kernel to schedule it for execution. sleep, swapped: process is in sleep state in secondary memory, making space for execution of other processes in main memory. it may resume once the task is fulfilled. pre-empted: kernel preempts an on-going process for allocation of another process, while the first process is moving from kernel to user mode. created: process is newly created but not running. this is the start state for all processes. zombie: process has been executed thoroughly and exit call has been enabled.
the process, thereby, no longer exists. but, it stores a statistical record for the process.
this is the final state of all processes.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
my personal notes arrow_drop_up save
what is shell
a shell is special user program which provide an interface to user to use operating system services. shell accept human readable commands from user and convert them into something which kernel can understand. it is a command language interpreter that execute commands read from input devices such as keyboards or from files. the shell gets started when the user logs in or start the terminal.
command line shell
graphical shell
command line shell
shell can be accessed by user using a command line interface. a special program called terminal in linux/macos or command prompt in windows os is provided to type in the human readable commands such as “cat”, “ls” etc. and then it is being execute. the result is then displayed on the terminal to the user. a terminal in ubuntu . system looks like this –
it will list all the files in current working directory in long listing format.
working with command line shell is bit difficult for the beginners because it’s hard to memorize so many commands. it is very powerful, it allows user to store commands in a file and execute them together. this way any repetitive task can be easily automated. these files are usually called batch files in windows and shell scripts in linux/macos systems.
graphical shells
graphical shells provide means for manipulating programs based on graphical user interface (gui), by allowing for operations such as opening, closing, moving and resizing windows, as well as switching focus between windows. window os or ubuntu os can be considered as good example which provide gui to user for interacting with program. user do not need to type in command for every actions.a typical gui in ubuntu system –
there are several shells are available for linux systems like –
bash (bourne again shell) – it is most widely used shell in linux systems. it is used as default login shell in linux systems and in macos. it can also be installed on windows os.
csh (c shell) – the c shell’s syntax and usage are very similar to the c programming language.
ksh (korn shell) – the korn shell also was the base for the posix shell standard specifications etc.
each shell does the same job but understand different commands and provide different built in functions.
shell scripting
usually shells are interactive that mean, they accept command as input from users and execute them. however some time we want to execute a bunch of commands routinely, so we have type in all commands each time in terminal.
as shell can also take commands as input from file we can write these commands in a file and can execute them in shell to avoid this repetitive work. these files are called shell scripts or shell programs. shell scripts are similar to the batch file in ms-dos. each shell script is saved with .sh file extension eg. myscript.sh
a shell script have syntax just like any other programming language. if you have any prior experience with any programming language like python, c/c++ etc. it would be very easy to get started with it.
a shell script comprises following elements –
shell keywords – if, else, break etc.
shell commands – cd, ls, echo, pwd, touch etc.
functions
control flow – if..then..else, case and shell loops etc.
why do we need shell scripts
there are many reasons to write shell scripts –
to avoid repetitive work and automation
system admins use shell scripting for routine backups
system monitoring
adding new functionality to the shell etc.
advantages of shell scripts
the command and syntax are exactly the same as those directly entered in command line, so programmer do not need to switch to entirely different syntax
writing shell scripts are much quicker
quick start
interactive debugging etc.
disadvantages of shell scripts
prone to costly errors, a single mistake can change the command which might be harmful
slow execution speed
design flaws within the language syntax or implementation
not well suited for large and complex task
provide minimal data structure unlike other scripting languages. etc
simple demo of shell scripting using bash shell
if you work on terminal, something you traverse deep down in directories. then for coming few directories up in path we have to execute command like this as shown below to get to the “python” directory –
it is quite frustrating, so why not we can have a utility where we just have to type the name of directory and we can directly jump to that without executing “cd ../” command again and again. save the script as “jump.sh”
for now we cannot execute our shell script because it do not have permissions. we have to make it executable by typing following command –
the crontab is a list of commands that you want to run on a regular schedule, and also the name of the command used to manage that list. crontab stands for “cron table, ” because it uses the job scheduler cron to execute tasks; cron itself is named after “chronos, ” the greek word for time.cron is the system process which will automatically perform tasks for you according to a set schedule. the schedule is called the crontab, which is also the name of the program used to edit that schedule.
linux crontab format
min hour dom mon dow cmd
crontab fields and allowed ranges (linux crontab syntax)
field description allowed value min minute field  to  hour hour field  to  dom day of month - mon month field - dow day of week - cmd command any command to be executed.
examples of cron jobs
. scheduling a job for a specific time
the basic usage of cron is to execute a job in a specific time as shown below. this will execute the full backup shell script (full-backup) on th june : am.
the time field uses  hours format. so, for  am use , and for  pm use .
    * /home/maverick/full-backup
 – th minute
 –  am
 – th day
 – th month (june)
* – every day of the week
.to view the crontab entries
view current logged-in user’s crontab entries : to view your crontab entries type crontab -l from your unix account.
view root crontab entries : login as root user (su – root) and do crontab -l.
to view crontab entries of other linux users : login to root and use -u {username} -l.
.to edit crontab entries
edit current logged-in user’s crontab entries.to edit a crontab entries, use crontab -e. by default this will edit the current logged-in users crontab.
.to schedule a job for every minute using cron.
ideally you may not have a requirement to schedule a job every minute. but understanding this example will will help you understand the other examples.
* * * * * cmd
the * means all the possible unit — i.e every minute of every hour through out the year. more than using this * directly, you will find it very useful in the following cases.
when you specify */ in minute field means every  minutes.
when you specify -/ in minute field mean every  minutes in the first  minute.
thus the above convention can be used for all the other  fields.
.to schedule a job for more than one time (e.g. twice a day)
the following script take a incremental backup twice a day every day.
this example executes the specified incremental backup shell script (incremental-backup) at : and : on every day. the comma separated value in a field specifies that the command needs to be executed in all the mentioned time.
 ,  * * * /home/maverick/bin/incremental-backup
 – th minute (top of the hour)
,  –  am and  pm
* – every day
* – every month
* – every day of the week
.to schedule a job for certain range of time (e.g. only on weekdays)
if you wanted a job to be scheduled for every hour with in a specific range of time then use the following.
cron job everyday during working hours :
this example checks the status of the database everyday (including weekends) during the working hours  a.m –  p.m  - * * * /home/maverick/bin/check-db-status  – th minute (top of the hour)
- –  am,  am,  am,  am,  pm,  pm,  pm,  pm,  pm,  pm
* – every day
* – every month
* – every day of the week
this example checks the status of the database everyday (including weekends) during the working hours  a.m –  p.m cron job every weekday during working hours :
this example checks the status of the database every weekday (i.e excluding sat and sun) during the working hours  a.m –  p.m.  - * * - /home/maverick/bin/check-db-status  – th minute (top of the hour)
- –  am,  am,  am,  am,  pm,  pm,  pm,  pm,  pm,  pm
* – every day
* – every month
- -mon, tue, wed, thu and fri (every weekday)
.to schedule a background cron job for every  minutes.
use the following, if you want to check the disk space every  minutes.
*/ * * * * /home/maverick/check-disk-space
it executes the specified command check-disk-space every  minutes through out the year. but you may have a requirement of executing the command only during certain hours or vice versa. the above examples shows how to do those things.instead of specifying values in the  fields, we can specify it using a single keyword as mentioned below.
there are special cases in which instead of the above  fields you can use @ followed by a keyword — such as reboot, midnight, yearly, hourly.
cron special keywords and its meaning
keyword equivalent @yearly     * @daily   * * * @hourly  * * * * @reboot run at startup.
.to schedule a job for first minute of every year using @yearly
if you want a job to be executed on the first minute of every year, then you can use the @yearly cron keyword as shown below.this will execute the system annual maintenance using annual-maintenance shell script at : on jan st for every year.
@yearly /home/maverick/bin/annual-maintenance
.to schedule a cron job beginning of every month using @monthly
it is as similar as the @yearly as above. but executes the command monthly once using @monthly cron keyword.this will execute the shell script tape-backup at : on st of every month.
@monthly /home/maverick/bin/tape-backup
.to schedule a background job every day using @daily
using the @daily cron keyword, this will do a daily log file cleanup using cleanup-logs shell script at : on every day.
@daily /home/maverick/bin/cleanup-logs "day started"
.to execute a linux command after every reboot using @reboot
using the @reboot cron keyword, this will execute the specified command once after the machine got booted every time.
@reboot cmd
reference : linux man page for cron
this article is contributed by kishlay verma. if you like geeksforgeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. see your article appearing on the geeksforgeeks main page and help other geeks.
please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
attention reader! don’t stop learning now. get hold of all the important cs theory concepts for sde interviews with the cs theory course at a student-friendly price and become industry ready.
how to limit search a specified directory in linux?
there is a command in linux to search for files in a directory hierarchy known as ‘find’. it searches the directory tree rooted at each given starting-point by evaluating the given expression from left to right, according to the rules of precedence, until the outcome is known (the left-hand side is false for and operations, true for or), at which point find moves on to the next file name. if no starting-point is specified, `.’ is assumed.
the find command by default travels down the entire directory tree recursively, which is time and resource consuming. however the depth of directory traversal can be specified(which are mindepth and maxdepth).
what are mindepth and maxdepth levels?
maxdepth levels : descend at most levels (a non-negative integer) levels of directories below the starting-points. -maxdepth  means only apply the tests and actions to the starting-points themselves.
: descend at most levels (a non-negative integer) levels of directories below the starting-points. -maxdepth  means only apply the tests and actions to the starting-points themselves. mindepth levels : do not apply any tests or actions at levels less than levels (a non-negative integer). -mindepth  means process all files except the starting-points.
given below some examples to illustrate how depth of the directory traversal can be specified using mindepth and maxdepth
find the passwd file under all sub-directories starting from the root directory. find / -name passwd
find the passwd file under root and one level down. (i.e root — level , and one sub-directory — level ) find / -maxdepth  -name passwd
find the passwd file under root and two levels down. (i.e root — level , and two sub-directories — level  and  ) find / -maxdepth  -name passwd
find the password file between sub-directory level  and . find / -mindepth  -maxdepth  -name passwd
there are two other ways to limit search a directory in linux :
corporate trainer
prof. arnab chakraborty is a calcutta university alumnus with b.sc. in physics hons gold medalist, b. tech and m. tech in computer science and engineering has twenty-six+ years of academic teaching experience in different universities, colleges and thirteen+ years of corporate training experiences for + companies and trained ,+ professionals. he has also completed mba from vidyasagar university with dual specialization in human resource management and marketing management. he is nlp and pmp trained, "global dmaic six sigma master black belt" certified by iqf (usa). he is certified by isa (usa) on "control and automation system". he is "global itil v foundation" certified as awarded by apmg (uk). qualified for "accredited management teacher" by aima (india). "star python" global certified from star certification (usa). "certified scrum master (csm)" global certification from scrum alliance (usa). he is also empaneled trainer for multiple corporates, e.g. hp, accenture, ibm etc
operating system tutorial
an operating system (os) is a collection of software that manages computer hardware resources and provides common services for computer programs. the operating system is a vital component of the system software in a computer system. this tutorial will take you through step by step approach while learning operating system concepts.
why to learn operating system?
an operating system (os) is an interface between a computer user and computer hardware. an operating system is a software which performs all the basic tasks like file management, memory management, process management, handling input and output, and controlling peripheral devices such as disk drives and printers.
some popular operating systems include linux operating system, windows operating system, vms, os/, aix, z/os, etc.
following are some of important functions of an operating system.
memory management
processor management
device management
file management
security
control over system performance
job accounting
error detecting aids
coordination between other software and users
applications of operating system
following are some of the important activities that an operating system performs −
security − by means of password and similar other techniques, it prevents unauthorized access to programs and data.
control over system performance − recording delays between request for a service and response from the system.
job accounting − keeping track of time and resources used by various jobs and users.
error detecting aids − production of dumps, traces, error messages, and other debugging and error detecting aids.
coordination between other softwares and users − coordination and assignment of compilers, interpreters, assemblers and other software to the various users of the computer systems.
audience
this tutorial has been prepared for the computer science graduates to help them understand the basic to advanced concepts related to operating system.
prerequisites
before you start proceeding with this tutorial, we are making an assumption that you are already aware of basic computer concepts like what is keyboard, mouse, monitor, input, output, primary memory and secondary memory etc. if you are not well aware of these concepts, then we will suggest to go through our short tutorial on computer fundamentals.
operating system - overview
advertisements
an operating system (os) is an interface between a computer user and computer hardware. an operating system is a software which performs all the basic tasks like file management, memory management, process management, handling input and output, and controlling peripheral devices such as disk drives and printers.
some popular operating systems include linux operating system, windows operating system, vms, os/, aix, z/os, etc.
definition
an operating system is a program that acts as an interface between the user and the computer hardware and controls the execution of all kinds of programs.
following are some of important functions of an operating system.
memory management
processor management
device management
file management
security
control over system performance
job accounting
error detecting aids
coordination between other software and users
memory management
memory management refers to management of primary memory or main memory. main memory is a large array of words or bytes where each word or byte has its own address.
main memory provides a fast storage that can be accessed directly by the cpu. for a program to be executed, it must in the main memory. an operating system does the following activities for memory management −
keeps tracks of primary memory, i.e., what part of it are in use by whom, what part are not in use.
in multiprogramming, the os decides which process will get memory when and how much.
allocates the memory when a process requests it to do so.
de-allocates the memory when a process no longer needs it or has been terminated.
processor management
in multiprogramming environment, the os decides which process gets the processor when and for how much time. this function is called process scheduling. an operating system does the following activities for processor management −
keeps tracks of processor and status of process. the program responsible for this task is known as traffic controller .
allocates the processor (cpu) to a process.
de-allocates processor when a process is no longer required.
device management
an operating system manages device communication via their respective drivers. it does the following activities for device management −
keeps tracks of all devices. program responsible for this task is known as the i/o controller .
decides which process gets the device when and for how much time.
allocates the device in the efficient way.
de-allocates devices.
file management
a file system is normally organized into directories for easy navigation and usage. these directories may contain files and other directions.
an operating system does the following activities for file management −
keeps track of information, location, uses, status etc. the collective facilities are often known as file system .
decides who gets the resources.
allocates the resources.
de-allocates the resources.
other important activities
following are some of the important activities that an operating system performs −
security − by means of password and similar other techniques, it prevents unauthorized access to programs and data.
control over system performance − recording delays between request for a service and response from the system.
job accounting − keeping track of time and resources used by various jobs and users.
error detecting aids − production of dumps, traces, error messages, and other debugging and error detecting aids.
coordination between other softwares and users − coordination and assignment of compilers, interpreters, assemblers and other software to the various users of the computer systems.
types of operating system
advertisements
operating systems are there from the very first computer generation and they keep evolving with time. in this chapter, we will discuss some of the important types of operating systems which are most commonly used.
batch operating system
the users of a batch operating system do not interact with the computer directly. each user prepares his job on an off-line device like punch cards and submits it to the computer operator. to speed up processing, jobs with similar needs are batched together and run as a group. the programmers leave their programs with the operator and the operator then sorts the programs with similar requirements into batches.
the problems with batch systems are as follows −
lack of interaction between the user and the job.
cpu is often idle, because the speed of the mechanical i/o devices is slower than the cpu.
difficult to provide the desired priority.
time-sharing operating systems
time-sharing is a technique which enables many people, located at various terminals, to use a particular computer system at the same time. time-sharing or multitasking is a logical extension of multiprogramming. processor's time which is shared among multiple users simultaneously is termed as time-sharing.
the main difference between multiprogrammed batch systems and time-sharing systems is that in case of multiprogrammed batch systems, the objective is to maximize processor use, whereas in time-sharing systems, the objective is to minimize response time.
multiple jobs are executed by the cpu by switching between them, but the switches occur so frequently. thus, the user can receive an immediate response. for example, in a transaction processing, the processor executes each user program in a short burst or quantum of computation. that is, if n users are present, then each user can get a time quantum. when the user submits the command, the response time is in few seconds at most.
the operating system uses cpu scheduling and multiprogramming to provide each user with a small portion of a time. computer systems that were designed primarily as batch systems have been modified to time-sharing systems.
advantages of timesharing operating systems are as follows −
provides the advantage of quick response.
avoids duplication of software.
reduces cpu idle time.
disadvantages of time-sharing operating systems are as follows −
problem of reliability.
question of security and integrity of user programs and data.
problem of data communication.
distributed operating system
distributed systems use multiple central processors to serve multiple real-time applications and multiple users. data processing jobs are distributed among the processors accordingly.
the processors communicate with one another through various communication lines (such as high-speed buses or telephone lines). these are referred as loosely coupled systems or distributed systems. processors in a distributed system may vary in size and function. these processors are referred as sites, nodes, computers, and so on.
the advantages of distributed systems are as follows −
with resource sharing facility, a user at one site may be able to use the resources available at another.
speedup the exchange of data with one another via electronic mail.
if one site fails in a distributed system, the remaining sites can potentially continue operating.
better service to the customers.
reduction of the load on the host computer.
reduction of delays in data processing.
network operating system
a network operating system runs on a server and provides the server the capability to manage data, users, groups, security, applications, and other networking functions. the primary purpose of the network operating system is to allow shared file and printer access among multiple computers in a network, typically a local area network (lan), a private network or to other networks.
examples of network operating systems include microsoft windows server , microsoft windows server , unix, linux, mac os x, novell netware, and bsd.
the advantages of network operating systems are as follows −
centralized servers are highly stable.
security is server managed.
upgrades to new technologies and hardware can be easily integrated into the system.
remote access to servers is possible from different locations and types of systems.
the disadvantages of network operating systems are as follows −
high cost of buying and running a server.
dependency on a central location for most operations.
regular maintenance and updates are required.
real time operating system
a real-time system is defined as a data processing system in which the time interval required to process and respond to inputs is so small that it controls the environment. the time taken by the system to respond to an input and display of required updated information is termed as the response time. so in this method, the response time is very less as compared to online processing.
real-time systems are used when there are rigid time requirements on the operation of a processor or the flow of data and real-time systems can be used as a control device in a dedicated application. a real-time operating system must have well-defined, fixed time constraints, otherwise the system will fail. for example, scientific experiments, medical imaging systems, industrial control systems, weapon systems, robots, air traffic control systems, etc.
there are two types of real-time operating systems.
hard real-time systems
hard real-time systems guarantee that critical tasks complete on time. in hard real-time systems, secondary storage is limited or missing and the data is stored in rom. in these systems, virtual memory is almost never found.
soft real-time systems
soft real-time systems are less restrictive. a critical real-time task gets priority over other tasks and retains the priority until it completes. soft real-time systems have limited utility than hard real-time systems. for example, multimedia, virtual reality, advanced scientific projects like undersea exploration and planetary rovers, etc.
operating system - services
advertisements
an operating system provides services to both the users and to the programs.
it provides programs an environment to execute.
it provides users the services to execute the programs in a convenient manner.
following are a few common services provided by an operating system −
program execution
i/o operations
file system manipulation
communication
error detection
resource allocation
protection
program execution
operating systems handle many kinds of activities from user programs to system programs like printer spooler, name servers, file server, etc. each of these activities is encapsulated as a process.
loads a program into memory.
executes the program.
handles program's execution.
provides a mechanism for process synchronization.
provides a mechanism for process communication.
provides a mechanism for deadlock handling.
i/o operation
an i/o subsystem comprises of i/o devices and their corresponding driver software. drivers hide the peculiarities of specific hardware devices from the users.
an operating system manages the communication between user and device drivers.
i/o operation means read or write operation with any file or any specific i/o device.
operating system provides the access to the required i/o device when required.
file system manipulation
a file represents a collection of related information. computers can store files on the disk (secondary storage), for long-term storage purpose. examples of storage media include magnetic tape, magnetic disk and optical disk drives like cd, dvd. each of these media has its own properties like speed, capacity, data transfer rate and data access methods.
a file system is normally organized into directories for easy navigation and usage. these directories may contain files and other directions. following are the major activities of an operating system with respect to file management −
program needs to read a file or write a file.
the operating system gives the permission to the program for operation on file.
permission varies from read-only, read-write, denied and so on.
operating system provides an interface to the user to create/delete files.
operating system provides an interface to the user to create/delete directories.
operating system provides an interface to create the backup of file system.
communication
in case of distributed systems which are a collection of processors that do not share memory, peripheral devices, or a clock, the operating system manages communications between all the processes. multiple processes communicate with one another through communication lines in the network.
the os handles routing and connection strategies, and the problems of contention and security. following are the major activities of an operating system with respect to communication −
two processes often require data to be transferred between them
both the processes can be on one computer or on different computers, but are connected through a computer network.
communication may be implemented by two methods, either by shared memory or by message passing.
error handling
errors can occur anytime and anywhere. an error may occur in cpu, in i/o devices or in the memory hardware. following are the major activities of an operating system with respect to error handling −
the os constantly checks for possible errors.
the os takes an appropriate action to ensure correct and consistent computing.
resource management
in case of multi-user or multi-tasking environment, resources such as main memory, cpu cycles and files storage are to be allocated to each user or job. following are the major activities of an operating system with respect to resource management −
the os manages all kinds of resources using schedulers.
cpu scheduling algorithms are used for better utilization of cpu.
protection
considering a computer system having multiple users and concurrent execution of multiple processes, the various processes must be protected from each other's activities.
protection refers to a mechanism or a way to control the access of programs, processes, or users to the resources defined by a computer system. following are the major activities of an operating system with respect to protection −
the os ensures that all access to system resources is controlled.
the os ensures that external i/o devices are protected from invalid access attempts.
the os provides authentication features for each user by means of passwords.
operating system - properties
advertisements
batch processing
batch processing is a technique in which an operating system collects the programs and data together in a batch before processing starts. an operating system does the following activities related to batch processing −
the os defines a job which has predefined sequence of commands, programs and data as a single unit.
the os keeps a number a jobs in memory and executes them without any manual information.
jobs are processed in the order of submission, i.e., first come first served fashion.
when a job completes its execution, its memory is released and the output for the job gets copied into an output spool for later printing or processing.
advantages
batch processing takes much of the work of the operator to the computer.
increased performance as a new job get started as soon as the previous job is finished, without any manual intervention.
disadvantages
difficult to debug program.
a job could enter an infinite loop.
due to lack of protection scheme, one batch job can affect pending jobs.
multitasking
multitasking is when multiple jobs are executed by the cpu simultaneously by switching between them. switches occur so frequently that the users may interact with each program while it is running. an os does the following activities related to multitasking −
the user gives instructions to the operating system or to a program directly, and receives an immediate response.
the os handles multitasking in the way that it can handle multiple operations/executes multiple programs at a time.
multitasking operating systems are also known as time-sharing systems.
these operating systems were developed to provide interactive use of a computer system at a reasonable cost.
a time-shared operating system uses the concept of cpu scheduling and multiprogramming to provide each user with a small portion of a time-shared cpu.
each user has at least one separate program in memory.
a program that is loaded into memory and is executing is commonly referred to as a process .
when a process executes, it typically executes for only a very short time before it either finishes or needs to perform i/o.
since interactive i/o typically runs at slower speeds, it may take a long time to complete. during this time, a cpu can be utilized by another process.
the operating system allows the users to share the computer simultaneously. since each action or command in a time-shared system tends to be short, only a little cpu time is needed for each user.
as the system switches cpu rapidly from one user/program to the next, each user is given the impression that he/she has his/her own cpu, whereas actually one cpu is being shared among many users.
multiprogramming
sharing the processor, when two or more programs reside in memory at the same time, is referred as multiprogramming. multiprogramming assumes a single shared processor. multiprogramming increases cpu utilization by organizing jobs so that the cpu always has one to execute.
the following figure shows the memory layout for a multiprogramming system.
an os does the following activities related to multiprogramming.
the operating system keeps several jobs in memory at a time.
this set of jobs is a subset of the jobs kept in the job pool.
the operating system picks and begins to execute one of the jobs in the memory.
multiprogramming operating systems monitor the state of all active programs and system resources using memory management programs to ensures that the cpu is never idle, unless there are no jobs to process.
advantages
high and efficient cpu utilization.
user feels that many programs are allotted cpu almost simultaneously.
disadvantages
cpu scheduling is required.
to accommodate many jobs in memory, memory management is required.
interactivity
interactivity refers to the ability of users to interact with a computer system. an operating system does the following activities related to interactivity −
provides the user an interface to interact with the system.
manages input devices to take inputs from the user. for example, keyboard.
manages output devices to show outputs to the user. for example, monitor.
the response time of the os needs to be short, since the user submits and waits for the result.
real time system
real-time systems are usually dedicated, embedded systems. an operating system does the following activities related to real-time system activity.
in such systems, operating systems typically read from and react to sensor data.
the operating system must guarantee response to events within fixed periods of time to ensure correct performance.
distributed environment
a distributed environment refers to multiple independent cpus or processors in a computer system. an operating system does the following activities related to distributed environment −
the os distributes computation logics among several physical processors.
the processors do not share memory or a clock. instead, each processor has its own local memory.
the os manages the communications between the processors. they communicate with each other through various communication lines.
spooling
spooling is an acronym for simultaneous peripheral operations on line. spooling refers to putting data of various i/o jobs in a buffer. this buffer is a special area in memory or hard disk which is accessible to i/o devices.
an operating system does the following activities related to distributed environment −
handles i/o device data spooling as devices have different data access rates.
maintains the spooling buffer which provides a waiting station where data can rest while the slower device catches up.
maintains parallel computation because of spooling process as a computer can perform i/o in parallel fashion. it becomes possible to have the computer read data from a tape, write data to disk and to write out to a tape printer while it is doing its computing task.
advantages
the spooling operation uses a disk as a very large buffer.
spooling is capable of overlapping i/o operation for one job with processor operations for another job.
operating system - processes
advertisements
process
a process is basically a program in execution. the execution of a process must progress in a sequential fashion.
a process is defined as an entity which represents the basic unit of work to be implemented in the system.
to put it in simple terms, we write our computer programs in a text file and when we execute this program, it becomes a process which performs all the tasks mentioned in the program.
when a program is loaded into the memory and it becomes a process, it can be divided into four sections ─ stack, heap, text and data. the following image shows a simplified layout of a process inside main memory −
s.n. component & description  stack the process stack contains the temporary data such as method/function parameters, return address and local variables.  heap this is dynamically allocated memory to a process during its run time.  text this includes the current activity represented by the value of program counter and the contents of the processor's registers.  data this section contains the global and static variables.
program
#include  int main() { printf("hello, world!
"); return ; }
a computer program is a collection of instructions that performs a specific task when executed by a computer. when we compare a program with a process, we can conclude that a process is a dynamic instance of a computer program.
a part of a computer program that performs a well-defined task is known as an algorithm. a collection of computer programs, libraries and related data are referred to as a software.
process life cycle
when a process executes, it passes through different states. these stages may differ in different operating systems, and the names of these states are also not standardized.
in general, a process can have one of the following five states at a time.
s.n. state & description  start this is the initial state when a process is first started/created.  ready the process is waiting to be assigned to a processor. ready processes are waiting to have the processor allocated to them by the operating system so that they can run. process may come into this state after start state or while running it by but interrupted by the scheduler to assign cpu to some other process.  running once the process has been assigned to a processor by the os scheduler, the process state is set to running and the processor executes its instructions.  waiting process moves into the waiting state if it needs to wait for a resource, such as waiting for user input, or waiting for a file to become available.  terminated or exit once the process finishes its execution, or it is terminated by the operating system, it is moved to the terminated state where it waits to be removed from main memory.
process control block (pcb)
a process control block is a data structure maintained by the operating system for every process. the pcb is identified by an integer process id (pid). a pcb keeps all the information needed to keep track of a process as listed below in the table −
s.n. information & description  process state the current state of the process i.e., whether it is ready, running, waiting, or whatever.  process privileges this is required to allow/disallow access to system resources.  process id unique identification for each of the process in the operating system.  pointer a pointer to parent process.  program counter program counter is a pointer to the address of the next instruction to be executed for this process.  cpu registers various cpu registers where process need to be stored for execution for running state.  cpu scheduling information process priority and other scheduling information which is required to schedule the process.  memory management information this includes the information of page table, memory limits, segment table depending on memory used by the operating system.  accounting information this includes the amount of cpu used for process execution, time limits, execution id etc.  io status information this includes a list of i/o devices allocated to the process.
the architecture of a pcb is completely dependent on operating system and may contain different information in different operating systems. here is a simplified diagram of a pcb −
the pcb is maintained for a process throughout its lifetime, and is deleted once the process terminates.
operating system - process scheduling
advertisements
definition
the process scheduling is the activity of the process manager that handles the removal of the running process from the cpu and the selection of another process on the basis of a particular strategy.
process scheduling is an essential part of a multiprogramming operating systems. such operating systems allow more than one process to be loaded into the executable memory at a time and the loaded process shares the cpu using time multiplexing.
process scheduling queues
the os maintains all pcbs in process scheduling queues. the os maintains a separate queue for each of the process states and pcbs of all processes in the same execution state are placed in the same queue. when the state of a process is changed, its pcb is unlinked from its current queue and moved to its new state queue.
the operating system maintains the following important process scheduling queues −
job queue − this queue keeps all the processes in the system.
ready queue − this queue keeps a set of all processes residing in main memory, ready and waiting to execute. a new process is always put in this queue.
device queues − the processes which are blocked due to unavailability of an i/o device constitute this queue.
the os can use different policies to manage each queue (fifo, round robin, priority, etc.). the os scheduler determines how to move processes between the ready and run queues which can only have one entry per processor core on the system; in the above diagram, it has been merged with the cpu.
two-state process model
two-state process model refers to running and non-running states which are described below −
s.n. state & description  running when a new process is created, it enters into the system as in the running state.  not running processes that are not running are kept in queue, waiting for their turn to execute. each entry in the queue is a pointer to a particular process. queue is implemented by using linked list. use of dispatcher is as follows. when a process is interrupted, that process is transferred in the waiting queue. if the process has completed or aborted, the process is discarded. in either case, the dispatcher then selects a process from the queue to execute.
schedulers
schedulers are special system software which handle process scheduling in various ways. their main task is to select the jobs to be submitted into the system and to decide which process to run. schedulers are of three types −
long-term scheduler
short-term scheduler
medium-term scheduler
long term scheduler
it is also called a job scheduler. a long-term scheduler determines which programs are admitted to the system for processing. it selects processes from the queue and loads them into memory for execution. process loads into the memory for cpu scheduling.
the primary objective of the job scheduler is to provide a balanced mix of jobs, such as i/o bound and processor bound. it also controls the degree of multiprogramming. if the degree of multiprogramming is stable, then the average rate of process creation must be equal to the average departure rate of processes leaving the system.
on some systems, the long-term scheduler may not be available or minimal. time-sharing operating systems have no long term scheduler. when a process changes the state from new to ready, then there is use of long-term scheduler.
short term scheduler
it is also called as cpu scheduler. its main objective is to increase system performance in accordance with the chosen set of criteria. it is the change of ready state to running state of the process. cpu scheduler selects a process among the processes that are ready to execute and allocates cpu to one of them.
short-term schedulers, also known as dispatchers, make the decision of which process to execute next. short-term schedulers are faster than long-term schedulers.
medium term scheduler
medium-term scheduling is a part of swapping. it removes the processes from the memory. it reduces the degree of multiprogramming. the medium-term scheduler is in-charge of handling the swapped out-processes.
a running process may become suspended if it makes an i/o request. a suspended processes cannot make any progress towards completion. in this condition, to remove the process from memory and make space for other processes, the suspended process is moved to the secondary storage. this process is called swapping, and the process is said to be swapped out or rolled out. swapping may be necessary to improve the process mix.
comparison among scheduler
s.n. long-term scheduler short-term scheduler medium-term scheduler  it is a job scheduler it is a cpu scheduler it is a process swapping scheduler.  speed is lesser than short term scheduler speed is fastest among other two speed is in between both short and long term scheduler.  it controls the degree of multiprogramming it provides lesser control over degree of multiprogramming it reduces the degree of multiprogramming.  it is almost absent or minimal in time sharing system it is also minimal in time sharing system it is a part of time sharing systems.  it selects processes from pool and loads them into memory for execution it selects those processes which are ready to execute it can re-introduce the process into memory and execution can be continued.
context switch
a context switch is the mechanism to store and restore the state or context of a cpu in process control block so that a process execution can be resumed from the same point at a later time. using this technique, a context switcher enables multiple processes to share a single cpu. context switching is an essential part of a multitasking operating system features.
when the scheduler switches the cpu from executing one process to execute another, the state from the current running process is stored into the process control block. after this, the state for the process to run next is loaded from its own pcb and used to set the pc, registers, etc. at that point, the second process can start executing.
context switches are computationally intensive since register and memory state must be saved and restored. to avoid the amount of context switching time, some hardware systems employ two or more sets of processor registers. when the process is switched, the following information is stored for later use.
program counter
scheduling information
base and limit register value
currently used register
changed state
i/o state information
accounting information
operating system scheduling algorithms
advertisements
a process scheduler schedules different processes to be assigned to the cpu based on particular scheduling algorithms. there are six popular process scheduling algorithms which we are going to discuss in this chapter −
first-come, first-served (fcfs) scheduling
shortest-job-next (sjn) scheduling
priority scheduling
shortest remaining time
round robin(rr) scheduling
multiple-level queues scheduling
these algorithms are either non-preemptive or preemptive. non-preemptive algorithms are designed so that once a process enters the running state, it cannot be preempted until it completes its allotted time, whereas the preemptive scheduling is based on priority where a scheduler may preempt a low priority running process anytime when a high priority process enters into a ready state.
first come first serve (fcfs)
jobs are executed on first come, first serve basis.
it is a non-preemptive, pre-emptive scheduling algorithm.
easy to understand and implement.
its implementation is based on fifo queue.
poor in performance as average wait time is high.
wait time of each process is as follows −
process wait time : service time - arrival time p  -  =  p  -  =  p  -  =  p  -  = 
average wait time: (+++) /  = .
shortest job next (sjn)
this is also known as shortest job first , or sjf
this is a non-preemptive, pre-emptive scheduling algorithm.
best approach to minimize waiting time.
easy to implement in batch systems where required cpu time is known in advance.
impossible to implement in interactive systems where required cpu time is not known.
the processer should know in advance how much time process will take.
given: table of processes, and their arrival time, execution time
process arrival time execution time service time p    p    p    p   
waiting time of each process is as follows −
process waiting time p  -  =  p  -  =  p  -  =  p  -  = 
average wait time: ( +  +  + )/ =  /  = .
priority based scheduling
priority scheduling is a non-preemptive algorithm and one of the most common scheduling algorithms in batch systems.
each process is assigned a priority. process with highest priority is to be executed first and so on.
processes with same priority are executed on first come first served basis.
priority can be decided based on memory requirements, time requirements or any other resource requirement.
given: table of processes, and their arrival time, execution time, and priority. here we are considering  is the lowest priority.
process arrival time execution time priority service time p     p     p     p    
waiting time of each process is as follows −
process waiting time p  -  =  p  -  =  p  -  =  p  -  = 
average wait time: ( +  +  + )/ =  /  = 
shortest remaining time
shortest remaining time (srt) is the preemptive version of the sjn algorithm.
the processor is allocated to the job closest to completion but it can be preempted by a newer ready job with shorter time to completion.
impossible to implement in interactive systems where required cpu time is not known.
it is often used in batch environments where short jobs need to give preference.
round robin scheduling
round robin is the preemptive process scheduling algorithm.
each process is provided a fix time to execute, it is called a quantum .
once a process is executed for a given time period, it is preempted and other process executes for a given time period.
context switching is used to save states of preempted processes.
wait time of each process is as follows −
process wait time : service time - arrival time p ( - ) + ( - ) =  p ( - ) =  p ( - ) + ( - ) + ( - ) =  p ( - ) + ( - ) = 
average wait time: (+++) /  = .
multiple-level queues scheduling
multiple-level queues are not an independent scheduling algorithm. they make use of other existing algorithms to group and schedule jobs with common characteristics.
multiple queues are maintained for processes with common characteristics.
each queue can have its own scheduling algorithms.
priorities are assigned to each queue.
for example, cpu-bound jobs can be scheduled in one queue and all i/o-bound jobs in another queue. the process scheduler then alternately selects jobs from each queue and assigns them to the cpu based on the algorithm assigned to the queue.
operating system - multi-threading
advertisements
what is thread?
a thread is also called a lightweight process. threads provide a way to improve application performance through parallelism. threads represent a software approach to improving performance of operating system by reducing the overhead thread is equivalent to a classical process.
each thread belongs to exactly one process and no thread can exist outside a process. each thread represents a separate flow of control. threads have been successfully used in implementing network servers and web server. they also provide a suitable foundation for parallel execution of applications on shared memory multiprocessors. the following figure shows the working of a single-threaded and a multithreaded process.
difference between process and thread
advantages of thread
threads minimize the context switching time.
use of threads provides concurrency within a process.
efficient communication.
it is more economical to create and context switch threads.
threads allow utilization of multiprocessor architectures to a greater scale and efficiency.
types of thread
threads are implemented in following two ways −
user level threads − user managed threads.
kernel level threads − operating system managed threads acting on kernel, an operating system core.
user level threads
advantages
thread switching does not require kernel mode privileges.
user level thread can run on any operating system.
scheduling can be application specific in the user level thread.
user level threads are fast to create and manage.
disadvantages
in a typical operating system, most system calls are blocking.
multithreaded application cannot take advantage of multiprocessing.
kernel level threads
the kernel maintains context information for the process as a whole and for individuals threads within the process. scheduling by the kernel is done on a thread basis. the kernel performs thread creation, scheduling and management in kernel space. kernel threads are generally slower to create and manage than the user threads.
advantages
kernel can simultaneously schedule multiple threads from the same process on multiple processes.
if one thread in a process is blocked, the kernel can schedule another thread of the same process.
kernel routines themselves can be multithreaded.
disadvantages
kernel threads are generally slower to create and manage than the user threads.
transfer of control from one thread to another within the same process requires a mode switch to the kernel.
multithreading models
some operating system provide a combined user level thread and kernel level thread facility. solaris is a good example of this combined approach. in a combined system, multiple threads within the same application can run in parallel on multiple processors and a blocking system call need not block the entire process. multithreading models are three types
many to many relationship.
many to one relationship.
one to one relationship.
many to many model
the many-to-many model multiplexes any number of user threads onto an equal or smaller number of kernel threads.
the following diagram shows the many-to-many threading model where  user level threads are multiplexing with  kernel level threads. in this model, developers can create as many user threads as necessary and the corresponding kernel threads can run in parallel on a multiprocessor machine. this model provides the best accuracy on concurrency and when a thread performs a blocking system call, the kernel can schedule another thread for execution.
many to one model
many-to-one model maps many user level threads to one kernel-level thread. thread management is done in user space by the thread library. when thread makes a blocking system call, the entire process will be blocked. only one thread can access the kernel at a time, so multiple threads are unable to run in parallel on multiprocessors.
if the user-level thread libraries are implemented in the operating system in such a way that the system does not support them, then the kernel threads use the many-to-one relationship modes.
one to one model
there is one-to-one relationship of user-level thread to the kernel-level thread. this model provides more concurrency than the many-to-one model. it also allows another thread to run when a thread makes a blocking system call. it supports multiple threads to execute in parallel on microprocessors.
disadvantage of this model is that creating user thread requires the corresponding kernel thread. os/, windows nt and windows  use one to one relationship model.
difference between user-level & kernel-level thread
s.n. user-level threads kernel-level thread  user-level threads are faster to create and manage. kernel-level threads are slower to create and manage.  implementation is by a thread library at the user level. operating system supports creation of kernel threads.  user-level thread is generic and can run on any operating system. kernel-level thread is specific to the operating system.  multi-threaded applications cannot take advantage of multiprocessing. kernel routines themselves can be multithreaded.
operating system - memory management
advertisements
memory management is the functionality of an operating system which handles or manages primary memory and moves processes back and forth between main memory and disk during execution. memory management keeps track of each and every memory location, regardless of either it is allocated to some process or it is free. it checks how much memory is to be allocated to processes. it decides which process will get memory at what time. it tracks whenever some memory gets freed or unallocated and correspondingly it updates the status.
this tutorial will teach you basic concepts related to memory management.
process address space
the process address space is the set of logical addresses that a process 
operating system - virtual memory
advertisements
a computer can address more memory than the amount physically installed on the system. this extra memory is actually called virtual memory and it is a section of a hard disk that's set up to emulate the computer's ram.
the main visible advantage of this scheme is that programs can be larger than physical memory. virtual memory serves two purposes. first, it allows us to extend the use of physical memory by using disk. second, it allows us to have memory protection, because each virtual address is translated to a physical address.
following are the situations, when entire program is not required to be loaded fully in main memory.
user written error handling routines are used only when an error occurred in the data or computation.
certain options and features of a program may be used rarely.
many tables are assigned a fixed amount of address space even though only a small amount of the table is actually used.
the ability to execute a program that is only partially in memory would counter many benefits.
less number of i/o would be needed to load or swap each user program into memory.
a program would no longer be constrained by the amount of physical memory that is available.
each user program could take less physical memory, more programs could be run the same time, with a corresponding increase in cpu utilization and throughput.
modern microprocessors intended for general-purpose use, a memory management unit, or mmu, is built into the hardware. the mmu's job is to translate virtual addresses into physical addresses. a basic example is given below −
virtual memory is commonly implemented by demand paging. it can also be implemented in a segmentation system. demand segmentation can also be used to provide virtual memory.
demand paging
a demand paging system is quite similar to a paging system with swapping where processes reside in secondary memory and pages are loaded only on demand, not in advance. when a context switch occurs, the operating system does not copy any of the old program’s pages out to the disk or any of the new program’s pages into the main memory instead, it just begins executing the new program after loading the first page and fetches that program’s pages as they are referenced.
while executing a program, if the program 
operating system - i/o hardware
advertisements
one of the important jobs of an operating system is to manage various i/o devices including mouse, keyboards, touch pad, disk drives, display adapters, usb devices, bit-mapped screen, led, analog-to-digital converter, on/off switch, network connections, audio i/o, printers etc.
an i/o system is required to take an application i/o request and send it to the physical device, then take whatever response comes back from the device and send it to the application. i/o devices can be divided into two categories −
block devices − a block device is one with which the driver communicates by sending entire blocks of data. for example, hard disks, usb cameras, disk-on-key etc.
character devices − a character device is one with which the driver communicates by sending and receiving single characters (bytes, octets). for example, serial ports, parallel ports, sounds cards etc
device controllers
device drivers are software modules that can be plugged into an os to handle a particular device. operating system takes help from device drivers to handle all i/o devices.
the device controller works like an interface between a device and a device driver. i/o units (keyboard, mouse, printer, etc.) typically consist of a mechanical component and an electronic component where electronic component is called the device controller.
there is always a device controller and a device driver for each device to communicate with the operating systems. a device controller may be able to handle multiple devices. as an interface its main task is to convert serial bit stream to block of bytes, perform error correction as necessary.
any device connected to the computer is connected by a plug and socket, and the socket is connected to a device controller. following is a model for connecting the cpu, memory, controllers, and i/o devices where cpu and device controllers all use a common bus for communication.
synchronous vs asynchronous i/o
synchronous i/o − in this scheme cpu execution waits while i/o proceeds
asynchronous i/o − i/o proceeds concurrently with cpu execution
communication to i/o devices
the cpu must have a way to pass information to and from an i/o device. there are three approaches available to communicate with the cpu and device.
special instruction i/o
memory-mapped i/o
direct memory access (dma)
special instruction i/o
this uses cpu instructions that are specifically made for controlling i/o devices. these instructions typically allow data to be sent to an i/o device or read from an i/o device.
memory-mapped i/o
when using memory-mapped i/o, the same address space is shared by memory and i/o devices. the device is connected directly to certain main memory locations so that i/o device can transfer block of data to/from memory without going through cpu.
while using memory mapped io, os allocates buffer in memory and informs i/o device to use that buffer to send data to the cpu. i/o device operates asynchronously with cpu, interrupts cpu when finished.
the advantage to this method is that every instruction which can access memory can be used to manipulate an i/o device. memory mapped io is used for most high-speed i/o devices like disks, communication interfaces.
direct memory access (dma)
slow devices like keyboards will generate an interrupt to the main cpu after each byte is transferred. if a fast device such as a disk generated an interrupt for each byte, the operating system would spend most of its time handling these interrupts. so a typical computer uses direct memory access (dma) hardware to reduce this overhead.
direct memory access (dma) means cpu grants i/o module authority to read from or write to memory without involvement. dma module itself controls exchange of data between main memory and the i/o device. cpu is only involved at the beginning and end of the transfer and interrupted only after entire block has been transferred.
direct memory access needs a special hardware called dma controller (dmac) that manages the data transfers and arbitrates access to the system bus. the controllers are programmed with source and destination pointers (where to read/write the data), counters to track the number of transferred bytes, and settings, which includes i/o and memory types, interrupts and states for the cpu cycles.
the operating system uses the dma hardware as follows −
step description  device driver is instructed to transfer disk data to a buffer address x.  device driver then instruct disk controller to transfer data to buffer.  disk controller starts dma transfer.  disk controller sends each byte to dma controller.  dma controller transfers bytes to buffer, increases the memory address, decreases the counter c until c becomes zero.  when c becomes zero, dma interrupts cpu to signal transfer completion.
polling vs interrupts i/o
a computer must have a way of detecting the arrival of any type of input. there are two ways that this can happen, known as polling and interrupts. both of these techniques allow the processor to deal with events that can happen at any time and that are not related to the process it is currently running.
polling i/o
polling is the simplest way for an i/o device to communicate with the processor. the process of periodically checking status of the device to see if it is time for the next i/o operation, is called polling. the i/o device simply puts the information in a status register, and the processor must come and get the information.
most of the time, devices will not require attention and when one does it will have to wait until it is next interrogated by the polling program. this is an inefficient method and much of the processors time is wasted on unnecessary polls.
compare this method to a teacher continually asking every student in a class, one after another, if they need help. obviously the more efficient method would be for a student to inform the teacher whenever they require assistance.
interrupts i/o
an alternative scheme for dealing with i/o is the interrupt-driven method. an interrupt is a signal to the microprocessor from a device that requires attention.
a device controller puts an interrupt signal on the bus when it needs cpu’s attention when cpu receives an interrupt, it saves its current state and invokes the appropriate interrupt handler using the interrupt vector (addresses of os routines to handle various events). when the interrupting device has been dealt with, the cpu continues with its original task as if it had never been interrupted.
operating system - i/o softwares
advertisements
i/o software is often organized in the following layers −
user level libraries − this provides simple interface to the user program to perform input and output. for example, stdio is a library provided by c and c++ programming languages.
kernel level modules − this provides device driver to interact with the device controller and device independent i/o modules used by the device drivers.
hardware − this layer includes actual hardware and hardware controller which interact with the device drivers and makes hardware alive.
a key concept in the design of i/o software is that it should be device independent where it should be possible to write programs that can access any i/o device without having to specify the device in advance. for example, a program that reads a file as input should be able to read a file on a floppy disk, on a hard disk, or on a cd-rom, without having to modify the program for each different device.
device drivers
a device driver performs the following jobs −
to accept request from the device independent software above to it.
interact with the device controller to take and give i/o and perform required error handling
making sure that the request is executed successfully
how a device driver handles a request is as follows: suppose a request comes to read a block n. if the driver is idle at the time a request arrives, it starts carrying out the request immediately. otherwise, if the driver is already busy with some other request, it places the new request in the queue of pending requests.
interrupt handlers
an interrupt handler, also known as an interrupt service routine or isr, is a piece of software or more specifically a callback function in an operating system or more specifically in a device driver, whose execution is triggered by the reception of an interrupt.
when the interrupt happens, the interrupt procedure does whatever it has to in order to handle the interrupt, updates data structures and wakes up process that was waiting for an interrupt to happen.
the interrupt mechanism accepts an address ─ a number that selects a specific interrupt handling routine/function from a small set. in most architectures, this address is an offset stored in a table called the interrupt vector table. this vector contains the memory addresses of specialized interrupt handlers.
device-independent i/o software
the basic function of the device-independent software is to perform the i/o functions that are common to all devices and to provide a uniform interface to the user-level software. though it is difficult to write completely device independent software but we can write some modules which are common among all the devices. following is a list of functions of device-independent i/o software −
uniform interfacing for device drivers
device naming - mnemonic names mapped to major and minor device numbers
device protection
providing a device-independent block size
buffering because data coming off a device cannot be stored in final destination.
storage allocation on block devices
allocation and releasing dedicated devices
error reporting
user-space i/o software
these are the libraries which provide richer and simplified interface to access the functionality of the kernel or ultimately interactive with the device drivers. most of the user-level i/o software consists of library procedures with some exception like spooling system which is a way of dealing with dedicated i/o devices in a multiprogramming system.
i/o libraries (e.g., stdio) are in user-space to provide an interface to the os resident device-independent i/o sw. for example putchar(), getchar(), printf() and scanf() are example of user level i/o library stdio available in c programming.
kernel i/o subsystem
kernel i/o subsystem is responsible to provide many services related to i/o. following are some of the services provided.
scheduling − kernel schedules a set of i/o requests to determine a good order in which to execute them. when an application issues a blocking i/o system call, the request is placed on the queue for that device. the kernel i/o scheduler rearranges the order of the queue to improve the overall system efficiency and the average response time experienced by the applications.
buffering − kernel i/o subsystem maintains a memory area known as buffer that stores data while they are transferred between two devices or between a device with an application operation. buffering is done to cope with a speed mismatch between the producer and consumer of a data stream or to adapt between devices that have different data transfer sizes.
caching − kernel maintains cache memory which is region of fast memory that holds copies of data. access to the cached copy is more efficient than access to the original.
spooling and device reservation − a spool is a buffer that holds output for a device, such as a printer, that cannot accept interleaved data streams. the spooling system copies the queued spool files to the printer one at a time. in some operating systems, spooling is managed by a system daemon process. in other operating systems, it is handled by an in kernel thread.
error handling − an operating system that uses protected memory can guard against many kinds of hardware and application errors.
operating system - file system
advertisements
file
a file is a named collection of related information that is recorded on secondary storage such as magnetic disks, magnetic tapes and optical disks. in general, a file is a sequence of bits, bytes, lines or records whose meaning is defined by the files creator and user.
file structure
a file structure should be according to a required format that the operating system can understand.
a file has a certain defined structure according to its type.
a text file is a sequence of characters organized into lines.
a source file is a sequence of procedures and functions.
an object file is a sequence of bytes organized into blocks that are understandable by the machine.
file type
file type refers to the ability of the operating system to distinguish different types of file such as text files source files and binary files etc. many operating systems support many types of files. operating system like ms-dos and unix have the following types of files −
ordinary files
these are the files that contain user information.
these may have text, databases or executable program.
the user can apply various operations on such files like add, modify, delete or even remove the entire file.
directory files
these files contain list of file names and other information related to these files.
special files
these files are also known as device files.
these files represent physical device like disks, terminals, printers, networks, tape drive etc.
these files are of two types −
character special files − data is handled character by character as in case of terminals or printers.
block special files − data is handled in blocks as in the case of disks and tapes.
file access mechanisms
file access mechanism refers to the manner in which the records of a file may be accessed. there are several ways to access files −
sequential access
direct/random access
indexed sequential access
sequential access
a sequential access is that in which the records are accessed in some sequence, i.e., the information in the file is processed in order, one record after the other. this access method is the most primitive one. example: compilers usually access files in this fashion.
direct/random access
random access file organization provides, accessing the records directly.
each record has its own address on the file with by the help of which it can be directly accessed for reading or writing.
the records need not be in any sequence within the file and they need not be in adjacent locations on the storage medium.
indexed sequential access
this mechanism is built up on base of sequential access.
an index is created for each file which contains pointers to various blocks.
index is searched sequentially and its pointer is used to access the file directly.
space allocation
files are allocated disk spaces by operating system. operating systems deploy following three main ways to allocate disk space to files.
contiguous allocation
linked allocation
indexed allocation
contiguous allocation
each file occupies a contiguous address space on disk.
assigned disk address is in linear order.
easy to implement.
external fragmentation is a major issue with this type of allocation technique.
linked allocation
each file carries a list of links to disk blocks.
directory contains link / pointer to first block of a file.
no external fragmentation
effectively used in sequential access file.
inefficient in case of direct access file.
indexed allocation
provides solutions to problems of contiguous and linked allocation.
a index block is created having all pointers to files.
each file has its own index block which stores the addresses of disk space occupied by the file.
directory contains the addresses of index blocks of files.
operating system - security
advertisements
security refers to providing a protection system to computer system resources such as cpu, memory, disk, software programs and most importantly data/information stored in the computer system. if a computer program is run by an unauthorized user, then he/she may cause severe damage to computer or data stored in it. so a computer system must be protected against unauthorized access, malicious access to system memory, viruses, worms etc. we're going to discuss following topics in this chapter.
authentication
one time passwords
program threats
system threats
computer security classifications
authentication
authentication refers to identifying each user of the system and associating the executing programs with those users. it is the responsibility of the operating system to create a protection system which ensures that a user who is running a particular program is authentic. operating systems generally identifies/authenticates users using following three ways −
username / password − user need to enter a registered username and password with operating system to login into the system.
user card/key − user need to punch card in card slot, or enter key generated by key generator in option provided by operating system to login into the system.
user attribute - fingerprint/ eye retina pattern/ signature − user need to pass his/her attribute via designated input device used by operating system to login into the system.
one time passwords
one-time passwords provide additional security along with normal authentication. in one-time password system, a unique password is required every time user tries to login into the system. once a one-time password is used, then it cannot be used again. one-time password are implemented in various ways.
random numbers − users are provided cards having numbers printed along with corresponding alphabets. system asks for numbers corresponding to few alphabets randomly chosen.
secret key − user are provided a hardware device which can create a secret id mapped with user id. system asks for such secret id which is to be generated every time prior to login.
network password − some commercial applications send one-time passwords to user on registered mobile/ email which is required to be entered prior to login.
program threats
operating system's processes and kernel do the designated task as instructed. if a user program made these process do malicious tasks, then it is known as program threats. one of the common example of program threat is a program installed in a computer which can store and send user credentials via network to some hacker. following is the list of some well-known program threats.
trojan horse − such program traps user login credentials and stores them to send to malicious user who can later on login to computer and can access system resources.
logic bomb − logic bomb is a situation when a program misbehaves only when certain conditions met otherwise it works as a genuine program. it is harder to detect.
system threats
system threats refers to misuse of system services and network connections to put user in trouble. system threats can be used to launch program threats on a complete network called as program attack. system threats creates such an environment that operating system resources/ user files are misused. following is the list of some well-known system threats.
worm − worm is a process which can choked down a system performance by using system resources to extreme levels. a worm process generates its multiple copies where each copy uses system resources, prevents all other processes to get required resources. worms processes can even shut down an entire network.
port scanning − port scanning is a mechanism or means by which a hacker can detects system vulnerabilities to make an attack on the system.
denial of service − denial of service attacks normally prevents user to make legitimate use of the system. for example, a user may not be able to use internet if denial of service attacks browser's content settings.
computer security classifications
as per the u.s. department of defense trusted computer system's evaluation criteria there are four security classifications in computer systems: a, b, c, and d. this is widely used specifications to determine and model the security of systems and of security solutions. following is the brief description of each classification.
s.n. classification type & description  type a highest level. uses formal design specifications and verification techniques. grants a high degree of assurance of process security.  type b provides mandatory protection system. have all the properties of a class c system. attaches a sensitivity label to each object. it is of three types. b − maintains the security label of each object in the system. label is used for making decisions to access control.
b − extends the sensitivity labels to each system resource, such as storage objects, supports covert channels and auditing of events.
b − allows creating lists or user groups for access-control to grant access or revoke access to a given named object.  type c provides protection and user accountability using audit capabilities. it is of two types. c − incorporates controls so that users can protect their private information and keep other users from accidentally reading / deleting their data. unix versions are mostly cl class.
c − adds an individual-level access control to the capabilities of a cl level system.  type d lowest level. minimum protection. ms-dos, window . fall in this category.
operating system - linux
advertisements
components of linux system
linux operating system has primarily three components
kernel − kernel is the core part of linux. it is responsible for all major activities of this operating system. it consists of various modules and it interacts directly with the underlying hardware. kernel provides the required abstraction to hide low level hardware details to system or application programs.
system utility − system utility programs are responsible to do specialized, individual level tasks.
kernel mode vs user mode
basic features
following are some of the important features of linux operating system.
portable − portability means software can works on different types of hardware in same way. linux kernel and application programs supports their installation on any kind of hardware platform.
multi-user − linux is a multiuser system means multiple users can access system resources like memory/ ram/ application programs at same time.
multiprogramming − linux is a multiprogramming system means multiple applications can run at same time.
hierarchical file system − linux provides a standard file structure in which system files/ user files are arranged.
shell − linux provides a special interpreter program which can be used to execute commands of the operating system. it can be used to do various types of operations, call application programs. etc.
security − linux provides user security using authentication features like password protection/ controlled access to specific files/ encryption of data.
architecture
the following illustration shows the architecture of a linux system −
the architecture of a linux system consists of the following layers −
hardware layer − hardware consists of all peripheral devices (ram/ hdd/ cpu etc).
kernel − it is the core component of operating system, interacts directly with hardware, provides low level services to upper layer components.
shell − an interface to kernel, hiding complexity of kernel's functions from users. the shell takes commands from the user and executes kernel's functions.
utilities − utility programs that provide the user most of the functionalities of an operating systems.
os exams questions with answers
advertisements
these selected questions and answers are prepared from operating systems exam point of view and will also help in quick revision to get good marks in operating systems examination. these questions has been prepared for the computer science graduates (b.c.a, m.c.a, b.tech, b.e. and so...), to help them understand and revise the basic to advanced concepts related to operating system.
following is the selected list of questions and their answers and will help in quick revision to get good marks in operating systems examination.
operating systems overview
operating systems process
operating systems types
operating systems process scheduling
operating systems memory allocation
operating systems semaphores
operating system - quick guide
advertisements
operating system - overview
an operating system (os) is an interface between a computer user and computer hardware. an operating system is a software which performs all the basic tasks like file management, memory management, process management, handling input and output, and controlling peripheral devices such as disk drives and printers.
some popular operating systems include linux operating system, windows operating system, vms, os/, aix, z/os, etc.
definition
an operating system is a program that acts as an interface between the user and the computer hardware and controls the execution of all kinds of programs.
following are some of important functions of an operating system.
memory management
processor management
device management
file management
security
control over system performance
job accounting
error detecting aids
coordination between other software and users
memory management
memory management refers to management of primary memory or main memory. main memory is a large array of words or bytes where each word or byte has its own address.
main memory provides a fast storage that can be accessed directly by the cpu. for a program to be executed, it must in the main memory. an operating system does the following activities for memory management −
keeps tracks of primary memory, i.e., what part of it are in use by whom, what part are not in use.
in multiprogramming, the os decides which process will get memory when and how much.
allocates the memory when a process requests it to do so.
de-allocates the memory when a process no longer needs it or has been terminated.
processor management
in multiprogramming environment, the os decides which process gets the processor when and for how much time. this function is called process scheduling. an operating system does the following activities for processor management −
keeps tracks of processor and status of process. the program responsible for this task is known as traffic controller .
allocates the processor (cpu) to a process.
de-allocates processor when a process is no longer required.
device management
an operating system manages device communication via their respective drivers. it does the following activities for device management −
keeps tracks of all devices. program responsible for this task is known as the i/o controller .
decides which process gets the device when and for how much time.
allocates the device in the efficient way.
de-allocates devices.
file management
a file system is normally organized into directories for easy navigation and usage. these directories may contain files and other directions.
an operating system does the following activities for file management −
keeps track of information, location, uses, status etc. the collective facilities are often known as file system .
decides who gets the resources.
allocates the resources.
de-allocates the resources.
other important activities
following are some of the important activities that an operating system performs −
security − by means of password and similar other techniques, it prevents unauthorized access to programs and data.
control over system performance − recording delays between request for a service and response from the system.
job accounting − keeping track of time and resources used by various jobs and users.
error detecting aids − production of dumps, traces, error messages, and other debugging and error detecting aids.
coordination between other softwares and users − coordination and assignment of compilers, interpreters, assemblers and other software to the various users of the computer systems.
types of operating system
operating systems are there from the very first computer generation and they keep evolving with time. in this chapter, we will discuss some of the important types of operating systems which are most commonly used.
batch operating system
the users of a batch operating system do not interact with the computer directly. each user prepares his job on an off-line device like punch cards and submits it to the computer operator. to speed up processing, jobs with similar needs are batched together and run as a group. the programmers leave their programs with the operator and the operator then sorts the programs with similar requirements into batches.
the problems with batch systems are as follows −
lack of interaction between the user and the job.
cpu is often idle, because the speed of the mechanical i/o devices is slower than the cpu.
difficult to provide the desired priority.
time-sharing operating systems
time-sharing is a technique which enables many people, located at various terminals, to use a particular computer system at the same time. time-sharing or multitasking is a logical extension of multiprogramming. processor's time which is shared among multiple users simultaneously is termed as time-sharing.
the main difference between multiprogrammed batch systems and time-sharing systems is that in case of multiprogrammed batch systems, the objective is to maximize processor use, whereas in time-sharing systems, the objective is to minimize response time.
multiple jobs are executed by the cpu by switching between them, but the switches occur so frequently. thus, the user can receive an immediate response. for example, in a transaction processing, the processor executes each user program in a short burst or quantum of computation. that is, if n users are present, then each user can get a time quantum. when the user submits the command, the response time is in few seconds at most.
the operating system uses cpu scheduling and multiprogramming to provide each user with a small portion of a time. computer systems that were designed primarily as batch systems have been modified to time-sharing systems.
advantages of timesharing operating systems are as follows −
provides the advantage of quick response.
avoids duplication of software.
reduces cpu idle time.
disadvantages of time-sharing operating systems are as follows −
problem of reliability.
question of security and integrity of user programs and data.
problem of data communication.
distributed operating system
distributed systems use multiple central processors to serve multiple real-time applications and multiple users. data processing jobs are distributed among the processors accordingly.
the processors communicate with one another through various communication lines (such as high-speed buses or telephone lines). these are referred as loosely coupled systems or distributed systems. processors in a distributed system may vary in size and function. these processors are referred as sites, nodes, computers, and so on.
the advantages of distributed systems are as follows −
with resource sharing facility, a user at one site may be able to use the resources available at another.
speedup the exchange of data with one another via electronic mail.
if one site fails in a distributed system, the remaining sites can potentially continue operating.
better service to the customers.
reduction of the load on the host computer.
reduction of delays in data processing.
network operating system
a network operating system runs on a server and provides the server the capability to manage data, users, groups, security, applications, and other networking functions. the primary purpose of the network operating system is to allow shared file and printer access among multiple computers in a network, typically a local area network (lan), a private network or to other networks.
examples of network operating systems include microsoft windows server , microsoft windows server , unix, linux, mac os x, novell netware, and bsd.
the advantages of network operating systems are as follows −
centralized servers are highly stable.
security is server managed.
upgrades to new technologies and hardware can be easily integrated into the system.
remote access to servers is possible from different locations and types of systems.
the disadvantages of network operating systems are as follows −
high cost of buying and running a server.
dependency on a central location for most operations.
regular maintenance and updates are required.
real time operating system
a real-time system is defined as a data processing system in which the time interval required to process and respond to inputs is so small that it controls the environment. the time taken by the system to respond to an input and display of required updated information is termed as the response time. so in this method, the response time is very less as compared to online processing.
real-time systems are used when there are rigid time requirements on the operation of a processor or the flow of data and real-time systems can be used as a control device in a dedicated application. a real-time operating system must have well-defined, fixed time constraints, otherwise the system will fail. for example, scientific experiments, medical imaging systems, industrial control systems, weapon systems, robots, air traffic control systems, etc.
there are two types of real-time operating systems.
hard real-time systems
hard real-time systems guarantee that critical tasks complete on time. in hard real-time systems, secondary storage is limited or missing and the data is stored in rom. in these systems, virtual memory is almost never found.
soft real-time systems
soft real-time systems are less restrictive. a critical real-time task gets priority over other tasks and retains the priority until it completes. soft real-time systems have limited utility than hard real-time systems. for example, multimedia, virtual reality, advanced scientific projects like undersea exploration and planetary rovers, etc.
operating system - services
an operating system provides services to both the users and to the programs.
it provides programs an environment to execute.
it provides users the services to execute the programs in a convenient manner.
following are a few common services provided by an operating system −
program execution
i/o operations
file system manipulation
communication
error detection
resource allocation
protection
program execution
operating systems handle many kinds of activities from user programs to system programs like printer spooler, name servers, file server, etc. each of these activities is encapsulated as a process.
loads a program into memory.
executes the program.
handles program's execution.
provides a mechanism for process synchronization.
provides a mechanism for process communication.
provides a mechanism for deadlock handling.
i/o operation
an i/o subsystem comprises of i/o devices and their corresponding driver software. drivers hide the peculiarities of specific hardware devices from the users.
an operating system manages the communication between user and device drivers.
i/o operation means read or write operation with any file or any specific i/o device.
operating system provides the access to the required i/o device when required.
file system manipulation
a file represents a collection of related information. computers can store files on the disk (secondary storage), for long-term storage purpose. examples of storage media include magnetic tape, magnetic disk and optical disk drives like cd, dvd. each of these media has its own properties like speed, capacity, data transfer rate and data access methods.
a file system is normally organized into directories for easy navigation and usage. these directories may contain files and other directions. following are the major activities of an operating system with respect to file management −
program needs to read a file or write a file.
the operating system gives the permission to the program for operation on file.
permission varies from read-only, read-write, denied and so on.
operating system provides an interface to the user to create/delete files.
operating system provides an interface to the user to create/delete directories.
operating system provides an interface to create the backup of file system.
communication
in case of distributed systems which are a collection of processors that do not share memory, peripheral devices, or a clock, the operating system manages communications between all the processes. multiple processes communicate with one another through communication lines in the network.
the os handles routing and connection strategies, and the problems of contention and security. following are the major activities of an operating system with respect to communication −
two processes often require data to be transferred between them
both the processes can be on one computer or on different computers, but are connected through a computer network.
communication may be implemented by two methods, either by shared memory or by message passing.
error handling
errors can occur anytime and anywhere. an error may occur in cpu, in i/o devices or in the memory hardware. following are the major activities of an operating system with respect to error handling −
the os constantly checks for possible errors.
the os takes an appropriate action to ensure correct and consistent computing.
resource management
in case of multi-user or multi-tasking environment, resources such as main memory, cpu cycles and files storage are to be allocated to each user or job. following are the major activities of an operating system with respect to resource management −
the os manages all kinds of resources using schedulers.
cpu scheduling algorithms are used for better utilization of cpu.
protection
considering a computer system having multiple users and concurrent execution of multiple processes, the various processes must be protected from each other's activities.
protection refers to a mechanism or a way to control the access of programs, processes, or users to the resources defined by a computer system. following are the major activities of an operating system with respect to protection −
the os ensures that all access to system resources is controlled.
the os ensures that external i/o devices are protected from invalid access attempts.
the os provides authentication features for each user by means of passwords.
operating system - properties
batch processing
batch processing is a technique in which an operating system collects the programs and data together in a batch before processing starts. an operating system does the following activities related to batch processing −
the os defines a job which has predefined sequence of commands, programs and data as a single unit.
the os keeps a number a jobs in memory and executes them without any manual information.
jobs are processed in the order of submission, i.e., first come first served fashion.
when a job completes its execution, its memory is released and the output for the job gets copied into an output spool for later printing or processing.
advantages
batch processing takes much of the work of the operator to the computer.
increased performance as a new job get started as soon as the previous job is finished, without any manual intervention.
disadvantages
difficult to debug program.
a job could enter an infinite loop.
due to lack of protection scheme, one batch job can affect pending jobs.
multitasking
multitasking is when multiple jobs are executed by the cpu simultaneously by switching between them. switches occur so frequently that the users may interact with each program while it is running. an os does the following activities related to multitasking −
the user gives instructions to the operating system or to a program directly, and receives an immediate response.
the os handles multitasking in the way that it can handle multiple operations/executes multiple programs at a time.
multitasking operating systems are also known as time-sharing systems.
these operating systems were developed to provide interactive use of a computer system at a reasonable cost.
a time-shared operating system uses the concept of cpu scheduling and multiprogramming to provide each user with a small portion of a time-shared cpu.
each user has at least one separate program in memory.
a program that is loaded into memory and is executing is commonly referred to as a process .
when a process executes, it typically executes for only a very short time before it either finishes or needs to perform i/o.
since interactive i/o typically runs at slower speeds, it may take a long time to complete. during this time, a cpu can be utilized by another process.
the operating system allows the users to share the computer simultaneously. since each action or command in a time-shared system tends to be short, only a little cpu time is needed for each user.
as the system switches cpu rapidly from one user/program to the next, each user is given the impression that he/she has his/her own cpu, whereas actually one cpu is being shared among many users.
multiprogramming
sharing the processor, when two or more programs reside in memory at the same time, is referred as multiprogramming. multiprogramming assumes a single shared processor. multiprogramming increases cpu utilization by organizing jobs so that the cpu always has one to execute.
the following figure shows the memory layout for a multiprogramming system.
an os does the following activities related to multiprogramming.
the operating system keeps several jobs in memory at a time.
this set of jobs is a subset of the jobs kept in the job pool.
the operating system picks and begins to execute one of the jobs in the memory.
multiprogramming operating systems monitor the state of all active programs and system resources using memory management programs to ensures that the cpu is never idle, unless there are no jobs to process.
advantages
high and efficient cpu utilization.
user feels that many programs are allotted cpu almost simultaneously.
disadvantages
cpu scheduling is required.
to accommodate many jobs in memory, memory management is required.
interactivity
interactivity refers to the ability of users to interact with a computer system. an operating system does the following activities related to interactivity −
provides the user an interface to interact with the system.
manages input devices to take inputs from the user. for example, keyboard.
manages output devices to show outputs to the user. for example, monitor.
the response time of the os needs to be short, since the user submits and waits for the result.
real time system
real-time systems are usually dedicated, embedded systems. an operating system does the following activities related to real-time system activity.
in such systems, operating systems typically read from and react to sensor data.
the operating system must guarantee response to events within fixed periods of time to ensure correct performance.
distributed environment
a distributed environment refers to multiple independent cpus or processors in a computer system. an operating system does the following activities related to distributed environment −
the os distributes computation logics among several physical processors.
the processors do not share memory or a clock. instead, each processor has its own local memory.
the os manages the communications between the processors. they communicate with each other through various communication lines.
spooling
spooling is an acronym for simultaneous peripheral operations on line. spooling refers to putting data of various i/o jobs in a buffer. this buffer is a special area in memory or hard disk which is accessible to i/o devices.
an operating system does the following activities related to distributed environment −
handles i/o device data spooling as devices have different data access rates.
maintains the spooling buffer which provides a waiting station where data can rest while the slower device catches up.
maintains parallel computation because of spooling process as a computer can perform i/o in parallel fashion. it becomes possible to have the computer read data from a tape, write data to disk and to write out to a tape printer while it is doing its computing task.
advantages
the spooling operation uses a disk as a very large buffer.
spooling is capable of overlapping i/o operation for one job with processor operations for another job.
operating system - processes
process
a process is basically a program in execution. the execution of a process must progress in a sequential fashion.
a process is defined as an entity which represents the basic unit of work to be implemented in the system.
to put it in simple terms, we write our computer programs in a text file and when we execute this program, it becomes a process which performs all the tasks mentioned in the program.
when a program is loaded into the memory and it becomes a process, it can be divided into four sections ─ stack, heap, text and data. the following image shows a simplified layout of a process inside main memory −
s.n. component & description  stack the process stack contains the temporary data such as method/function parameters, return address and local variables.  heap this is dynamically allocated memory to a process during its run time.  text this includes the current activity represented by the value of program counter and the contents of the processor's registers.  data this section contains the global and static variables.
program
#include  int main() { printf("hello, world!
"); return ; }
a computer program is a collection of instructions that performs a specific task when executed by a computer. when we compare a program with a process, we can conclude that a process is a dynamic instance of a computer program.
a part of a computer program that performs a well-defined task is known as an algorithm. a collection of computer programs, libraries and related data are referred to as a software.
process life cycle
when a process executes, it passes through different states. these stages may differ in different operating systems, and the names of these states are also not standardized.
in general, a process can have one of the following five states at a time.
s.n. state & description  start this is the initial state when a process is first started/created.  ready the process is waiting to be assigned to a processor. ready processes are waiting to have the processor allocated to them by the operating system so that they can run. process may come into this state after start state or while running it by but interrupted by the scheduler to assign cpu to some other process.  running once the process has been assigned to a processor by the os scheduler, the process state is set to running and the processor executes its instructions.  waiting process moves into the waiting state if it needs to wait for a resource, such as waiting for user input, or waiting for a file to become available.  terminated or exit once the process finishes its execution, or it is terminated by the operating system, it is moved to the terminated state where it waits to be removed from main memory.
process control block (pcb)
a process control block is a data structure maintained by the operating system for every process. the pcb is identified by an integer process id (pid). a pcb keeps all the information needed to keep track of a process as listed below in the table −
s.n. information & description  process state the current state of the process i.e., whether it is ready, running, waiting, or whatever.  process privileges this is required to allow/disallow access to system resources.  process id unique identification for each of the process in the operating system.  pointer a pointer to parent process.  program counter program counter is a pointer to the address of the next instruction to be executed for this process.  cpu registers various cpu registers where process need to be stored for execution for running state.  cpu scheduling information process priority and other scheduling information which is required to schedule the process.  memory management information this includes the information of page table, memory limits, segment table depending on memory used by the operating system.  accounting information this includes the amount of cpu used for process execution, time limits, execution id etc.  io status information this includes a list of i/o devices allocated to the process.
the architecture of a pcb is completely dependent on operating system and may contain different information in different operating systems. here is a simplified diagram of a pcb −
the pcb is maintained for a process throughout its lifetime, and is deleted once the process terminates.
operating system - process scheduling
definition
the process scheduling is the activity of the process manager that handles the removal of the running process from the cpu and the selection of another process on the basis of a particular strategy.
process scheduling is an essential part of a multiprogramming operating systems. such operating systems allow more than one process to be loaded into the executable memory at a time and the loaded process shares the cpu using time multiplexing.
process scheduling queues
the os maintains all pcbs in process scheduling queues. the os maintains a separate queue for each of the process states and pcbs of all processes in the same execution state are placed in the same queue. when the state of a process is changed, its pcb is unlinked from its current queue and moved to its new state queue.
the operating system maintains the following important process scheduling queues −
job queue − this queue keeps all the processes in the system.
ready queue − this queue keeps a set of all processes residing in main memory, ready and waiting to execute. a new process is always put in this queue.
device queues − the processes which are blocked due to unavailability of an i/o device constitute this queue.
the os can use different policies to manage each queue (fifo, round robin, priority, etc.). the os scheduler determines how to move processes between the ready and run queues which can only have one entry per processor core on the system; in the above diagram, it has been merged with the cpu.
two-state process model
two-state process model refers to running and non-running states which are described below −
s.n. state & description  running when a new process is created, it enters into the system as in the running state.  not running processes that are not running are kept in queue, waiting for their turn to execute. each entry in the queue is a pointer to a particular process. queue is implemented by using linked list. use of dispatcher is as follows. when a process is interrupted, that process is transferred in the waiting queue. if the process has completed or aborted, the process is discarded. in either case, the dispatcher then selects a process from the queue to execute.
schedulers
schedulers are special system software which handle process scheduling in various ways. their main task is to select the jobs to be submitted into the system and to decide which process to run. schedulers are of three types −
long-term scheduler
short-term scheduler
medium-term scheduler
long term scheduler
it is also called a job scheduler. a long-term scheduler determines which programs are admitted to the system for processing. it selects processes from the queue and loads them into memory for execution. process loads into the memory for cpu scheduling.
the primary objective of the job scheduler is to provide a balanced mix of jobs, such as i/o bound and processor bound. it also controls the degree of multiprogramming. if the degree of multiprogramming is stable, then the average rate of process creation must be equal to the average departure rate of processes leaving the system.
on some systems, the long-term scheduler may not be available or minimal. time-sharing operating systems have no long term scheduler. when a process changes the state from new to ready, then there is use of long-term scheduler.
short term scheduler
it is also called as cpu scheduler. its main objective is to increase system performance in accordance with the chosen set of criteria. it is the change of ready state to running state of the process. cpu scheduler selects a process among the processes that are ready to execute and allocates cpu to one of them.
short-term schedulers, also known as dispatchers, make the decision of which process to execute next. short-term schedulers are faster than long-term schedulers.
medium term scheduler
medium-term scheduling is a part of swapping. it removes the processes from the memory. it reduces the degree of multiprogramming. the medium-term scheduler is in-charge of handling the swapped out-processes.
a running process may become suspended if it makes an i/o request. a suspended processes cannot make any progress towards completion. in this condition, to remove the process from memory and make space for other processes, the suspended process is moved to the secondary storage. this process is called swapping, and the process is said to be swapped out or rolled out. swapping may be necessary to improve the process mix.
comparison among scheduler
s.n. long-term scheduler short-term scheduler medium-term scheduler  it is a job scheduler it is a cpu scheduler it is a process swapping scheduler.  speed is lesser than short term scheduler speed is fastest among other two speed is in between both short and long term scheduler.  it controls the degree of multiprogramming it provides lesser control over degree of multiprogramming it reduces the degree of multiprogramming.  it is almost absent or minimal in time sharing system it is also minimal in time sharing system it is a part of time sharing systems.  it selects processes from pool and loads them into memory for execution it selects those processes which are ready to execute it can re-introduce the process into memory and execution can be continued.
context switch
a context switch is the mechanism to store and restore the state or context of a cpu in process control block so that a process execution can be resumed from the same point at a later time. using this technique, a context switcher enables multiple processes to share a single cpu. context switching is an essential part of a multitasking operating system features.
when the scheduler switches the cpu from executing one process to execute another, the state from the current running process is stored into the process control block. after this, the state for the process to run next is loaded from its own pcb and used to set the pc, registers, etc. at that point, the second process can start executing.
context switches are computationally intensive since register and memory state must be saved and restored. to avoid the amount of context switching time, some hardware systems employ two or more sets of processor registers. when the process is switched, the following information is stored for later use.
program counter
scheduling information
base and limit register value
currently used register
changed state
i/o state information
accounting information
operating system scheduling algorithms
a process scheduler schedules different processes to be assigned to the cpu based on particular scheduling algorithms. there are six popular process scheduling algorithms which we are going to discuss in this chapter −
first-come, first-served (fcfs) scheduling
shortest-job-next (sjn) scheduling
priority scheduling
shortest remaining time
round robin(rr) scheduling
multiple-level queues scheduling
these algorithms are either non-preemptive or preemptive. non-preemptive algorithms are designed so that once a process enters the running state, it cannot be preempted until it completes its allotted time, whereas the preemptive scheduling is based on priority where a scheduler may preempt a low priority running process anytime when a high priority process enters into a ready state.
first come first serve (fcfs)
jobs are executed on first come, first serve basis.
it is a non-preemptive, pre-emptive scheduling algorithm.
easy to understand and implement.
its implementation is based on fifo queue.
poor in performance as average wait time is high.
wait time of each process is as follows −
process wait time : service time - arrival time p  -  =  p  -  =  p  -  =  p  -  = 
average wait time: (+++) /  = .
shortest job next (sjn)
this is also known as shortest job first , or sjf
this is a non-preemptive, pre-emptive scheduling algorithm.
best approach to minimize waiting time.
easy to implement in batch systems where required cpu time is known in advance.
impossible to implement in interactive systems where required cpu time is not known.
the processer should know in advance how much time process will take.
given: table of processes, and their arrival time, execution time
process arrival time execution time service time p    p    p    p   
waiting time of each process is as follows −
process waiting time p  -  =  p  -  =  p  -  =  p  -  = 
average wait time: ( +  +  + )/ =  /  = .
priority based scheduling
priority scheduling is a non-preemptive algorithm and one of the most common scheduling algorithms in batch systems.
each process is assigned a priority. process with highest priority is to be executed first and so on.
processes with same priority are executed on first come first served basis.
priority can be decided based on memory requirements, time requirements or any other resource requirement.
given: table of processes, and their arrival time, execution time, and priority. here we are considering  is the lowest priority.
process arrival time execution time priority service time p     p     p     p    
waiting time of each process is as follows −
process waiting time p  -  =  p  -  =  p  -  =  p  -  = 
average wait time: ( +  +  + )/ =  /  = 
shortest remaining time
shortest remaining time (srt) is the preemptive version of the sjn algorithm.
the processor is allocated to the job closest to completion but it can be preempted by a newer ready job with shorter time to completion.
impossible to implement in interactive systems where required cpu time is not known.
it is often used in batch environments where short jobs need to give preference.
round robin scheduling
round robin is the preemptive process scheduling algorithm.
each process is provided a fix time to execute, it is called a quantum .
once a process is executed for a given time period, it is preempted and other process executes for a given time period.
context switching is used to save states of preempted processes.
wait time of each process is as follows −
process wait time : service time - arrival time p ( - ) + ( - ) =  p ( - ) =  p ( - ) + ( - ) + ( - ) =  p ( - ) + ( - ) = 
average wait time: (+++) /  = .
multiple-level queues scheduling
multiple-level queues are not an independent scheduling algorithm. they make use of other existing algorithms to group and schedule jobs with common characteristics.
multiple queues are maintained for processes with common characteristics.
each queue can have its own scheduling algorithms.
priorities are assigned to each queue.
for example, cpu-bound jobs can be scheduled in one queue and all i/o-bound jobs in another queue. the process scheduler then alternately selects jobs from each queue and assigns them to the cpu based on the algorithm assigned to the queue.
operating system - multi-threading
what is thread?
a thread is also called a lightweight process. threads provide a way to improve application performance through parallelism. threads represent a software approach to improving performance of operating system by reducing the overhead thread is equivalent to a classical process.
each thread belongs to exactly one process and no thread can exist outside a process. each thread represents a separate flow of control. threads have been successfully used in implementing network servers and web server. they also provide a suitable foundation for parallel execution of applications on shared memory multiprocessors. the following figure shows the working of a single-threaded and a multithreaded process.
difference between process and thread
advantages of thread
threads minimize the context switching time.
use of threads provides concurrency within a process.
efficient communication.
it is more economical to create and context switch threads.
threads allow utilization of multiprocessor architectures to a greater scale and efficiency.
types of thread
threads are implemented in following two ways −
user level threads − user managed threads.
kernel level threads − operating system managed threads acting on kernel, an operating system core.
user level threads
advantages
thread switching does not require kernel mode privileges.
user level thread can run on any operating system.
scheduling can be application specific in the user level thread.
user level threads are fast to create and manage.
disadvantages
in a typical operating system, most system calls are blocking.
multithreaded application cannot take advantage of multiprocessing.
kernel level threads
the kernel maintains context information for the process as a whole and for individuals threads within the process. scheduling by the kernel is done on a thread basis. the kernel performs thread creation, scheduling and management in kernel space. kernel threads are generally slower to create and manage than the user threads.
advantages
kernel can simultaneously schedule multiple threads from the same process on multiple processes.
if one thread in a process is blocked, the kernel can schedule another thread of the same process.
kernel routines themselves can be multithreaded.
disadvantages
kernel threads are generally slower to create and manage than the user threads.
transfer of control from one thread to another within the same process requires a mode switch to the kernel.
multithreading models
some operating system provide a combined user level thread and kernel level thread facility. solaris is a good example of this combined approach. in a combined system, multiple threads within the same application can run in parallel on multiple processors and a blocking system call need not block the entire process. multithreading models are three types
many to many relationship.
many to one relationship.
one to one relationship.
many to many model
the many-to-many model multiplexes any number of user threads onto an equal or smaller number of kernel threads.
the following diagram shows the many-to-many threading model where  user level threads are multiplexing with  kernel level threads. in this model, developers can create as many user threads as necessary and the corresponding kernel threads can run in parallel on a multiprocessor machine. this model provides the best accuracy on concurrency and when a thread performs a blocking system call, the kernel can schedule another thread for execution.
many to one model
many-to-one model maps many user level threads to one kernel-level thread. thread management is done in user space by the thread library. when thread makes a blocking system call, the entire process will be blocked. only one thread can access the kernel at a time, so multiple threads are unable to run in parallel on multiprocessors.
if the user-level thread libraries are implemented in the operating system in such a way that the system does not support them, then the kernel threads use the many-to-one relationship modes.
one to one model
there is one-to-one relationship of user-level thread to the kernel-level thread. this model provides more concurrency than the many-to-one model. it also allows another thread to run when a thread makes a blocking system call. it supports multiple threads to execute in parallel on microprocessors.
disadvantage of this model is that creating user thread requires the corresponding kernel thread. os/, windows nt and windows  use one to one relationship model.
difference between user-level & kernel-level thread
s.n. user-level threads kernel-level thread  user-level threads are faster to create and manage. kernel-level threads are slower to create and manage.  implementation is by a thread library at the user level. operating system supports creation of kernel threads.  user-level thread is generic and can run on any operating system. kernel-level thread is specific to the operating system.  multi-threaded applications cannot take advantage of multiprocessing. kernel routines themselves can be multithreaded.
operating system - memory management
memory management is the functionality of an operating system which handles or manages primary memory and moves processes back and forth between main memory and disk during execution. memory management keeps track of each and every memory location, regardless of either it is allocated to some process or it is free. it checks how much memory is to be allocated to processes. it decides which process will get memory at what time. it tracks whenever some memory gets freed or unallocated and correspondingly it updates the status.
this tutorial will teach you basic concepts related to memory management.
process address space
the process address space is the set of logical addresses that a process 
operating system - useful resources
advertisements
the following resources contain additional information on operating system please use them to get more in-depth knowledge on this topic.
useful links on operating system
operating system − a wikipage giving a short description about operating system.
what is an operating system − an operating system is the most important software that runs on a computer.
computer basics by bbc − an introduction to computers including computer parts and health and safety.
basic computer literacy information − a quick go through basic computer literacy information.
useful books on operating system
to enlist your site on this page, please drop an email to contact@tutorialspoint.com
discuss operating system
advertisements
an operating system (os) is a collection of software that manages computer hardware resources and provides common services for computer programs. the operating system is a vital component of the system software in a computer system. this tutorial will take you through step by step approach while learning operating system concepts.
operating system tutorial
operating system tutorial provides the basic and advanced concepts of operating system . our operating system tutorial is designed for beginners, professionals and gate aspirants. we have designed this tutorial after the completion of a deep research about every concept.
the content is described in detailed manner and has the ability to answer most of your queries. the tutorial also contains the numerical examples based on previous year gate questions which will help you to address the problems in a practical manner.
operating system can be defined as an interface between user and the hardware. it provides an environment to the user so that, the user can perform its task in convenient and efficient way.
the operating system tutorial is divided into various parts based on its functions such as process management, process synchronization, deadlocks and file management.
operating system definition and function
we need a system which can act as an intermediary and manage all the processes and resources present in the system.
an operating system can be defined as an interface between user and hardware. it is responsible for the execution of all the processes, resource allocation, cpu management, file management and many other tasks.
the purpose of an operating system is to provide an environment in which a user can execute programs in convenient and efficient manner.
structure of a computer system
a computer system consists of:
users (people who are using the computer)
application programs (compilers, databases, games, video player, browsers, etc.)
system programs (shells, editors, compilers, etc.)
operating system ( a special program which acts as an interface between user and hardware )
hardware ( cpu, disks, memory, etc)
what does an operating system do?
process management process synchronization memory management cpu scheduling file management security
operating system index
prerequisites
before learning the operating system tutorial, you must have the basic knowledge about the way in which a computer system operates.
audience
our operating system tutorial is designed to help beginners, professionals and gate aspirants.
problem
we can assure you that you will not find any problem in this operating system tutorial. however, if you find any, you can post the problem in the contact form.
next → ← prev types of os there are many types of operating system exists in the current scenario: batch operating system in the era of s, the batch processing was very popular. the jobs were executed in batches. people were used to have a single computer which was called mainframe. in batch operating system, access is given to more than one person; they submit their respective jobs to the system for the execution. the system put all of the jobs in a queue on the basis of first come first serve and then executes the jobs one by one. the users collect their respective output when all the jobs get executed. disadvantages of batch os . starvation batch processing suffers from starvation. if there are five jobs j, j, j, j, j and j present in the batch. if the execution time of j is very high then other four jobs will never be going to get executed or they will have to wait for a very high time. hence the other processes get starved. . not interactive batch processing is not suitable for the jobs which are dependent on the user's input. if a job requires the input of two numbers from the console then it will never be going to get it in the batch processing scenario since the user is not present at the time of execution. multiprogramming operating system multiprogramming is an extension to the batch processing where the cpu is kept always busy. each process needs two types of system time: cpu time and io time. in multiprogramming environment, for the time a process does its i/o, the cpu can start the execution of other processes. therefore, multiprogramming improves the efficiency of the system. multiprocessing operating system in multiprocessing, parallel computing is achieved. there are more than one processors present in the system which can execute more than one process at the same time. this will increase the throughput of the system. real time operating system in real time systems, each job carries a certain deadline within which the job is supposed to be completed, otherwise the huge loss will be there or even if the result is produced then it will be completely useless. the application of a real time system exists in the case of military applications, if you want to drop a missile then the missile is supposed to be dropped with certain precision.
next topic process management introduction
← prev next →
process management introduction
a program does nothing unless its instructions are executed by a cpu. a program in execution is called a process. in order to accomplish its task, process needs the computer resources.
there may exist more than one process in the system which may require the same resource at the same time. therefore, the operating system has to manage all the processes and the resources in a convenient and efficient way.
some resources may need to be executed by one process at one time to maintain the consistency otherwise the system can become inconsistent and deadlock may occur.
the operating system is responsible for the following activities in connection with process management
scheduling processes and threads on the cpus. creating and deleting both user and system processes. suspending and resuming processes. providing mechanisms for process synchronization. providing mechanisms for process communication.
next → ← prev attributes of a process the attributes of the process are used by the operating system to create the process control block (pcb) for each of them. this is also called context of the process. attributes which are stored in the pcb are described below. . process id when a process is created, a unique id is assigned to the process which is used for unique identification of the process in the system. . program counter a program counter stores the address of the last instruction of the process on which the process was suspended. the cpu uses this address when the execution of this process is resumed. . process state the process, from its creation to the completion, goes through various states which are new, ready, running and waiting. we will discuss about them later in detail. . priority every process has its own priority. the process with the highest priority among the processes gets the cpu first. this is also stored on the process control block. . general purpose registers every process has its own set of registers which are used to hold the data which is generated during the execution of the process. . list of open files during the execution, every process uses some files which need to be present in the main memory. os also maintains a list of open files in the pcb. . list of open devices os also maintain the list of all open devices which are used during the execution of the process.
next topic process states
← prev next →
next → ← prev process states state diagram
the process, from its creation to completion, passes through various states. the minimum number of states is five. the names of the states are not standardized although the process may be in one of the following states during execution. . new a program which is going to be picked up by the os into the main memory is called a new process. . ready whenever a process is created, it directly enters in the ready state, in which, it waits for the cpu to be assigned. the os picks the new processes from the secondary memory and put all of them in the main memory. the processes which are ready for the execution and reside in the main memory are called ready state processes. there can be many processes present in the ready state. . running one of the processes from the ready state will be chosen by the os depending upon the scheduling algorithm. hence, if we have only one cpu in our system, the number of running processes for a particular time will always be one. if we have n processors in the system then we can have n processes running simultaneously. . block or wait from the running state, a process can make the transition to the block or wait state depending upon the scheduling algorithm or the intrinsic behavior of the process. when a process waits for a certain resource to be assigned or for the input from the user then the os move this process to the block or wait state and assigns the cpu to the other processes. . completion or termination when a process finishes its execution, it comes in the termination state. all the context of the process (process control block) will also be deleted the process will be terminated by the operating system. . suspend ready a process in the ready state, which is moved to secondary memory from the main memory due to lack of the resources (mainly primary memory) is called in the suspend ready state. if the main memory is full and a higher priority process comes for the execution then the os have to make the room for the process in the main memory by throwing the lower priority process out into the secondary memory. the suspend ready processes remain in the secondary memory until the main memory gets available. . suspend wait instead of removing the process from the ready queue, it's better to remove the blocked process which is waiting for some resources in the main memory. since it is already waiting for some resource to get available hence it is better if it waits in the secondary memory and make room for the higher priority process. these processes complete their execution once the main memory gets available and their wait is finished.
operations on the process . creation once the process is created, it will be ready and come into the ready queue (main memory) and will be ready for the execution. . scheduling out of the many processes present in the ready queue, the operating system chooses one process and start executing it. selecting the process which is to be executed next, is known as scheduling. . execution once the process is scheduled for the execution, the processor starts executing it. process may come to the blocked or wait state during the execution then in that case the processor starts executing the other processes. . deletion/killing once the purpose of the process gets over then the os will kill the process. the context of the process (pcb) will be deleted and the process gets terminated by the operating system.
next topic process schedulers
← prev next →
next → ← prev process schedulers operating system uses various schedulers for the process scheduling described below. . long term scheduler long term scheduler is also known as job scheduler. it chooses the processes from the pool (secondary memory) and keeps them in the ready queue maintained in the primary memory. long term scheduler mainly controls the degree of multiprogramming. the purpose of long term scheduler is to choose a perfect mix of io bound and cpu bound processes among the jobs present in the pool. if the job scheduler chooses more io bound processes then all of the jobs may reside in the blocked state all the time and the cpu will remain idle most of the time. this will reduce the degree of multiprogramming. therefore, the job of long term scheduler is very critical and may affect the system for a very long time. . short term scheduler short term scheduler is also known as cpu scheduler. it selects one of the jobs from the ready queue and dispatch to the cpu for the execution. a scheduling algorithm is used to select which job is going to be dispatched for the execution. the job of the short term scheduler can be very critical in the sense that if it selects job whose cpu burst time is very high then all the jobs after that, will have to wait in the ready queue for a very long time. this problem is called starvation which may arise if the short term scheduler makes some mistakes while selecting the job. . medium term scheduler medium term scheduler takes care of the swapped out processes.if the running state processes needs some io time for the completion then there is a need to change its state from running to waiting. medium term scheduler is used for this purpose. it removes the process from the running state to make room for the other processes. such processes are the swapped out processes and this procedure is called swapping. the medium term scheduler is responsible for suspending and resuming the processes. it reduces the degree of multiprogramming. the swapping is necessary to have a perfect mix of processes in the ready queue.
next topic process queues
← prev next →
next → ← prev process queues the operating system manages various types of queues for each of the process states. the pcb related to the process is also stored in the queue of the same state. if the process is moved from one state to another state then its pcb is also unlinked from the corresponding queue and added to the other state queue in which the transition is made.
there are the following queues maintained by the operating system. . job queue in starting, all the processes get stored in the job queue. it is maintained in the secondary memory. the long term scheduler (job scheduler) picks some of the jobs and put them in the primary memory. . ready queue ready queue is maintained in primary memory. the short term scheduler picks the job from the ready queue and dispatch to the cpu for the execution. . waiting queue when the process needs some io operation in order to complete its execution, os changes the state of the process from running to waiting. the context (pcb) associated with the process gets stored on the waiting queue which will be used by the processor when the process finishes the io.
next topic various time related to process
← prev next →
various times related to the process
. arrival time
the time at which the process enters into the ready queue is called the arrival time.
. burst time
the total amount of time required by the cpu to execute the whole process is called the burst time. this does not include the waiting time. it is confusing to calculate the execution time for a process even before executing it hence the scheduling problems based on the burst time cannot be implemented in reality.
. completion time
the time at which the process enters into the completion state or the time at which the process completes its execution, is called completion time.
. turnaround time
the total amount of time spent by the process from its arrival to its completion, is called turnaround time.
. waiting time
the total amount of time for which the process waits for the cpu to be assigned is called waiting time.
. response time
the difference between the arrival time and the time at which the process first gets the cpu is called response time.
next → ← prev cpu scheduling in the uniprogrammming systems like ms dos, when a process waits for any i/o operation to be done, the cpu remains idol. this is an overhead since it wastes the time and causes the problem of starvation. however, in multiprogramming systems, the cpu doesn't remain idle during the waiting time of the process and it starts executing other processes. operating system has to define which process the cpu will be given. in multiprogramming systems, the operating system schedules the processes on the cpu to have the maximum utilization of it and this procedure is called cpu scheduling. the operating system uses various scheduling algorithm to schedule the processes. this is a task of the short term scheduler to schedule the cpu for the number of processes present in the job pool. whenever the running process requests some io operation then the short term scheduler saves the current context of the process (also called pcb) and changes its state from running to waiting. during the time, process is in waiting state; the short term scheduler picks another process from the ready queue and assigns the cpu to this process. this procedure is called context switching. what is saved in the process control block? the operating system maintains a process control block during the lifetime of the process. the process control block is deleted when the process is terminated or killed. there is the following information which is saved in the process control block and is changing with the state of the process.
why do we need scheduling? in multiprogramming, if the long term scheduler picks more i/o bound processes then most of the time, the cpu remains idol. the task of operating system is to optimize the utilization of resources. if most of the running processes change their state from running to waiting then there may always be a possibility of deadlock in the system. hence to reduce this overhead, the os needs to schedule the jobs to get the optimal utilization of cpu and to avoid the possibility to deadlock.
next topic scheduling algorithms
← prev next →
next → ← prev scheduling algorithms there are various algorithms which are used by the operating system to schedule the processes on the processor in an efficient way. the purpose of a scheduling algorithm maximum cpu utilization fare allocation of cpu maximum throughput minimum turnaround time minimum waiting time minimum response time there are the following algorithms which can be used to schedule the jobs. . first come first serve it is the simplest algorithm to implement. the process with the minimal arrival time will get the cpu first. the lesser the arrival time, the sooner will the process gets the cpu. it is the non-preemptive type of scheduling. . round robin in the round robin scheduling algorithm, the os defines a time quantum (slice). all the processes will get executed in the cyclic way. each of the process will get the cpu for a small amount of time (called time quantum) and then get back to the ready queue to wait for its next turn. it is a preemptive type of scheduling. . shortest job first the job with the shortest burst time will get the cpu first. the lesser the burst time, the sooner will the process get the cpu. it is the non-preemptive type of scheduling. . shortest remaining time first it is the preemptive form of sjf. in this algorithm, the os schedules the job according to the remaining time of the execution. . priority based scheduling in this algorithm, the priority will be assigned to each of the processes. the higher the priority, the sooner will the process get the cpu. if the priority of the two processes is same then they will be scheduled according to their arrival time. . highest response ratio next in this scheduling algorithm, the process with highest response ratio will be scheduled next. this reduces the starvation in the system.
next topic fcfs scheduling
← prev next →
next → ← prev fcfs scheduling first come first serve (fcfs) scheduling algorithm simply schedules the jobs according to their arrival time. the job which comes first in the ready queue will get the cpu first. the lesser the arrival time of the job, the sooner will the job get the cpu. fcfs scheduling may cause the problem of starvation if the burst time of the first process is the longest among all the jobs. advantages of fcfs simple
easy
first come, first serv disadvantages of fcfs the scheduling method is non preemptive, the process will run to the completion. due to the non-preemptive nature of the algorithm, the problem of starvation may occur. although it is easy to implement, but it is poor in performance since the average waiting time is higher as compare to other scheduling algorithms. example let's take an example of the fcfs scheduling algorithm. in the following schedule, there are  processes with process id p, p, p, p and p. p arrives at time , p at time , p at time , p arrives at time  and process p arrives at time  in the ready queue. the processes and their respective arrival and burst time are given in the following table. the turnaround time and the waiting time are calculated by using the following formula. turn around time = completion time - arrival time waiting time = turnaround time - burst time the average waiting time is determined by summing the respective waiting time of all the processes and divided the sum by the total number of processes. process id arrival time burst time completion time turn around time waiting time                               avg waiting time=/ (gantt chart)
next topic convoy effect in fcfs
← prev next →
next → ← prev convoy effect in fcfs fcfs may suffer from the convoy effect if the burst time of the first job is the highest among all. as in the real life, if a convoy is passing through the road then the other persons may get blocked until it passes completely. this can be simulated in the operating system also. if the cpu gets the processes of the higher burst time at the front end of the ready queue then the processes of lower burst time may get blocked which means they may never get the cpu if the job in the execution has a very high burst time. this is called convoy effect or starvation.
example in the example, we have  processes named as p, p and p. the burt time of process p is highest. the turnaround time and the waiting time in the following table, are calculated by the formula, turn around time = completion time - arrival time waiting time = turn around time - burst time in the first scenario, the process p arrives at the first in the queue although; the burst time of the process is the highest among all. since, the scheduling algorithm, we are following is fcfs hence the cpu will execute the process p first. in this schedule, the average waiting time of the system will be very high. that is because of the convoy effect. the other processes p, p have to wait for their turn for  units of time although their burst time is very low. this schedule suffers from starvation. process id arrival time burst time completion time turn around time waiting time                   avg waiting time = / in the second scenario, if process p would have arrived at the last of the queue and the other processes p and p at earlier then the problem of starvation would not be there. following example shows the deviation in the waiting times of both the scenarios. although the length of the schedule is same that is  units but the waiting time will be lesser in this schedule. process id arrival time burst time completion time turn around time waiting time                   avg waiting time=/
next topic fcfs with overhead
← prev next →
fcfs with overhead
in the above examples, we are assuming that all the processes are the cpu bound processes only. we were also neglecting the context switching time.
however if the time taken by the scheduler in context switching is considered then the average waiting time of the system will be increased which also affects the efficiency of the system.
context switching is always an overhead. the following example describeshow the efficiency will be affected if the context switching time is considered in the system.
example
in the following example, we are considering five processes p, p, p, p, p and p. their arrival time and burst time are given below.
process id arrival time burst time                  
if the context switching time of the system is  unit then the gantt chart of the system will be prepared as follows.
given δ= unit;
the system will take extra  unit of time (overhead) after the execution of every process to schedule the next process.
inefficiency= (/) x  % efficiencyͷ = (-/) x  %
next → ← prev shortest job first (sjf) scheduling till now, we were scheduling the processes according to their arrival time (in fcfs scheduling). however, sjf scheduling algorithm, schedules the processes according to their burst time. in sjf scheduling, the process with the lowest burst time, among the list of available processes in the ready queue, is going to be scheduled next. however, it is very difficult to predict the burst time needed for a process hence this algorithm is very difficult to implement in the system. advantages of sjf maximum throughput minimum average waiting and turnaround time disadvantages of sjf may suffer with the problem of starvation it is not implementable because the exact burst time for a process can't be known in advance. there are different techniques available by which, the cpu burst time of the process can be determined. we will discuss them later in detail. example in the following example, there are five jobs named as p, p, p, p and p. their arrival time and burst time are given in the table below. pid arrival time burst time completion time turn around time waiting time                               since, no process arrives at time  hence; there will be an empty slot in the gantt chart from time  to  (the time at which the first process arrives). according to the algorithm, the os schedules the process which is having the lowest burst time among the available processes in the ready queue. till now, we have only one process in the ready queue hence the scheduler will schedule this to the processor no matter what is its burst time. this will be executed till  units of time. till then we have three more processes arrived in the ready queue hence the scheduler will choose the process with the lowest burst time. among the processes given in the table, p will be executed next since it is having the lowest burst time among all the available processes. so that's how the procedure will go on in shortest job first (sjf) scheduling algorithm. avg waiting time = /
next topic prediction of cpu burst time for a process in sjf
← prev next →
next → ← prev prediction of cpu burst time for a process in sjf the sjf algorithm is one of the best scheduling algorithms since it provides the maximum throughput and minimal waiting time but the problem with the algorithm is, the cpu burst time can't be known in advance. we can approximate the cpu burst time for a process. there are various techniques which can be used to assume the cpu burst time for a process. our assumption needs to be accurate in order to utilize the algorithm optimally. there are the following techniques used for the assumption of cpu burst time for a process. . static techniques
process size we can predict the burst time of the process from its size. if we have two processes t_old and t_new and the actual burst time of the old process is known as  secs and the size of the process is  kb. we know that the size of p_new is  kb. then the probability of p_new having the similar burst time as  secs is maximum. if, p_old →  kb p_new →  kb bt(p_old) →  secs then, bt(p_new) →  secs hence, in this technique, we actually predict the burst time of a new process according to the burst time of an old process of similar size as of new process. process type we can also predict the burst time of the process according to its type. a process can be of various types defined as follows. os process
a process can be an operating system process like schedulers, compilers, program managers and many more system processes. their burst time is generally lower for example,  to  units of time. user process
the processes initiated by the users are called user processes. there can be three types of processes as follows. interactive process
the interactive processes are the one which interact with the user time to time or execution of which totally depends upon the user inputs for example various games are such processes. there burst time needs to be lower since they don't need cpu for a large amount of time, they mainly depend upon the user's interactivity with the process hence they are mainly io bound processes. foreground process
foreground processes are the processes which are used by the user to perform their needs such as ms office, editors, utility software etc. these types of processes have a bit higher burst time since they are a perfect mix of cpu and io bound processes. background process background processes supports the execution of other processes. they work in hidden mode. for example, key logger is the process which records the keys pressed by the user and activities of the user on the system. they are mainly cpu bound processes and needs cpu for a higher amount of time. . dynamic techniques
simple averaging in simple averaging, there are given list of n processes p(i).......p(n). let t(i) denotes the burst time of the process p(i). let τ(n) denotes the predicted burst time of pth process. then according to the simple averaging, the predicted burst time of process n+ will be calculated as, τ(n+) = (/n) ∑ t(i) where, <=i<=n and ∑ t(i) is the summation of actual burst time of all the processes available till now. exponential averaging or aging let, tn be the actual burst time of nth process.τ(n) be the predicted burst time for nth process then the cpu burst time for the next process (n+) will be calculated as, τ(n+) = α. tn + (-α) . τ(n) where, α is the smoothing. its value lies between  and .
next topic srtf scheduling algorithm
← prev next →
next → ← prev shortest remaining time first (srtf) scheduling algorithm this algorithm is the preemptive version of sjf scheduling. in srtf, the execution of the process can be stopped after certain amount of time. at the arrival of every process, the short term scheduler schedules the process with the least remaining burst time among the list of available processes and the running process. once all the processes are available in the ready queue, no preemption will be done and the algorithm will work as sjf scheduling. the context of the process is saved in the process control block when the process is removed from the execution and the next process is scheduled. this pcb is accessed on the next execution of this process. example in this example, there are five jobs p, p, p, p, p and p. their arrival time and burst time are given below in the table. process id arrival time burst time completion time turn around time waiting time response time                                           avg waiting time = / the gantt chart is prepared according to the arrival and burst time given in the table. since, at time , the only available process is p with cpu burst time . this is the only available process in the list therefore it is scheduled. the next process arrives at time unit . since the algorithm we are using is srtf which is a preemptive one, the current execution is stopped and the scheduler checks for the process with the least burst time.
till now, there are two processes available in the ready queue. the os has executed p for one unit of time till now; the remaining burst time of p is  units. the burst time of process p is  units. hence process p is scheduled on the cpu according to the algorithm. the next process p arrives at time unit . at this time, the execution of process p is stopped and the process with the least remaining burst time is searched. since the process p has  unit of burst time hence it will be given priority over others. the next process p arrives at time unit . at this arrival, the scheduler will stop the execution of p and check which process is having least burst time among the available processes (p, p, p and p). p and p are having the remaining burst time  units and  units respectively. p and p are having the remaining burst time  unit each. since, both are equal hence the scheduling will be done according to their arrival time. p arrives earlier than p and therefore it will be scheduled again. the next process p arrives at time unit . till this time, the process p has completed its execution and it is no more in the list. the scheduler will compare the remaining burst time of all the available processes. since the burst time of process p is  which is least among all hence this will be scheduled. the next process p arrives at time unit , till this time, the process p has completed its execution. we have  available processes till now, that are p (), p (), p () and p (). the burst time of p is the least among all hence p is scheduled. since, now, all the processes are available hence the algorithm will now work same as sjf. p will be executed till its completion and then the process with the least remaining time will be scheduled. once all the processes arrive, no preemption is done and the algorithm will work as sjf.
next topic srtf gate  example
← prev next →
next → ← prev srtf gate  example if we talk about scheduling algorithm from the gate point of view, they generally ask simple numerical questions about finding the average waiting time and turnaround time. let's discuss the question asked in gate  on srtf. q. given the arrival time and burst time of  jobs in the table below. calculate the average waiting time of the system. process id arrival time burst time completion time turn around time waiting time                   there are three jobs p, p and p. p arrives at time unit ; it will be scheduled first for the time until the next process arrives. p arrives at  unit of time. its burst time is  units which is least among the jobs in the queue. hence it will be scheduled next. at time , p will arrive with burst time . since remaining burst time of p is  units which are least among the available jobs. hence the processor will continue its execution till its completion. because all the jobs have been arrived so no preemption will be done now and all the jobs will be executed till the completion according to sjf. avg waiting time = (++)/ =  units
next topic round robin scheduling
← prev next →
next → ← prev round robin scheduling algorithm round robin scheduling algorithm is one of the most popular scheduling algorithm which can actually be implemented in most of the operating systems. this is the preemptive version of first come first serve scheduling. the algorithm focuses on time sharing. in this algorithm, every process gets executed in a cyclic way. a certain time slice is defined in the system which is called time quantum. each process present in the ready queue is assigned the cpu for that time quantum, if the execution of the process is completed during that time then the process will terminate else the process will go back to the ready queue and waits for the next turn to complete the execution.
advantages it can be actually implementable in the system because it is not depending on the burst time. it doesn't suffer from the problem of starvation or convoy effect. all the jobs get a fare allocation of cpu. disadvantages the higher the time quantum, the higher the response time in the system. the lower the time quantum, the higher the context switching overhead in the system. deciding a perfect time quantum is really a very difficult task in the system.
next topic rr scheduling example
← prev next →
next → ← prev rr scheduling example in the following example, there are six processes named as p, p, p, p, p and p. their arrival time and burst time are given below in the table. the time quantum of the system is  units. process id arrival time burst time                   according to the algorithm, we have to maintain the ready queue and the gantt chart. the structure of both the data structures will be changed after every scheduling. ready queue: initially, at time , process p arrives which will be scheduled for the time slice  units. hence in the ready queue, there will be only one process p at starting with cpu burst time  units. p  gantt chart the p will be executed for  units first. ready queue meanwhile the execution of p, four more processes p, p, p and p arrives in the ready queue. p has not completed yet, it needs another  unit of time hence it will also be added back to the ready queue. p p p p p      gantt chart after p, p will be executed for  units of time which is shown in the gantt chart. ready queue during the execution of p, one more process p is arrived in the ready queue. since p has not completed yet hence, p will also be added back to the ready queue with the remaining burst time  units. p p p p p p       gantt chart after p and p, p will get executed for  units of time since its cpu burst time is only  seconds. ready queue since p has been completed, hence it will be terminated and not be added to the ready queue. the next process will be executed is p. p p p p p      gantt chart after, p, p and p, p will get executed. its burst time is only  unit which is lesser then the time quantum hence it will be completed. ready queue the next process in the ready queue is p with  units of burst time. since p is completed hence it will not be added back to the queue. p p p p     gantt chart p will be executed for the whole time slice because it requires  units of burst time which is higher than the time slice. ready queue p has not been completed yet; it will be added back to the queue with the remaining burst time of  unit. p p p p     gantt chart the process p will be given the next turn to complete its execution. since it only requires  unit of burst time hence it will be completed. ready queue p is completed and will not be added back to the ready queue. the next process p requires only  units of burst time and it will be executed next. p p p    gantt chart p will be executed for  units of time till completion. ready queue since p is completed, hence it will not be added again to the queue. there are only two processes present in the ready queue. the next process p requires only  units of time. p p   gantt chart p will get executed again, since it only requires only  units of time hence this will be completed. ready queue now, the only available process in the queue is p which requires  unit of burst time. since the time slice is of  units hence it will be completed in the next burst. p  gantt chart p will get executed till completion. the completion time, turnaround time and waiting time will be calculated as shown in the table below. as, we know, turn around time = completion time - arrival time waiting time = turn around time - burst time
process id arrival time burst time completion time turn around time waiting time                                     avg waiting time = (+++++)/ = / units
next topic hrrn scheduling
← prev next →
next → ← prev highest response ratio next (hrrn) scheduling highest response ratio next (hrnn) is one of the most optimal scheduling algorithms. this is a non-preemptive algorithm in which, the scheduling is done on the basis of an extra parameter called response ratio. a response ratio is calculated for each of the available jobs and the job with the highest response ratio is given priority over the others. response ratio is calculated by the given formula. response ratio = (w+s)/s where, w → waiting time s → service time or burst time if we look at the formula, we will notice that the job with the shorter burst time will be given priority but it is also including an extra factor called waiting time. since, hrnn α w hrnn α (/s) hence, this algorithm not only favors shorter job but it also concern the waiting time of the longer jobs. its mode is non preemptive hence context switching is minimal in this algorithm.
next topic hrnn example
← prev next →
hrnn example
in the following example, there are  processes given. their arrival time and burst time are given in the table.
process id arrival time burst time               
at time , the process p arrives with the cpu burst time of  units. since it is the only process arrived till now hence this will get scheduled immediately.
p is executed for  units, meanwhile, only one process p arrives at time . this will get scheduled immediately since the os doesn't have a choice.
p is executed for  units. meanwhile, all the processes get available. we have to calculate the response ratio for all the remaining jobs.
rr (p) = ((-) +)/ =  rr (p) = (+)/ =  rr (p) = (+)/ = 
since, the response ratio of p is higher hence p will be scheduled first.
p is scheduled for  unit. the next available processes are p and p. let's calculate their response ratio.
rr ( p) = (+)/ = . rr (p) = (+)/ = .
the response ratio of p is higher hence p will be scheduled.
now, the only available process is p with the burst time of  units, since there is no other process available hence this will be scheduled.
process id arrival time burst time completion time turn around time waiting time                              
average waiting time = /
next → ← prev priority scheduling in priority scheduling, there is a priority number assigned to each process. in some systems, the lower the number, the higher the priority. while, in the others, the higher the number, the higher will be the priority. the process with the higher priority among the available processes is given the cpu. there are two types of priority scheduling algorithm exists. one is preemptive priority scheduling while the other is non preemptive priority scheduling.
the priority number assigned to each of the process may or may not vary. if the priority number doesn't change itself throughout the process, it is called static priority, while if it keeps changing itself at the regular intervals, it is called dynamic priority.
next topic non preemptive priority scheduling
← prev next →
next → ← prev non preemptive priority scheduling in the non preemptive priority scheduling, the processes are scheduled according to the priority number assigned to them. once the process gets scheduled, it will run till the completion. generally, the lower the priority number, the higher is the priority of the process. the people might get confused with the priority numbers, hence in the gate, there clearly mention which one is the highest priority and which one is the lowest one. example in the example, there are  processes p, p, p, p, p, p and p. their priorities, arrival time and burst time are given in the table. process id priority arrival time burst time                             we can prepare the gantt chart according to the non preemptive priority scheduling. the process p arrives at time  with the burst time of  units and the priority number . since no other process has arrived till now hence the os will schedule it immediately. meanwhile the execution of p, two more processes p and p are arrived. since the priority of p is  hence the cpu will execute p over p. meanwhile the execution of p, all the processes get available in the ready queue. the process with the lowest priority number will be given the priority. since p has priority number assigned as  hence it will be executed just after p. after p, p has the least priority number among the available processes; it will get executed for the whole burst time. since all the jobs are available in the ready queue hence all the jobs will get executed according to their priorities. if two jobs have similar priority number assigned to them, the one with the least arrival time will be executed. from the gantt chart prepared, we can determine the completion time of every process. the turnaround time, waiting time and response time will be determined. turn around time = completion time - arrival time waiting time = turn around time - burst time process id priority arrival time burst time completion time turnaround time waiting time response time                                                         avg waiting time = (++++++)/ = / units
next topic preemptive priority scheduling
← prev next →
next → ← prev preemptive priority scheduling in preemptive priority scheduling, at the time of arrival of a process in the ready queue, its priority is compared with the priority of the other processes present in the ready queue as well as with the one which is being executed by the cpu at that point of time. the one with the highest priority among all the available processes will be given the cpu next. the difference between preemptive priority scheduling and non preemptive priority scheduling is that, in the preemptive priority scheduling, the job which is being executed can be stopped at the arrival of a higher priority job. once all the jobs get available in the ready queue, the algorithm will behave as non-preemptive priority scheduling, which means the job scheduled will run till the completion and no preemption will be done. example there are  processes p, p, p, p, p, p and p given. their respective priorities, arrival times and burst times are given in the table below. process id priority arrival time burst time  (l)                    (h)       gantt chart preparation at time , p arrives with the burst time of  units and priority . since no other process is available hence this will be scheduled till next job arrives or its completion (whichever is lesser). at time , p arrives. p has completed its execution and no other process is available at this time hence the operating system has to schedule it regardless of the priority assigned to it. the next process p arrives at time unit , the priority of p is higher to p. hence the execution of p will be stopped and p will be scheduled on the cpu. during the execution of p, three more processes p, p and p becomes available. since, all these three have the priority lower to the process in execution so ps can't preempt the process. p will complete its execution and then p will be scheduled with the priority highest among the available processes. meanwhile the execution of p, all the processes got available in the ready queue. at this point, the algorithm will start behaving as non preemptive priority scheduling. hence now, once all the processes get available in the ready queue, the os just took the process with the highest priority and execute that process till completion. in this case, p will be scheduled and will be executed till the completion. since p is completed, the other process with the highest priority available in the ready queue is p. hence p will be scheduled next. p is given the cpu till the completion. since its remaining burst time is  units hence p will be scheduled after this. the only remaining process is p with the least priority, the operating system has no choice unless of executing it. this will be executed at the last. the completion time of each process is determined with the help of gantt chart. the turnaround time and the waiting time can be calculated by the following formula. turnaround time = completion time - arrival time waiting time = turn around time - burst time process id priority arrival time burst time completion time turn around time waiting time                                                  avg waiting time = (++++++)/ = / =  units
next topic srtf:io bound processes
← prev next →
next → ← prev srtf with processes contains cpu and io time till now, we were considering the cpu bound jobs only. however, the process might need some io operation or some resource to complete its execution. in this example, we are considering, the io bound processes. in the example, there are four jobs with process id p, p, p and p are available. their arrival time, and the cpu burst time are given in the table below. process id arrival time (burst time, io burst time, burst time)   (,,)   (,,)   (,,)   (,,) gantt chart preparation at time , the process p and p arrives. since the algorithm we are using is srtf hence, the process with the shortest burst time will be scheduled on the cpu. in this case, it is p.
from time  to time , p will be in running state.
p also needs some io time in order to complete its execution. after  unit of execution, p will change its state from running to waiting. the processor becomes free to execute other jobs. since no other process is available at this point of time other than p so p will get executed. the following diagram illustrates the processes and states at time . the process p went to waiting state and the cpu becomes idol at this time.
from time  to , since p is being in waiting state, and no other process is available in ready queue, hence the only available process p will be executed in this period of time.
at time , the process p arrived with the total cpu burst time of  units. since the remaining burst time of p is lesser then p hence cpu will continue its execution.
hence, p will remain in the running state from time  to time .
since p is an io bound process. at time unit , it will change its state from running to waiting. processor becomes free for the execution of other jobs. since p also becomes available at time  because it has completed the io operation and now it needs another  unit of cpu burst time. p is also available and requires  units of total cpu burst time.
the process with the least remaining cpu burst time among the available processes will get executed. in our case, such process is p which requires  unit of burst time hence it will be given the cpu.
at time , p is finished. p is still in waiting state. at this point of time, the only available process is p, hence it will be given the cpu.
from time  to time , p will be in the running state; meanwhile, p will still be in waiting state.
at time , the process p arrives in the ready queue. the p has also done with the io and becomes available for the execution. p is not yet finished and still needs another  unit of cpu burst time. from time  to time , the reaming cpu burst time of process p is least among the available processes, hence p will be given the cpu.
p needs some io operation in order to complete its execution. at time , p will change its state from running to waiting. the cpu becomes free to execute the other processes. process p and p are available out of which, the process with the least remaining burst time will get executed.
from time  to time , the process p will get executed.
at time , the io of process p is finished and it will now be available in the ready state along with p which is already waiting there for its turn. in order to complete its execution, it needs another  unit of burst time. p is in running state at this point of the time while no process is present in the waiting state.
from time  to  , the process p will get executed since its remaining cpu burst time is lesser then the processes p and p available in the ready queue.
at time , execution of p is finished, and now the cpu becomes idol. the process with the lesser cpu burst time among the ready processes will get the cpu turn. from time  to , the process p will get executed till its completion because of the fact that its remaining cpu burst time is the between the two available processes. it needs  units of more cpu burst time, since no other process will be arrived in the ready state hence no preemption will be done and it will be executed till the completion.
at time , the process p will get completed, since there is only one process p available in the ready state hence p will be given the cpu.
p needs  units of cpu burst time before io, hence it will be executed till time  (for  units) and then it will change its state from running to waiting. at time , the process p changes its state from running to waiting. since this is the only process in the system hence the cpu will remain idol until p becomes available again.
at time , p will be done with the io operation and becomes available in the ready state.
from time , the process p will get scheduled. since no other process is in ready queue hence the processor don't have any choice. it will be executed till completion.
final gantt chart: process id arrival time total cpu burst time completion time turn around time waiting time                         average waiting time = (+++)/ = / units
next topic process synchronization introduction
← prev next →
introduction
when two or more process cooperates with each other, their order of execution must be preserved otherwise there can be conflicts in their execution and inappropriate outputs can be produced.
a cooperative process is the one which can affect the execution of other process or can be affected by the execution of other process. such processes need to be synchronized so that their order of execution can be guaranteed.
the procedure involved in preserving the appropriate order of execution of cooperative processes is known as process synchronization. there are various synchronization mechanisms that are used to synchronize the processes.
race condition
a race condition typically occurs when two or more threads try to read, write and possibly make the decisions based on the memory that they are accessing concurrently.
critical section
the regions of a program that try to access shared resources and may cause race conditions are called critical section. to avoid race condition among the processes, we need to assure that only one process at a time can execute within the critical section.
next → ← prev the critical section problem critical section is the part of a program which tries to access shared resources. that resource may be any resource in a computer like a memory location, data structure, cpu or any io device. the critical section cannot be executed by more than one process at the same time; operating system faces the difficulties in allowing and disallowing the processes from entering the critical section. the critical section problem is used to design a set of protocols which can ensure that the race condition among the processes will never arise. in order to synchronize the cooperative processes, our main task is to solve the critical section problem. we need to provide a solution in such a way that the following conditions can be satisfied. requirements of synchronization mechanisms
primary mutual exclusion our solution must provide mutual exclusion. by mutual exclusion, we mean that if one process is executing inside critical section then the other process must not enter in the critical section.
progress progress means that if one process doesn't need to execute into critical section then it should not stop other processes to get into the critical section. secondary bounded waiting we should be able to predict the waiting time for every process to get into the critical section. the process must not be endlessly waiting for getting into the critical section. architectural neutrality our mechanism must be architectural natural. it means that if our solution is working fine on one architecture then it should also run on the other ones as well.
next topic lock variable mechanism
← prev next →
next topic tsl mechanism
← prev next →
mutual exclusion is guaranteed in tsl mechanism since a process can never be preempted just before setting the lock variable. only one process can see the lock variable as  at a particular time and that's why, the mutual exclusion is guaranteed. progress
according to the definition of the progress, a process which doesn't want to enter in the critical section should not stop other processes to get into it. in tsl mechanism, a process will execute the tsl instruction only when it wants to get into the critical section. the value of the lock will always be  if no process doesn't want to enter into the critical section hence the progress is always guaranteed in tsl. bounded waiting
bounded waiting is not guaranteed in tsl. some process might not get a chance for so long. we cannot predict for a process that it will definitely get a chance to enter in critical section after a certain time. architectural neutrality tsl doesn't provide architectural neutrality. it depends on the hardware platform. the tsl instruction is provided by the operating system. some platforms might not provide that. hence it is not architectural natural.
next topic priority inversion in tsl
← prev next →
next → ← prev priority inversion in tsl mechanism, there can be a problem of priority inversion. let?s say that there are two cooperative processes, p and p. the priority of p is  while that of p is . p arrives earlier and got scheduled by the cpu. since it is a cooperative process and wants to execute in the critical section hence it will enter in the critical section by setting the lock variable to . now, p arrives in the ready queue. the priority of p is higher than p hence according to priority scheduling, p is scheduled and p got preempted. p is also a cooperative process and wants to execute inside the critical section. although, p got preempted but it the value of lock variable will be shown as  since p is not completed and it is yet to finish its critical section. p needs to finish the critical section but according to the scheduling algorithm, cpu is with p. p wants to execute in the critical section, but according to the synchronization mechanism, critical section is with p. this is a kind of lock where each of the process neither executes nor completes. such kind of lock is called spin lock. this is different from deadlock since they are not in blocked state. one is in ready state and the other is in running state, but neither of the two is being executed.
next topic turn variable or strict alternation
← prev next →
the turn variable is equal to i hence pi will get the chance to enter into the critical section. the value of pi remains i until pi finishes critical section.
pi finishes its critical section and assigns j to turn variable. pj will get the chance to enter into the critical section. the value of turn remains j until pj finishes its critical section.
next topic interested variable mechanism
← prev next →
next → ← prev interested variable mechanism we have to make sure that the progress must be provided by our synchronization mechanism. in the turn variable mechanism, progress was not provided due to the fact that the process which doesn't want to enter in the critical section does not consider the other interested process as well. the other process will also have to wait regardless of the fact that there is no one inside the critical section. if the operating system can make use of an extra variable along with the turn variable then this problem can be solved and our problem can provide progress to most of the extent. interested variable mechanism makes use of an extra boolean variable to make sure that the progress is provided. for process pi non cs int[i] = t ; while ( int[j] == t ) ; critical section int[i] = f ; for process pj non cs int [] = t ; while ( int[i] == t ) ; critical section int[j]=f ; in this mechanism, an extra variable interested is used. this is a boolean variable used to store the interest of the processes to get enter inside the critical section. a process which wants to enter in the critical section first checks in the entry section whether the other process is interested to get inside. the process will wait for the time until the other process is interested. in exit section, the process makes the value of its interest variable false so that the other process can get into the critical section. the table shows the possible values of interest variable of both the processes and the process which get the chance in the scenario. interest [pi] interest [pj] process which get the chance true true the process which first shows interest. true false pi false true pj false false x let's analyze the mechanism on the basis of the requirements. mutual exclusion in interested variable mechanism, if one process is interested in getting into the cpu then the other process will wait until it becomes uninterested. therefore, more than one process can never be present in the critical section at the same time hence the mechanism guarantees mutual exclusion. progress in this mechanism, if a process is not interested in getting into the critical section then it will not stop the other process from getting into the critical section. therefore the progress will definitely be provided by this method. bounded waiting to analyze bounded waiting, let us consider two processes pi and pj, are the cooperative processes wants to execute in the critical section. the instructions executed by the processes are shown below in relative manner. process pi process pj process pi process pj . int [pi] = true
shes the critical section and makes an exit by setting the interest variable to false. now, a case can be possible when pi again wants to enter in the critical section and set its interested variable to true and checks whether the interested variable of pj is true. here, pj's interest variable is true hence pi will get stuck in the while loop and waits for pj become uninterested. since, pj still stuck in the while loop waiting for the pi' interested variable to become false. therefore, both the processes are waiting for each other and none of them is getting into the critical section. this is a condition of deadlock and bounded waiting can never be provided in the case of deadlock. therefore, we can say that the interested variable mechanism doesn't guarantee deadlock. architectural neutrality the mechanism is a complete software mechanism executed in the user mode therefore it guarantees portability or architectural neutrality.
next topic paterson solution
next topic synchronization mechanism without busy waiting
synchronization mechanism without busy waiting
all the solutions we have seen till now were intended to provide mutual exclusion with busy waiting. however, busy waiting is not the optimal allocation of resources because it keeps cpu busy all the time in checking the while loops condition continuously although the process is waiting for the critical section to become available.
all the synchronization mechanism with busy waiting are also suffering from the priority inversion problem that is there is always a possibility of spin lock whenever there is a process with the higher priority has to wait outside the critical section since the mechanism intends to execute the lower priority process in the critical section.
however these problems need a proper solution without busy waiting and priority inversion.
#define n  //maximum slots in buffer #define count= //items in the buffer void producer (void) { int item; while(true) { item = produce_item(); //producer produces an item if(count == n) //if the buffer is full then the producer will sleep sleep(); insert_item (item); //the item is inserted into buffer count=count+; if(count==) //the producer will wake up the //consumer if there is at least  item in the buffer wake-up(consumer); } } void consumer (void) { int item; while(true) { { if(count == ) //the consumer will sleep if the buffer is empty. sleep(); item = remove_item(); count = count - ; if(count == n-) //if there is at least one slot available in the buffer //then the consumer will wake up producer wake-up(producer); consume_item(item); //the item is read by consumer. } } }
the producer produces the item and inserts it into the buffer. the value of the global variable count got increased at each insertion. if the buffer is filled completely and no slot is available then the producer will sleep, otherwise it keep inserting. on the consumer's end, the value of count got decreased by  at each consumption. if the buffer is empty at any point of time then the consumer will sleep otherwise, it keeps consuming the items and decreasing the value of count by . the consumer will be waked up by the producer if there is at least  item available in the buffer which is to be consumed. the producer will be waked up by the consumer if there is at least one slot available in the buffer so that the producer can write that. well, the problem arises in the case when the consumer got preempted just before it was about to sleep. now the consumer is neither sleeping nor consuming. since the producer is not aware of the fact that consumer is not actually sleeping therefore it keep waking the consumer while the consumer is not responding since it is not sleeping. this leads to the wastage of system calls. when the consumer get scheduled again, it will sleep because it was about to sleep when it was preempted. the producer keep writing in the buffer and it got filled after some time. the producer will also sleep at that time keeping in the mind that the consumer will wake him up when there is a slot available in the buffer. the consumer is also sleeping and not aware with the fact that the producer will wake him up. this is a kind of deadlock where neither producer nor consumer is active and waiting for each other to wake them up. this is a serious problem which needs to be addressed. using a flag bit to get rid of this problem a flag bit can be used in order to get rid of this problem. the producer can set the bit when it calls wake-up on the first time. when the consumer got scheduled, it checks the bit. the consumer will now get to know that the producer tried to wake him and therefore it will not sleep and get into the ready state to consume whatever produced by the producer. this solution works for only one pair of producer and consumer, what if there are n producers and n consumers. in that case, there is a need to maintain an integer which can record how many wake-up calls have been made and how many consumers need not sleep. this integer variable is called semaphore. we will discuss more about semaphore later in detail.
next topic semaphore introduction
← prev next →
introduction to semaphore
to get rid of the problem of wasting the wake-up signals, dijkstra proposed an approach which involves storing all the wake-up calls. dijkstra states that, instead of giving the wake-up calls directly to the consumer, producer can store the wake-up call in a variable. any of the consumers can read it whenever it needs to do so.
semaphore is the variables which storesthe entire wake up calls that are being transferred from producer to consumer. it is a variable on which read, modify and update happens automatically in kernel mode.
semaphore cannot be implemented in the user mode because race condition may always arise when two or more processes try to access the variable simultaneously. it always needs support from the operating system to be implemented.
according to the demand of the situation, semaphore can be divided into two categories.
counting semaphore binary semaphore or mutex
we will discuss each one in detail.
struct semaphore { int value; // processes that can enter in the critical section simultaneously. queue type l; // l contains set of processes which get blocked } down (semaphore s) { s.value = s.value - ; //semaphore's value will get decreased when a new //process enter in the critical section if (s.value< ) { put_process(pcb) in l; //if the value is negative then //the process will get into the blocked state. sleep(); } else return; } up (semaphore s) { s.value = s.value+; //semaphore value will get increased when //it makes an exit from the critical section. if(s.value<=) { select a process from l; //if the value of semaphore is positive //then wake one of the processes in the blocked queue. wake-up(); } } }
in this mechanism, the entry and exit in the critical section are performed on the basis of the value of counting semaphore. the value of counting semaphore at any point of time indicates the maximum number of processes that can enter in the critical section at the same time. a process which wants to enter in the critical section first decrease the semaphore value by  and then check whether it gets negative or not. if it gets negative then the process is pushed in the list of blocked processes (i.e. q) otherwise it gets enter in the critical section. when a process exits from the critical section, it increases the counting semaphore by  and then checks whether it is negative or zero. if it is negative then that means that at least one process is waiting in the blocked state hence, to ensure bounded waiting, the first process among the list of blocked processes will wake up and gets enter in the critical section. the processes in the blocked list will get waked in the order in which they slept. if the value of counting semaphore is negative then it states the number of processes in the blocked state while if it is positive then it states the number of slots available in the critical section.
next topic problem on counting semaphore
← prev next →
problem on counting semaphore
the questions are being asked on counting semaphore in gate. generally the questions are very simple that contains only subtraction and addition.
wait → decre → down → p signal → inc → up → v
the following type questions can be asked in gate.
a counting semaphore was initialized to . then p (wait) and v (signal) operations were computed on this semaphore. what is the result?
s =  (initial)  p (wait) : s = s - =  -  =  then  v : s = s +  = +  = 
hence, the final value of counting semaphore is .
next → ← prev binary semaphore or mutex in counting semaphore, mutual exclusion was not provided because we has the set of processes which required to execute in the critical section simultaneously. however, binary semaphore strictly provides mutual exclusion. here, instead of having more than  slots available in the critical section, we can only have at most  process in the critical section. the semaphore can have only two values,  or . let's see the programming implementation of binary semaphore.
structbsemaphore { enum value(,); //value is enumerated data type which can only have two values  or . queue type l; } /* l contains all pcbs corresponding to process blocked while processing down operation unsuccessfully. */ down (bsemaphore s) { if (s.value == ) // if a slot is available in the //critical section then let the process enter in the queue. { s.value = ; // initialize the value to  so that no other process can read it as . } else { put the process (pcb) in s.l; //if no slot is available //then let the process wait in the blocked queue. sleep(); } } up (bsemaphore s) { if (s.l is empty) //an empty blocked processes list implies that no process //has ever tried to get enter in the critical section. { s.value =; } else { select a process from s.l; wakeup(); // if it is not empty then wake the first process of the blocked queue. } }
next topic introduction deadlocks
← prev next →
next → ← prev introduction to deadlock every process needs some resources to complete its execution. however, the resource is granted in a sequential order. the process requests for some resource. os grant the resource if it is available otherwise let the process waits. the process uses it and release on the completion. a deadlock is a situation where each of the computer process waits for a resource which is being assigned to some another process. in this situation, none of the process gets executed since the resource it needs, is held by some other process which is also waiting for some other resource to be released. let us assume that there are three processes p, p and p. there are three different resources r, r and r. r is assigned to p, r is assigned to p and r is assigned to p. after some time, p demands for r which is being used by p. p halts its execution since it can't complete without r. p also demands for r which is being used by p. p also stops its execution because it can't continue without r. p also demands for r which is being used by p therefore p also stops its execution. in this scenario, a cycle is being formed among the three processes. none of the process is progressing and they are all waiting. the computer becomes unresponsive since all the processes got blocked. difference between starvation and deadlock sr. deadlock starvation  deadlock is a situation where no process got blocked and no process proceeds starvation is a situation where the low priority process got blocked and the high priority processes proceed.  deadlock is an infinite waiting. starvation is a long waiting but not infinite.  every deadlock is always a starvation. every starvation need not be deadlock.  the requested resource is blocked by the other process. the requested resource is continuously be used by the higher priority processes.  deadlock happens when mutual exclusion, hold and wait, no preemption and circular wait occurs simultaneously. it occurs due to the uncontrolled priority and resource management. necessary conditions for deadlocks mutual exclusion a resource can only be shared in mutually exclusive manner. it implies, if two process cannot use the same resource at the same time. hold and wait a process waits for some resources while holding another resource at the same time. no preemption the process which once scheduled will be executed till the completion. no other process can be scheduled by the scheduler meanwhile. circular wait all the processes must be waiting for the resources in a cyclic manner so that the last process is waiting for the resource which is being held by the first process.
next topic strategies handling deadlock
← prev next →
next → ← prev strategies for handling deadlock . deadlock ignorance deadlock ignorance is the most widely used approach among all the mechanism. this is being used by many operating systems mainly for end user uses. in this approach, the operating system assumes that deadlock never occurs. it simply ignores deadlock. this approach is best suitable for a single end user system where user uses the system only for browsing and all other normal stuff. there is always a tradeoff between correctness and performance. the operating systems like windows and linux mainly focus upon performance. however, the performance of the system decreases if it uses deadlock handling mechanism all the time if deadlock happens  out of  times then it is completely unnecessary to use the deadlock handling mechanism all the time. in these types of systems, the user has to simply restart the computer in the case of deadlock. windows and linux are mainly using this approach. . deadlock prevention deadlock happens only when mutual exclusion, hold and wait, no preemption and circular wait holds simultaneously. if it is possible to violate one of the four conditions at any time then the deadlock can never occur in the system. the idea behind the approach is very simple that we have to fail one of the four conditions but there can be a big argument on its physical implementation in the system. we will discuss it later in detail. . deadlock avoidance in deadlock avoidance, the operating system checks whether the system is in safe state or in unsafe state at every step which the operating system performs. the process continues until the system is in safe state. once the system moves to unsafe state, the os has to backtrack one step. in simple words, the os reviews each allocation so that the allocation doesn't cause the deadlock in the system. we will discuss deadlock avoidance later in detail. . deadlock detection and recovery this approach let the processes fall in deadlock and then periodically check whether deadlock occur in the system or not. if it occurs then it applies some of the recovery methods to the system to get rid of deadlock. we will discuss deadlock detection and recovery later in more detail since it is a matter of discussion.
next topic deadlock prevention
← prev next →
next → ← prev deadlock prevention if we simulate deadlock with a table which is standing on its four legs then we can also simulate four legs with the four conditions which when occurs simultaneously, cause the deadlock. however, if we break one of the legs of the table then the table will fall definitely. the same happens with deadlock, if we can be able to violate one of the four necessary conditions and don't let them occur together then we can prevent the deadlock. let's see how we can prevent each of the conditions. . mutual exclusion mutual section from the resource point of view is the fact that a resource can never be used by more than one process simultaneously which is fair enough but that is the main reason behind the deadlock. if a resource could have been used by more than one process at the same time then the process would have never been waiting for any resource. however, if we can be able to violate resources behaving in the mutually exclusive manner then the deadlock can be prevented. spooling for a device like printer, spooling can work. there is a memory associated with the printer which stores jobs from each of the process into it. later, printer collects all the jobs and print each one of them according to fcfs. by using this mechanism, the process doesn't have to wait for the printer and it can continue whatever it was doing. later, it collects the output when it is produced.
although, spooling can be an effective approach to violate mutual exclusion but it suffers from two kinds of problems. this cannot be applied to every resource. after some point of time, there may arise a race condition between the processes to get space in that spool. we cannot force a resource to be used by more than one process at the same time since it will not be fair enough and some serious problems may arise in the performance. therefore, we cannot violate mutual exclusion for a process practically. . hold and wait hold and wait condition lies when a process holds a resource and waiting for some other resource to complete its task. deadlock occurs because there can be more than one process which are holding one resource and waiting for other in the cyclic order. however, we have to find out some mechanism by which a process either doesn't hold any resource or doesn't wait. that means, a process must be assigned all the necessary resources before the execution starts. a process must not wait for any resource once the execution has been started. !(hold and wait) = !hold or !wait (negation of hold and wait is, either you don't hold or you don't wait) this can be implemented practically if a process declares all the resources initially. however, this sounds very practical but can't be done in the computer system because a process can't determine necessary resources initially. process is the set of instructions which are executed by the cpu. each of the instruction may demand multiple resources at the multiple times. the need cannot be fixed by the os. the problem with the approach is: practically not possible. possibility of getting starved will be increases due to the fact that some process may hold a resource for a very long time. . no preemption deadlock arises due to the fact that a process can't be stopped once it starts. however, if we take the resource away from the process which is causing deadlock then we can prevent deadlock. this is not a good approach at all since if we take a resource away which is being used by the process then all the work which it has done till now can become inconsistent. consider a printer is being used by any process. if we take the printer away from that process and assign it to some other process then all the data which has been printed can become inconsistent and ineffective and also the fact that the process can't start printing again from where it has left which causes performance inefficiency. . circular wait to violate circular wait, we can assign a priority number to each of the resource. a process can't request for a lesser priority resource. this ensures that not a single process can request a resource which is being utilized by some other process and no cycle will be formed.
among all the methods, violating circular wait is the only approach that can be implemented practically.
next topic deadlock avoidance
← prev next →
next → ← prev deadlock avoidance in deadlock avoidance, the request for any resource will be granted if the resulting state of the system doesn't cause deadlock in the system. the state of the system will continuously be checked for safe and unsafe states. in order to avoid deadlocks, the process must tell os, the maximum number of resources a process can request to complete its execution. the simplest and most useful approach states that the process should declare the maximum number of resources of each type it may ever need. the deadlock avoidance algorithm examines the resource allocations so that there can never be a circular wait condition. safe and unsafe states the resource allocation state of a system can be defined by the instances of available and allocated resources, and the maximum instance of the resources demanded by the processes. a state of a system recorded at some random time is shown below. resources assigned process type  type  type  type  a     b     c     d     resources still needed process type  type  type  type  a     b     c     d    
e = (   ) p = (   ) a = (   )
above tables and vector e, p and a describes the resource allocation state of a system. there are  processes and  types of the resources in a system. table  shows the instances of each resource assigned to each process. table  shows the instances of the resources, each process still needs. vector e is the representation of total instances of each resource in the system. vector p represents the instances of resources that have been assigned to processes. vector a represents the number of resources that are not in use. a state of the system is called safe if the system can allocate all the resources requested by all the processes without entering into deadlock. if the system cannot fulfill the request of all processes then the state of the system is called unsafe. the key of deadlock avoidance approach is when the request is made for resources then the request must only be approved in the case if the resulting state is also a safe state.
next topic resource allocation graph
← prev next →
next → ← prev resource allocation graph the resource allocation graph is the pictorial representation of the state of a system. as its name suggests, the resource allocation graph is the complete information about all the processes which are holding some resources or waiting for some resources. it also contains the information about all the instances of all the resources whether they are available or being used by the processes. in resource allocation graph, the process is represented by a circle while the resource is represented by a rectangle. let's see the types of vertices and edges in detail.
vertices are mainly of two types, resource and process. each of them will be represented by a different shape. circle represents process while rectangle represents resource. a resource can have more than one instance. each instance will be represented by a dot inside the rectangle.
edges in rag are also of two types, one represents assignment and other represents the wait of a process for a resource. the above image shows each of them. a resource is shown as assigned to a process if the tail of the arrow is attached to an instance to the resource and the head is attached to a process. a process is shown as waiting for a resource if the tail of an arrow is attached to the process while the head is pointing towards the resource.
example let'sconsider  processes p, p and p, and two types of resources r and r. the resources are having  instance each. according to the graph, r is being used by p, p is holding r and waiting for r, p is waiting for r as well as r. the graph is deadlock free since no cycle is being formed in the graph.
next topic deadlock detection using rag
← prev next →
next → ← prev deadlock detection using rag if a cycle is being formed in a resource allocation graph where all the resources have the single instance then the system is deadlocked. in case of resource allocation graph with multi-instanced resource types, cycle is a necessary condition of deadlock but not the sufficient condition. the following example contains three processes p, p, p and three resources r, r, r. all the resources are having single instances each.
if we analyze the graph then we can find out that there is a cycle formed in the graph since the system is satisfying all the four conditions of deadlock. allocation matrix allocation matrix can be formed by using the resource allocation graph of a system. in allocation matrix, an entry will be made for each of the resource assigned. for example, in the following matrix, en entry is being made in front of p and below r since r is assigned to p. process r r r p    p    p    request matrix in request matrix, an entry will be made for each of the resource requested. as in the following example, p needs r therefore an entry is being made in front of p and below r. process r r r p    p    p    avial = (,,) neither we are having any resource available in the system nor a process going to release. each of the process needs at least single resource to complete therefore they will continuously be holding each one of them. we cannot fulfill the demand of at least one process using the available resources therefore the system is deadlocked as determined earlier when we detected a cycle in the graph.
next topic deadlock detection and recovery
← prev next →
next → ← prev deadlock detection and recovery in this approach, the os doesn't apply any mechanism to avoid or prevent the deadlocks. therefore the system considers that the deadlock will definitely occur. in order to get rid of deadlocks, the os periodically checks the system for any deadlock. in case, it finds any of the deadlock then the os will recover the system using some recovery techniques. the main task of the os is detecting the deadlocks. the os can detect the deadlocks with the help of resource allocation graph.
in single instanced resource types, if a cycle is being formed in the system then there will definitely be a deadlock. on the other hand, in multiple instanced resource type graph, detecting a cycle is not just enough. we have to apply the safety algorithm on the system by converting the resource allocation graph into the allocation matrix and request matrix. in order to recover the system from deadlocks, either os considers resources or processes. for resource
preempt the resource we can snatch one of the resources from the owner of the resource (process) and give it to the other process with the expectation that it will complete the execution and will release this resource sooner. well, choosing a resource which will be snatched is going to be a bit difficult. rollback to a safe state system passes through various states to get into the deadlock state. the operating system canrollback the system to the previous safe state. for this purpose, os needs to implement check pointing at every state. the moment, we get into deadlock, we will rollback all the allocations to get into the previous safe state. for process
kill a process killing a process can solve our problem but the bigger concern is to decide which process to kill. generally, operating system kills a process which has done least amount of work until now. kill all process this is not a suggestible approach but can be implemented if the problem becomes very serious. killing all process will lead to inefficiency in the system because all the processes will execute again from starting.
next topic memory management introduction
← prev next →
next → ← prev what is memory? computer memory can be defined as a collection of some data represented in the binary format. on the basis of various functions, memory can be classified into various categories. we will discuss each one of them later in detail. a computer device that is capable to store any information or data temporally or permanently, is called storage device. how data is being stored in a computer system? in order to understand memory management, we have to make everything clear about how data is being stored in a computer system. machine understands only binary language that is  or . computer converts every data into binary language first and then stores it into the memory. that means if we have a program line written as int α =  then the computer converts it into the binary language and then store it into the memory blocks. the representation of inti =  is shown below.
the binary representation of  is . here, we are considering  bit system therefore, the size of int is  bytes i.e.  bit.  memory block stores  bit. if we are using signed integer then the most significant bit in the memory array is always a signed bit. signed bit value  represents positive integer while  represents negative integer. here, the range of values that can be stored using the memory array is - to +. well, we can enlarge this range by using unsigned int. in that case, the bit which is now storing the sign will also store the bit value and therefore the range will be  to ,. need for multi programming however, the cpu can directly access the main memory, registers and cache of the system. the program always executes in main memory. the size of main memory affects degree of multi programming to most of the extant. if the size of the main memory is larger than cpu can load more processes in the main memory at the same time and therefore will increase degree of multi programming as well as cpu utilization. let's consider, process size =  mb main memory size =  mb the process can only reside in the main memory at any time. if the time for which the process does io is p, then, cpu utilization = (-p) let's say, p = % cpu utilization =  % now, increase the memory size, let's say it is  mb. process size =  mb two processes can reside in the main memory at the same time. let's say the time for which, one process does its io is p, then cpu utilization = (-p^) let's say p =  % cpu utilization = (-.) =. =  % therefore, we can state that the cpu utilization will be increased if the memory size gets increased.
next topic fixed partitioning
← prev next →
next → ← prev fixed partitioning the earliest and one of the simplest technique which can be used to load more than one processes into the main memory is fixed partitioning or contiguous memory allocation. in this technique, the main memory is divided into partitions of equal or different sizes. the operating system always resides in the first partition while the other partitions can be used to store user processes. the memory is assigned to the processes in contiguous way. in fixed partitioning, the partitions cannot overlap. a process must be contiguously present in a partition for the execution. there are various cons of using this technique. . internal fragmentation if the size of the process is lesser then the total size of the partition then some size of the partition get wasted and remain unused. this is wastage of the memory and called internal fragmentation. as shown in the image below, the  mb partition is used to load only  mb process and the remaining  mb got wasted. . external fragmentation the total unused space of various partitions cannot be used to load the processes even though there is space available but not in the contiguous form. as shown in the image below, the remaining  mb space of each partition cannot be used as a unit to store a  mb process. despite of the fact that the sufficient space is available to load the process, process will not be loaded. . limitation on the size of the process if the process size is larger than the size of maximum sized partition then that process cannot be loaded into the memory. therefore, a limitation can be imposed on the process size that is it cannot be larger than the size of the largest partition. . degree of multiprogramming is less by degree of multi programming, we simply mean the maximum number of processes that can be loaded into the memory at the same time. in fixed partitioning, the degree of multiprogramming is fixed and very less due to the fact that the size of the partition cannot be varied according to the size of processes.
next topic dynamic partitioning
← prev next →
next → ← prev dynamic partitioning dynamic partitioning tries to overcome the problems caused by fixed partitioning. in this technique, the partition size is not declared initially. it is declared at the time of process loading. the first partition is reserved for the operating system. the remaining space is divided into parts. the size of each partition will be equal to the size of the process. the partition size varies according to the need of the process so that the internal fragmentation can be avoided.
advantages of dynamic partitioning over fixed partitioning
. no internal fragmentation given the fact that the partitions in dynamic partitioning are created according to the need of the process, it is clear that there will not be any internal fragmentation because there will not be any unused remaining space in the partition. . no limitation on the size of the process in fixed partitioning, the process with the size greater than the size of the largest partition could not be executed due to the lack of sufficient contiguous memory. here, in dynamic partitioning, the process size can't be restricted since the partition size is decided according to the process size. . degree of multiprogramming is dynamic due to the absence of internal fragmentation, there will not be any unused space in the partition hence more processes can be loaded in the memory at the same time. disadvantages of dynamic partitioning
external fragmentation absence of internal fragmentation doesn't mean that there will not be external fragmentation. let's consider three processes p ( mb) and p ( mb) and p ( mb) are being loaded in the respective partitions of the main memory. after some time p and p got completed and their assigned space is freed. now there are two unused partitions ( mb and  mb) available in the main memory but they cannot be used to load a  mb process in the memory since they are not contiguously located. the rule says that the process must be contiguously present in the main memory to get executed. we need to change this rule to avoid external fragmentation.
complex memory allocation in fixed partitioning, the list of partitions is made once and will never change but in dynamic partitioning, the allocation and deallocation is very complex since the partition size will be varied every time when it is assigned to a new process. os has to keep track of all the partitions. due to the fact that the allocation and deallocation are done very frequently in dynamic memory allocation and the partition size will be changed at each time, it is going to be very difficult for os to manage everything.
next topic compaction
← prev next →
next → ← prev compaction we got to know that the dynamic partitioning suffers from external fragmentation. however, this can cause some serious problems. to avoid compaction, we need to change the rule which says that the process can't be stored in the different places in the memory. we can also use compaction to minimize the probability of external fragmentation. in compaction, all the free partitions are made contiguous and all the loaded partitions are brought together. by applying this technique, we can store the bigger processes in the memory. the free partitions are merged which can now be allocated according to the needs of new processes. this technique is also called defragmentation.
as shown in the image above, the process p, which could not be loaded into the memory due to the lack of contiguous space, can be loaded now in the memory since the free partitions are made contiguous. problem with compaction the efficiency of the system is decreased in the case of compaction due to the fact that all the free spaces will be transferred from several places to a single place. huge amount of time is invested for this procedure and the cpu will remain idle for all this time. despite of the fact that the compaction avoids external fragmentation, it makes system inefficient. let us consider that os needs  ns to copy  byte from one place to another.  b transfer needs  ns  mb transfer needs  x ^ x  x  ^ - secs hence, it is proved to some extent that the larger size memory transfer needs some huge amount of time that is in seconds.
next topic bit map for dynamic partitioning
← prev next →
next → ← prev bit map for dynamic partitioning the main concern for dynamic partitioning is keeping track of all the free and allocated partitions. however, the operating system uses following data structures for this task. bit map linked list bit map is the least famous data structure to store the details. in this scheme, the main memory is divided into the collection of allocation units. one or more allocation units may be allocated to a process according to the need of that process. however, the size of the allocation unit is fixed that is defined by the operating system and never changed. although the partition size may vary but the allocation size is fixed. the main task of the operating system is to keep track of whether the partition is free or filled. for this purpose, the operating system also manages another data structure that is called bitmap. the process or the hole in allocation units is represented by a flag bit of bitmap. in the image shown below, a flag bit is defined for every bit of allocation units. however, it is not the general case, it depends on the os that, for how many bits of the allocation units, it wants to store the flag bit. the flag bit is set to  if there is a contiguously present process at the adjacent bit in allocation unit otherwise it is set to . a string of s in the bitmap shows that there is a hole in the relative allocation unit while the string of s represents the process in the relative allocation unit.
disadvantages of using bitmap . the os has to assign some memory for bitmap as well since it stores the details about allocation units. that much amount of memory cannot be used to load any process therefore that decreases the degree of multiprogramming as well as throughput. in the above image, the allocation unit is of  bits that is . bits. here,  bit of the bitmap is representing  bit of allocation unit. size of  allocation unit =  bits size of bitmap = /(+) = / of total main memory. therefore, in this bitmap configuration, / of total main memory is wasted. . to identify any hole in the memory, the os need to search the string of s in the bitmap. this searching takes a huge amount of time which makes the system inefficient to some extent
next topic linked list for dynamic partitioning
← prev next →
next → ← prev linked list for dynamic partitioning the better and the most popular approach to keep track the free or filled partitions is using linked list. in this approach, the operating system maintains a linked list where each node represents each partition. every node has three fields. first field of the node stores a flag bit which shows whether the partition is a hole or some process is inside. second field stores the starting index of the partition. third filed stores the end index of the partition. if a partition is freed at some point of time then that partition will be merged with its adjacent free partition without doing any extra effort. there are some points which need to be focused while using this approach. the os must be very clear about the location of the new node which is to be added in the linked list. however, adding the node according to the increasing order of starting index is suggestible. using a doubly linked list will make some positive effects on the performance due to the fact that a node in the doubly link list can also keep track of its previous node.
next topic partitioning algorithms
← prev next →
next → ← prev partitioning algorithms there are various algorithms which are implemented by the operating system in order to find out the holes in the linked list and allocate them to the processes. the explanation about each of the algorithm is given below. . first fit algorithm first fit algorithm scans the linked list and whenever it finds the first big enough hole to store a process, it stops scanning and load the process into that hole. this procedure produces two partitions. out of them, one partition will be a hole while the other partition will store the process. first fit algorithm maintains the linked list according to the increasing order of starting index. this is the simplest to implement among all the algorithms and produces bigger holes as compare to the other algorithms. . next fit algorithm next fit algorithm is similar to first fit algorithm except the fact that, next fit scans the linked list from the node where it previously allocated a hole. next fit doesn't scan the whole list, it starts scanning the list from the next node. the idea behind the next fit is the fact that the list has been scanned once therefore the probability of finding the hole is larger in the remaining part of the list. experiments over the algorithm have shown that the next fit is not better then the first fit. so it is not being used these days in most of the cases. . best fit algorithm the best fit algorithm tries to find out the smallest hole possible in the list that can accommodate the size requirement of the process. using best fit has some disadvantages. . it is slower because it scans the entire list every time and tries to find out the smallest hole which can satisfy the requirement the process. due to the fact that the difference between the whole size and the process size is very small, the holes produced will be as small as it cannot be used to load any process and therefore it remains useless.
despite of the fact that the name of the algorithm is best fit, it is not the best algorithm among all. . worst fit algorithm the worst fit algorithm scans the entire list every time and tries to find out the biggest hole in the list which can fulfill the requirement of the process. despite of the fact that this algorithm produces the larger holes to load the other processes, this is not the better approach due to the fact that it is slower because it searches the entire list every time again and again. . quick fit algorithm the quick fit algorithm suggestsmaintaining the different lists of frequently used sizes. although, it is not practically suggestible because the procedure takes so much time to create the different lists and then expending the holes to load a process. the first fit algorithm is the best algorithm among all because it takes lesser time compare to the other algorithms. it produces bigger holes that can be used to load other processes later on. it is easiest to implement.
next topic gate question on best fit and first fit
← prev next →
next → ← prev gate question on best fit and first fit from the gate point of view, numerical on best fit and first fit are being asked frequently in  mark. let's have a look on the one given as below. q. process requests are given as;  k ,  k ,  k ,  k
determine the algorithm which can optimally satisfy this requirement. first fit algorithm best fit algorithm neither of the two both of them in the question, there are five partitions in the memory.  partitions are having processes inside them and two partitions are holes. our task is to check the algorithm which can satisfy the request optimally. using first fit algorithm let's see, how first fit algorithm works on this problem. .  k requirement the algorithm scans the list until it gets first hole which should be big enough to satisfy the request of  k. it gets the space in the second partition which is free hence it allocates  k out of  k to the process and the remaining  k is produced as hole.
.  k requirement the  k requirement can be fulfilled by allocating the third partition which is  k in size to the process. no free space is produced as free space.
.  k requirement  k requirement can be fulfilled by using the fifth partition of  k size. out of  k,  k will be allocated and remaining  k will be there as a hole.
.  k requirement since we are having a  k free partition hence we can allocate that much space to the process which is demanding just  k space.
using first fit algorithm, we have fulfilled the entire request optimally and no useless space is remaining. let's see, how best fit algorithm performs for the problem. using best fit algorithm .  k requirement to allocate  k space using best fit approach, need to scan the whole list and then we find that a  k partition is free and the smallest among all, which can accommodate the need of the process. therefore  k out of those  k free partition is allocated to the process and the remaining o k is produced as a hole.
.  k requirement to satisfy this need, we will again scan the whole list and then find the  k space is free which the exact match of the need is. therefore, it will be allocated for the process.
.  k requirement  k need is close enough to the  k space. the algorithm scans the whole list and then allocates  k out of  k from the th free partition.
.  k requirement  k requirement will get the space of  k from the th free partition but the algorithm will scan the whole list in the process of taking this decision.
by following both of the algorithms, we have noticed that both the algorithms perform similar to most of the extant in this case. both can satisfy the need of the processes but however, the best fit algorithm scans the list again and again which takes lot of time. therefore, if you ask me that which algorithm performs in more optimal way then it will be first fit algorithm for sure. therefore, the answer in this case is a.
next topic need for paging
← prev next →
next → ← prev need for paging disadvantage of dynamic partitioning the main disadvantage of dynamic partitioning is external fragmentation. although, this can be removed by compaction but as we have discussed earlier, the compaction makes the system inefficient. we need to find out a mechanism which can load the processes in the partitions in a more optimal way. let us discuss a dynamic and flexible mechanism called paging. need for paging lets consider a process p of size  mb and the main memory which is divided into three partitions. out of the three partitions, two partitions are holes of size  mb each. p needs  mb space in the main memory to be loaded. we have two holes of  mb each but they are not contiguous. although, there is  mb space available in the main memory in the form of those holes but that remains useless until it become contiguous. this is a serious problem to address. we need to have some kind of mechanism which can store one process at different locations of the memory. the idea behind paging is to divide the process in pages so that, we can store them in the memory at different holes. we will discuss paging with the examples in the next sections.
next topic paging with example
← prev next →
next → ← prev paging with example in operating systems, paging is a storage mechanism used to retrieve processes from the secondary storage into the main memory in the form of pages. the main idea behind the paging is to divide each process in the form of pages. the main memory will also be divided in the form of frames. one page of the process is to be stored in one of the frames of the memory. the pages can be stored at the different locations of the memory but the priority is always to find the contiguous frames or holes. pages of the process are brought into the main memory only when they are required otherwise they reside in the secondary storage. different operating system defines different frame sizes. the sizes of each frame must be equal. considering the fact that the pages are mapped to the frames in paging, page size needs to be as same as frame size.
example let us consider the main memory size  kb and frame size is  kb therefore the main memory will be divided into the collection of  frames of  kb each. there are  processes in the system that is p, p, p and p of  kb each. each process is divided into pages of  kb each so that one page can be stored in one frame. initially, all the frames are empty therefore pages of the processes will get stored in the contiguous way. frames, pages and the mapping between the two is shown in the image below.
let us consider that, p and p are moved to waiting state after some time. now,  frames become empty and therefore other pages can be loaded in that empty place. the process p of size  kb ( pages) is waiting inside the ready queue. given the fact that, we have  non contiguous frames available in the memory and paging provides the flexibility of storing the process at the different places. therefore, we can load the pages of process p in the place of p and p.
memory management unit the purpose of memory management unit (mmu) is to convert the logical address into the physical address. the logical address is the address generated by the cpu for every page while the physical address is the actual address of the frame where each page will be stored. when a page is to be accessed by the cpu by using the logical address, the operating system needs to obtain the physical address to access that page physically. the logical address has two parts. page number offset memory management unit of os needs to convert the page number to the frame number. example considering the above image, let's say that the cpu demands th word of th page of process p. since the page number  of process p gets stored at frame number  therefore the th word of th frame will be returned as the physical address.
next topic basics of binary addresses
← prev next →
basics of binary addresses
computer system assigns the binary addresses to the memory locations. however, the system uses amount of bits to address a memory location.
using  bit, we can address two memory locations. using  bits we can address  and using  bits we can address  memory locations.
a pattern can be identified in the mapping between the number of bits in the address and the range of the memory locations.
we know,
using  bit we can represent ^ i.e  memory locations. using  bits, we can represent ^ i.e.  memory locations. using  bits, we can represent ^ i.e.  memory locations. therefore, if we generalize this, using n bits, we can assign ^n memory locations. n bits of address →  ^ n memory locations
these n bits can be divided into two parts, that are, k bits and (n-k) bits.
physical and logical address space
physical address space
physical address space in a system can be defined as the size of the main memory. it is really important to compare the process size with the physical address space. the process size must be less than the physical address space.
physical address space = size of the main memory
if, physical address space =  kb =  ^  kb =  ^  x  ^  bytes =  ^  bytes
let us consider,
word size =  bytes =  ^  bytes
hence,
physical address space (in words) = ( ^ ) / ( ^ ) =  ^  words
therefore,
physical address =  bits
in general,
if, physical address space = n words
then, physical address = log  n
logical address space
logical address space can be defined as the size of the process. the size of the process should be less enough so that it can reside in the main memory.
let's say,
logical address space =  mb = ( ^  x  ^ ) bytes =  ^  bytes
word size =  bytes =  ^  bytes
logical address space (in words) = ( ^ ) / ( ^ ) =  ^  words
logical address =  bits
in general,
if, logical address space = l words
then, logical address = log  l bits
what is a word?
page table
page table is a data structure used by the virtual memory system to store the mapping between logical addresses and physical addresses.
logical addresses are generated by the cpu for the pages of the processes therefore they are generally used by the processes.
physical addresses are the actual frame address of the memory. they are generally used by the hardware or more specifically by ram subsystems.
the image given below considers,
physical address space = m words
logical address space = l words
page size = p words
physical address = log  m = m bits
logical address = log  l = l bits
page offset = log  p = p bits
the cpu always accesses the processes through their logical addresses. however, the main memory recognizes physical address only.
in this situation, a unit named as memory management unit comes into the picture. it converts the page number of the logical address to the frame number of the physical address. the offset remains same in both the addresses.
to perform this task, memory management unit needs a special kind of mapping which is done by page table. the page table stores all the frame numbers corresponding to the page numbers of the page table.
in other words, the page table maps the page number to its actual location (frame number) in the memory.
in the image given below shows, how the required word of the frame is accessed with the help of offset.
mapping from page table to main memory
in operating systems, there is always a requirement of mapping from logical address to the physical address. however, this process involves various steps which are defined as follows.
. generation of logical address
cpu generates logical address for each page of the process. this contains two parts: page number and offset.
. scaling
to determine the actual page number of the process, cpu stores the page table base in a special register. each time the address is generated, the value of the page table base is added to the page number to get the actual location of the page entry in the table. this process is called scaling.
. generation of physical address
the frame number of the desired page is determined by its entry in the page table. a physical address is generated which also contains two parts : frame number and offset. the offset will be similar to the offset of the logical address therefore it will be copied from the logical address.
. getting actual frame number
the frame number and the offset from the physical address is mapped to the main memory in order to get the actual word address.
next → ← prev page table entry along with page frame number, the page table also contains some of the bits representing the extra information regarding the page. let's see what the each bit represents about the page. . caching disabled sometimes, there are differences between the information closest to the cpu and the information closest to the user. operating system always wants cpu to access user's data as soon as possible. cpu accesses cache which can be inaccurate in some of the cases, therefore, os can disable the cache for the required pages. this bit is set to  if the cache is disabled. . referenced there are variouspage replacement algorithms which will be covered later in this tutorial. this bit is set to  if the page is referred in the last clock cycle otherwise it remains . . modified this bit will be set if the page has been modified otherwise it remains . . protection the protection field represents the protection level which is applied on the page. it can be read only or read & write or execute. we need to remember that it is not a bit rather it is a field which contains many bits. . present/absent in the concept of demand paging, all the pages doesn't need to be present in the main memory therefore, for all the pages that are present in the main memory, this bit will be set to  and the bit will be  for all the pages which are absent. if some page is not present in the main memory then it is called page fault.
next topic page table size
← prev next →
next → ← prev size of the page table however, the part of the process which is being executed by the cpu must be present in the main memory during that time period. the page table must also be present in the main memory all the time because it has the entry for all the pages. the size of the page table depends upon the number of entries in the table and the bytes stored in one entry. let's consider, logical address =  bits logical address space =  ^  bytes let's say, page size =  kb =  ^  bytes page offset =  number of bits in a page = logical address - page offset =  -  =  bits number of pages =  ^  =  x  x  ^  =  kb let's say, page table entry =  byte therefore, the size of the page table =  kb x  byte =  kb here we are lucky enough to get the page table size equal to the frame size. now, the page table will be simply stored in one of the frames of the main memory. the cpu maintains a register which contains the base address of that frame, every page number from the logical address will first be added to that base address so that we can access the actual location of the word being asked. however, in some cases, the page table size and the frame size might not be same. in those cases, the page table is considered as the collection of frames and will be stored in the different frames.
next topic finding optimal page size
← prev next →
next → ← prev finding optimal page size we have seen that the bigger page table size cause an extra overhead because we have to divide that table into the pages and then store that into the main memory. our concern must be about executing processes not on the execution of page table. page table provides a support for the execution of the process. the larger the page table, the higher the overhead. we know that, page table size = number of page entries in page table x size of one page entry let's consider an example, virtual address space =  gb =  x  ^  bytes page size =  kb =  x  ^  bytes number of pages in page table = ( x  ^ )/( x  ^ ) =  m pages there will be  million pages which is quite big number. however, try to make page size larger, say  mb. then, number of pages in page table = ( x  ^ )/( x  ^ ) =  k pages. if we compare the two scenarios, we can find out that the page table size is anti proportional to page size. in paging, there is always wastage on the last page. if the virtual address space is not a multiple of page size, then there will be some bytes remaining and we have to assign a full page to those many bytes. this is simply a overhead. let's consider, page size =  kb virtual address space =  kb then number of pages =  kb /  kb the number of pages will be  although the th page will only contain  byte and the remaining page will be wasted. in general, if page size = p bytes entry size = e bytes virtual address space = s bytes then, overhead o = (s/p) x e + (p/) on an average, the wasted number of pages in a virtual space is p/(the half of total number of pages). for, the minimal overhead, ∂o/∂p =  -s/(p^) + ½ =  p = √ (.s.e) bytes hence, if the page size √(.s.e) bytes then the overhead will be minimal.
next topic virtual memory
← prev next →
next → ← prev virtual memory virtual memory is a storage scheme that provides user an illusion of having a very big main memory. this is done by treating a part of secondary memory as the main memory. in this scheme, user can load the bigger size processes than the available main memory by having the illusion that the memory is available to load the process. instead of loading one big process in the main memory, the operating system loads the different parts of more than one process in the main memory. by doing this, the degree of multiprogramming will be increased and therefore, the cpu utilization will also be increased. how virtual memory works? in modern word, virtual memory has become quite common these days. in this scheme, whenever some pages needs to be loaded in the main memory for the execution and the memory is not available for those many pages, then in that case, instead of stopping the pages from entering in the main memory, the os search for the ram area that are least used in the recent times or that are not referenced and copy that into the secondary memory to make the space for the new pages in the main memory. since all this procedure happens automatically, therefore it makes the computer feel like it is having the unlimited ram. demand paging demand paging is a popular method of virtual memory management. in demand paging, the pages of a process which are least used, get stored in the secondary memory. a page is copied to the main memory when its demand is made or page fault occurs. there are various page replacement algorithms which are used to determine the pages which will be replaced. we will discuss each one of them later in detail. snapshot of a virtual memory management system let us assume  processes, p and p, contains  pages each. each page size is  kb. the main memory contains  frame of  kb each. the os resides in the first two partitions. in the third partition, st page of p is stored and the other frames are also shown as filled with the different pages of processes in the main memory. the page tables of both the pages are  kb size each and therefore they can be fit in one frame each. the page tables of both the processes contain various information that is also shown in the image. the cpu contains a register which contains the base address of page table that is  in the case of p and  in the case of p. this page table base address will be added to the page number of the logical address when it comes to accessing the actual corresponding entry.
advantages of virtual memory the degree of multiprogramming will be increased. user can run large application with less real ram. there is no need to buy more memory rams. disadvantages of virtual memory the system becomes slower since swapping takes time. it takes more time in switching between applications. the user will have the lesser hard disk space for its use.
next topic translation look aside buffer
← prev next →
next → ← prev translation look aside buffer drawbacks of paging size of page table can be very big and therefore it wastes main memory. cpu will take more time to read a single word from the main memory. how to decrease the page table size the page table size can be decreased by increasing the page size but it will cause internal fragmentation and there will also be page wastage. other way is to use multilevel paging but that increases the effective access time therefore this is not a practical approach. how to decrease the effective access time cpu can use a register having the page table stored inside it so that the access time to access page table can become quite less but the register are not cheaper and they are very small in compare to the page table size therefore, this is also not a practical approach. to overcome these many drawbacks in paging, we have to look for a memory that is cheaper than the register and faster than the main memory so that the time taken by the cpu to access page table again and again can be reduced and it can only focus to access the actual word. locality of reference in operating systems, the concept of locality of reference states that, instead of loading the entire process in the main memory, os can load only those number of pages in the main memory that are frequently accessed by the cpu and along with that, the os can also load only those page table entries which are corresponding to those many pages. translation look aside buffer (tlb) a translation look aside buffer can be defined as a memory cache which can be used to reduce the time taken to access the page table again and again. it is a memory cache which is closer to the cpu and the time taken by cpu to access tlb is lesser then that taken to access main memory. in other words, we can say that tlb is faster and smaller than the main memory but cheaper and bigger than the register. tlb follows the concept of locality of reference which means that it contains only the entries of those many pages that are frequently accessed by the cpu.
in translation look aside buffers, there are tags and keys with the help of which, the mapping is done. tlb hit is a condition where the desired entry is found in translation look aside buffer. if this happens then the cpu simply access the actual location in the main memory. however, if the entry is not found in tlb (tlb miss) then cpu has to access page table in the main memory and then access the actual frame in the main memory. therefore, in the case of tlb hit, the effective access time will be lesser as compare to the case of tlb miss. if the probability of tlb hit is p% (tlb hit rate) then the probability of tlb miss (tlb miss rate) will be (-p) %. therefore, the effective access time can be defined as; eat = p (t + m) + ( - p) (t + k.m + m) where, p → tlb hit rate, t → time taken to access tlb, m → time taken to access main memory k = , if the single level paging has been implemented. by the formula, we come to know that effective access time will be decreased if the tlb hit rate is increased. effective access time will be increased in the case of multilevel paging.
next topic gate  question on tlb
← prev next →
gate question on tlb
gate | gate-cs--(set-)
consider a paging hardware with a tlb. assume that the entire page table and all the pages are in the physical memory. it takes  milliseconds to search the tlb and  milliseconds to access the physical memory. if the tlb hit ratio is ., the effective memory access time (in milliseconds) is _________.
a. 
b. 
c. 
d. 
given,
tlb hit ratio = . therefore, tlb miss ratio = . time taken to access tlb (t) =  ms time taken to access main memory (m) =  ms
effective access time (eat) = . (  +  ) + . (  +  +  ) =  x . + . x  = 
hence, the right answer is option b.
next → ← prev demand paging according to the concept of virtual memory, in order to execute some process, only a part of the process needs to be present in the main memory which means that only a few pages will only be present in the main memory at any time. however, deciding, which pages need to be kept in the main memory and which need to be kept in the secondary memory, is going to be difficult because we cannot say in advance that a process will require a particular page at particular time. therefore, to overcome this problem, there is a concept called demand paging is introduced. it suggests keeping all pages of the frames in the secondary memory until they are required. in other words, it says that do not load any page in the main memory until it is required. whenever any page is referred for the first time in the main memory, then that page will be found in the secondary memory. after that, it may or may not be present in the main memory depending upon the page replacement algorithm which will be covered later in this tutorial. what is a page fault? if the referred page is not present in the main memory then there will be a miss and the concept is called page miss or page fault. the cpu has to access the missed page from the secondary memory. if the number of page fault is very high then the effective access time of the system will become very high. what is thrashing? if the number of page faults is equal to the number of referred pages or the number of page faults are so high so that the cpu remains busy in just reading the pages from the secondary memory then the effective access time will be the time taken by the cpu to read one word from the secondary memory and it will be so high. the concept is called thrashing. if the page fault rate is pf %, the time taken in getting a page from the secondary memory and again restarting is s (service time) and the memory access time is ma then the effective access time can be given as; eat = pf x s + ( - pf) x (ma)
next topic inverted page table
← prev next →
next → ← prev inverted page table inverted page table is the global page table which is maintained by the operating system for all the processes. in inverted page table, the number of entries is equal to the number of frames in the main memory. it can be used to overcome the drawbacks of page table. there is always a space reserved for the page regardless of the fact that whether it is present in the main memory or not. however, this is simply the wastage of the memory if the page is not present.
we can save this wastage by just inverting the page table. we can save the details only for the pages which are present in the main memory. frames are the indices and the information saved inside the block will be process id and page number.
next topic page replacement algorithms
← prev next →
next → ← prev page replacement algorithms the page replacement algorithm decides which memory page is to be replaced. the process of replacement is sometimes called swap out or write to disk. page replacement is done when the requested page is not found in the main memory (page fault).
there are two main aspects of virtual memory, frame allocation and page replacement. it is very important to have the optimal frame allocation and page replacement algorithm. frame allocation is all about how many frames are to be allocated to the process while the page replacement is all about determining the page number which needs to be replaced in order to make space for the requested page. what if the algorithm is not optimal? . if the number of frames which are allocated to a process is not sufficient or accurate then there can be a problem of thrashing. due to the lack of frames, most of the pages will be residing in the main memory and therefore more page faults will occur. however, if os allocates more frames to the process then there can be internal fragmentation. . if the page replacement algorithm is not optimal then there will also be the problem of thrashing. if the number of pages that are replaced by the requested pages will be referred in the near future then there will be more number of swap-in and swap-out and therefore the os has to perform more replacements then usual which causes performance deficiency. therefore, the task of an optimal page replacement algorithm is to choose the page which can limit the thrashing. types of page replacement algorithms there are various page replacement algorithms. each algorithm has a different method by which the pages can be replaced. optimal page replacement algorithm → this algorithms replaces the page which will not be referred for so long in future. although it can not be practically implementable but it can be used as a benchmark. other algorithms are compared to this in terms of optimality. least recent used (lru) page replacement algorithm → this algorithm replaces the page which has not been referred for a long time. this algorithm is just opposite to the optimal page replacement algorithm. in this, we look at the past instead of staring at future. fifo → in this algorithm, a queue is maintained. the page which is assigned the frame first will be replaced first. in other words, the page which resides at the rare end of the queue will be replaced on the every page fault.
next topic gate  question on lru and fifo
← prev next →
next → ← prev gate  question on lru and fifo q. consider a main memory with five page frames and the following sequence of page 
next → ← prev numerical on optimal, lru and fifo q. consider a reference string: , , , , , , , , , . the number of frames in the memory is . find out the number of page faults respective to: optimal page replacement algorithm fifo page replacement algorithm lru page replacement algorithm optimal page replacement algorithm
number of page faults in optimal page replacement algorithm =  lru page replacement algorithm
number of page faults in lru =  fifo page replacement algorithm
number of page faults in fifo = 
next topic beladys anamoly
← prev next →
belady'sanomaly
in the case of lru and optimal page replacement algorithms, it is seen that the number of page faults will be reduced if we increase the number of frames. however, balady found that, in fifo page replacement algorithm, the number of page faults will get increased with the increment in number of frames.
this is the strange behavior shown by fifo algorithm in some of the cases. this is an anomaly called as belady'sanomaly.
let's examine such example :
the reference string is given as            . let's analyze the behavior of fifo algorithm in two cases.
case : number of frames = 
request             frame            frame             frame              miss/hit miss miss miss miss miss miss miss hit hit miss miss hit
number of page faults = 
case : number of frames = 
request             frame           frame            frame             frame              miss/hit miss miss miss miss hit hit miss miss miss miss miss miss
number of page faults = 
therefore, in this example, the number of page faults is increasing by increasing the number of frames hence this suffers from belady'sanomaly.
next → ← prev segmentation in operating systems, segmentation is a memory management technique in which, the memory is divided into the variable size parts. each part is known as segment which can be allocated to a process. the details about each segment are stored in a table called as segment table. segment table is stored in one (or many) of the segments. segment table contains mainly two information about segment: base: it is the base address of the segment limit: it is the length of the segment. why segmentation is required? till now, we were using paging as our main memory management technique. paging is more close to operating system rather than the user. it divides all the process into the form of pages regardless of the fact that a process can have some relative parts of functions which needs to be loaded in the same page. operating system doesn't care about the user's view of the process. it may divide the same function into different pages and those pages may or may not be loaded at the same time into the memory. it decreases the efficiency of the system. it is better to have segmentation which divides the process into the segments. each segment contain same type of functions such as main function can be included in one segment and the library functions can be included in the other segment, translation of logical address into physical address by segment table cpu generates a logical address which contains two parts: segment number offset the segment number is mapped to the segment table. the limit of the respective segment is compared with the offset. if the offset is less than the limit then the address is valid otherwise it throws an error as the address is invalid. in the case of valid address, the base address of the segment is added to the offset to get the physical address of actual word in the main memory.
advantages of segmentation no internal fragmentation average segment size is larger than the actual page size. less overhead it is easier to relocate segments than entire address space. the segment table is of lesser size as compare to the page table in paging. disadvantages it can have external fragmentation. it is difficult to allocate contiguous memory to variable sized partition. costly memory management algorithms.
next topic paging vs segmentation
← prev next →
next → ← prev paging vs segmentation sr no. paging segmentation  non-contiguous memory allocation non-contiguous memory allocation  paging divides program into fixed size pages. segmentation divides program into variable size segments.  os is responsible compiler is responsible.  paging is faster than segmentation segmentation is slower than paging  paging is closer to operating system segmentation is closer to user  it suffers from internal fragmentation it suffers from external fragmentation  there is no external fragmentation there is no external fragmentation  logical address is divided into page number and page offset logical address is divided into segment number and segment offset  page table is used to maintain the page information. segment table maintains the segment information  page table entry has the frame number and some flag bits to represent details about pages. segment table entry has the base address of the segment and some protection bits for the segments.
next topic segmented paging
← prev next →
next → ← prev segmented paging pure segmentation is not very popular and not being used in many of the operating systems. however, segmentation can be combined with paging to get the best features out of both the techniques. in segmented paging, the main memory is divided into variable size segments which are further divided into fixed size pages. pages are smaller than segments. each segment has a page table which means every program has multiple page tables. the logical address is represented as segment number (base address), page number and page offset. segment number → it points to the appropriate segment number. page number → it points to the exact page within the segment page offset → used as an offset within the page frame each page table contains the various information about every page of the segment. the segment table contains the information about every segment. each segment table entry points to a page table entry and every page table entry is mapped to one of the page within a segment.
translation of logical address to physical address the cpu generates a logical address which is divided into two parts: segment number and segment offset. the segment offset must be less than the segment limit. offset is further divided into page number and page offset. to map the exact page number in the page table, the page number is added into the page table base. the actual frame number with the page offset is mapped to the main memory to get the desired word in the page of the certain segment of the process.
advantages of segmented paging it reduces memory usage. page table size is limited by the segment size. segment table has only one entry corresponding to one actual segment. external fragmentation is not there. it simplifies memory allocation. disadvantages of segmented paging internal fragmentation will be there. the complexity level will be much higher as compare to paging. page tables need to be contiguously stored in the memory.
next topic attributes of the file
← prev next →
next → ← prev what is a file ? a file can be defined as a data structure which stores the sequence of records. files are stored in a file system, which may exist on a disk or in the main memory. files can be simple (plain text) or complex (specially-formatted). the collection of files is known as directory. the collection of directories at the different levels, is known as file system.
attributes of the file .name every file carries a name by which the file is recognized in the file system. one directory cannot have two files with the same name. .identifier along with the name, each file has its own extension which identifies the type of the file. for example, a text file has the extension .txt, a video file can have the extension .mp. .type in a file system, the files are classified in different types such as video files, audio files, text files, executable files, etc. .location in the file system, there are several locations on which, the files can be stored. each file carries its location as its attribute. .size the size of the file is one of its most important attribute. by size of the file, we mean the number of bytes acquired by the file in the memory. .protection the admin of the computer may want the different protections for the different files. therefore each file carries its own set of permissions to the different group of users. .time and date every file carries a time stamp which contains the time and date on which the file is last modified.
next topic operations on the file
← prev next →
next → ← prev operations on the file there are various operations which can be implemented on a file. we will see all of them in detail. .create creation of the file is the most important operation on the file. different types of files are created by different methods for example text editors are used to create a text file, word processors are used to create a word file and image editors are used to create the image files. .write writing the file is different from creating the file. the os maintains a write pointer for every file which points to the position in the file from which, the data needs to be written. .read every file is opened in three different modes : read, write and append. a read pointer is maintained by the os, pointing to the position up to which, the data has been read. .re-position re-positioning is simply moving the file pointers forward or backward depending upon the user's requirement. it is also called as seeking. .delete deleting the file will not only delete all the data stored inside the file, it also deletes all the attributes of the file. the space which is allocated to the file will now become available and can be allocated to the other files. .truncate truncating is simply deleting the file except deleting attributes. the file is not completely deleted although the information stored inside the file get replaced.
next topic file access methods
← prev next →
next → ← prev file access methods let's look at various ways to access files stored in secondary memory. sequential access
most of the operating systems access the file sequentially. in other words, we can say that most of the files need to be accessed sequentially by the operating system. in sequential access, the os read the file word by word. a pointer is maintained which initially points to the base address of the file. if the user wants to read first word of the file then the pointer provides that word to the user and increases its value by  word. this process continues till the end of the file. modern word systems do provide the concept of direct access and indexed access but the most used method is sequential access due to the fact that most of the files such as text files, audio files, video files, etc need to be sequentially accessed. direct access the direct access is mostly required in the case of database systems. in most of the cases, we need filtered information from the database. the sequential access can be very slow and inefficient in such cases. suppose every block of the storage stores  records and we know that the record we needed is stored in th block. in that case, the sequential access will not be implemented because it will traverse all the blocks in order to access the needed record. direct access will give the required result despite of the fact that the operating system has to perform some complex tasks such as determining the desired block number. however, that is generally implemented in database applications.
indexed access if a file can be sorted on any of the filed then an index can be assigned to a group of certain records. however, a particular record can be accessed by its index. the index is nothing but the address of a record in the file. in index accessing, searching in a large database became very quick and easy but we need to have some extra space in the memory to store the index value.
next topic directory structure
← prev next →
directory structure
what is a directory?
directory can be defined as the listing of the related files on the disk. the directory may store some or the entire file attributes.
to get the benefit of different file systems on the different operating systems, a hard disk can be divided into the number of partitions of different sizes. the partitions are also called volumes or mini disks.
each partition must have at least one directory in which, all the files of the partition can be listed. a directory entry is maintained for each file in the directory which stores all the information related to that file.
a directory can be viewed as a file which contains the meta data of the bunch of files.
every directory supports a number of common operations on the file:
file creation search for the file file deletion renaming the file traversing files listing of files
next → ← prev single level directory the simplest method is to have one big list of all the files on the disk. the entire system will contain only one directory which is supposed to mention all the files present in the file system. the directory contains one entry per each file present on the file system.
this type of directories can be used for a simple system. advantages implementation is very simple. if the sizes of the files are very small then the searching becomes faster. file creation, searching, deletion is very simple since we have only one directory. disadvantages we cannot have two files with the same name. the directory may be very big therefore searching for a file may take so much time. protection cannot be implemented for multiple users. there are no ways to group same kind of files. choosing the unique name for every file is a bit complex and limits the number of files in the system because most of the operating system limits the number of characters used to construct the file name.
next topic two level directory
← prev next →
next → ← prev two level directory in two level directory systems, we can create a separate directory for each user. there is one master directory which contains separate directories dedicated to each user. for each user, there is a different directory present at the second level, containing group of user's file. the system doesn't let a user to enter in the other user's directory without permission.
characteristics of two level directory system each files has a path name as /user-name/directory-name/ different users can have the same file name. searching becomes more efficient as only one user's list needs to be traversed. the same kind of files cannot be grouped into a single directory for a particular user. every operating system maintains a variable as pwd which contains the present directory name (present user name) so that the searching can be done appropriately.
next topic tree structured directory
← prev next →
next → ← prev tree structured directory in tree structured directory system, any directory entry can either be a file or sub directory. tree structured directory system overcomes the drawbacks of two level directory system. the similar kind of files can now be grouped in one directory. each user has its own directory and it cannot enter in the other user's directory. however, the user has the permission to read the root's data but he cannot write or modify this. only administrator of the system has the complete access of root directory. searching is more efficient in this directory structure. the concept of current working directory is used. a file can be accessed by two types of path, either relative or absolute. absolute path is the path of the file with respect to the root directory of the system while relative path is the path with respect to the current working directory of the system. in tree structured directory systems, the user is given the privilege to create the files as well as directories.
permissions on the file and directory a tree structured directory system may consist of various levels therefore there is a set of permissions assigned to each file and directory. the permissions are r w x which are regarding reading, writing and the execution of the files or directory. the permissions are assigned to three types of users: owner, group and others. there is a identification bit which differentiate between directory and file. for a directory, it is d and for a file, it is dot (.) the following snapshot shows the permissions assigned to a file in a linux based system. initial bit d represents that it is a directory.
next topic acyclic graph directories
← prev next →
acyclic-graph structured directories
the tree structured directory system doesn't allow the same file to exist in multiple directories therefore sharing is major concern in tree structured directory system. we can provide sharing by making the directory an acyclic graph. in this system, two or more directory entry can point to the same file or sub directory. that file or sub directory is shared between the two directory entries.
these kinds of directory graphs can be made using links or aliases. we can have multiple paths for a same file. links can either be symbolic (logical) or hard link (physical).
if a file gets deleted in acyclic graph structured directory system, then
. in the case of soft link, the file just gets deleted and we are left with a dangling pointer.
. in the case of hard link, the actual file will be deleted only if all the 
next → ← prev file systems file system is the part of the operating system which is responsible for file management. it provides a mechanism to store the data and access to the file contents including data and programs. some operating systems treats everything as a file for example ubuntu. the file system takes care of the following issues file structure
we have seen various data structures in which the file can be stored. the task of the file system is to maintain an optimal file structure. recovering free space
whenever a file gets deleted from the hard disk, there is a free space created in the disk. there can be many such spaces which need to be recovered in order to reallocate them to other files. disk space assignment to the files
the major concern about the file is deciding where to store the files on the hard disk. there are various disks scheduling algorithm which will be covered later in this tutorial. tracking data location a file may or may not be stored within only one block. it can be stored in the non contiguous blocks on the disk. we need to keep track of all the blocks on which the part of the files reside.
next topic file system structure
← prev next →
next → ← prev file system structure file system provide efficient access to the disk by allowing data to be stored, located and retrieved in a convenient way. a file system must be able to store the file, locate the file and retrieve the file. most of the operating systems use layering approach for every task including file systems. every layer of the file system is responsible for some activities. the image shown below, elaborates how the file system is divided in different layers, and also the functionality of each layer.
when an application program asks for a file, the first request is directed to the logical file system. the logical file system contains the meta data of the file and directory structure. if the application program doesn't have the required permissions of the file then this layer will throw an error. logical file systems also verify the path to the file.
generally, files are divided into various logical blocks. files are to be stored in the hard disk and to be retrieved from the hard disk. hard disk is divided into various tracks and sectors. therefore, in order to store and retrieve the files, the logical blocks need to be mapped to physical blocks. this mapping is done by file organization module. it is also responsible for free space management.
once file organization module decided which physical block the application program needs, it passes this information to basic file system. the basic file system is responsible for issuing the commands to i/o control in order to fetch those blocks.
next topic master boot record
← prev next →
next → ← prev master boot record (mbr) master boot record is the information present in the first sector of any hard disk. it contains the information regarding how and where the operating system is located in the hard disk so that it can be booted in the ram. mbr is sometimes called master partition table because it includes a partition table which locates every partition in the hard disk. master boot record (mbr) also includes a program which reads the boot sector record of the partition that contains operating system.
next topic on disk data structures
← prev next →
there are various on disk data structures that are used to implement a file system. this structure may vary depending upon the operating system.
boot control block
boot control block contains all the information which is needed to boot an operating system from that volume. it is called boot block in unix file system. in ntfs, it is called the partition boot sector.
volume control block
volume control block all the information regarding that volume such as number of blocks, size of each block, partition table, pointers to free blocks and free fcb blocks. in unix file system, it is known as super block. in ntfs, this information is stored inside master file table.
directory structure (per file system)
a directory structure (per file system) contains file names and pointers to corresponding fcbs. in unix, it includes inode numbers associated to file names.
file control block
file control block contains all the details about the file such as ownership details, permission details, file size,etc. in ufs, this detail is stored in inode. in ntfs, this information is stored inside master file table as a relational database structure. a typical file control block is shown in the image below.
next → ← prev in memory data structure till now, we have discussed the data structures that are required to be present on the hard disk in order to implement file systems. here, we will discuss the data structures required to be present in memory in order to implement the file system. the in-memory data structures are used for file system management as well as performance improvement via caching. this information is loaded on the mount time and discarded on ejection. in-memory mount table in-memory mount table contains the list of all the devices which are being mounted to the system. whenever the connection is maintained to a device, its entry will be done in the mount table. in-memory directory structure cache this is the list of directory which is recently accessed by the cpu. the directories present in the list can also be accessed in the near future so it will be better to store them temporally in cache. system-wide open file table this is the list of all the open files in the system at a particular time. whenever the user open any file for reading or writing, the entry will be made in this open file table. per process open file table it is the list of open files subjected to every process. since there is already a list which is there for every open file in the system thereforeit only contains pointers to the appropriate entry in the system wide table.
next topic directory implementation
← prev next →
next → ← prev directory implementation there is the number of algorithms by using which, the directories can be implemented. however, the selection of an appropriate directory implementation algorithm may significantly affect the performance of the system. the directory implementation algorithms are classified according to the data structure they are using. there are mainly two algorithms which are used in these days. . linear list in this algorithm, all the files in a directory are maintained as singly lined list. each file contains the pointers to the data blocks which are assigned to it and the next file in the directory. characteristics when a new file is created, then the entire list is checked whether the new file name is matching to a existing file name or not. in case, it doesn't exist, the file can be created at the beginning or at the end. therefore, searching for a unique name is a big concern because traversing the whole list takes time. the list needs to be traversed in case of every operation (creation, deletion, updating, etc) on the files therefore the systems become inefficient.
. hash table to overcome the drawbacks of singly linked list implementation of directories, there is an alternative approach that is hash table. this approach suggests to use hash table along with the linked lists. a key-value pair for each file in the directory gets generated and stored in the hash table. the key can be determined by applying the hash function on the file name while the key points to the corresponding file stored in the directory. now, searching becomes efficient due to the fact that now, entire list will not be searched on every operating. only hash table entries are checked using the key and if an entry found then the corresponding file will be fetched using the value.
next topic allocation methods
← prev next →
next → ← prev allocation methods there are various methods which can be used to allocate disk space to the files. selection of an appropriate allocation method will significantly affect the performance and efficiency of the system. allocation method provides a way in which the disk will be utilized and the files will be accessed. there are following methods which can be used for allocation. contiguous allocation. extents linked allocation clustering fat indexed allocation linked indexed allocation multilevel indexed allocation inode we will discuss three of the most used methods in detail.
next topic contiguous allocation
← prev next →
next → ← prev contiguous allocation if the blocks are allocated to the file in such a way that all the logical blocks of the file get the contiguous physical block in the hard disk then such allocation scheme is known as contiguous allocation. in the image shown below, there are three files in the directory. the starting block and the length of each file are mentioned in the table. we can check in the table that the contiguous blocks are assigned to each file as per its need.
advantages it is simple to implement. we will get excellent read performance. supports random access into files. disadvantages the disk will become fragmented. it may be difficult to have a file grow.
next topic linked list allocation
← prev next →
next → ← prev linked list allocation linked list allocation solves all problems of contiguous allocation. in linked list allocation, each file is considered as the linked list of disk blocks. however, the disks blocks allocated to a particular file need not to be contiguous on the disk. each disk block allocated to a file contains a pointer which points to the next disk block allocated to the same file.
advantages there is no external fragmentation with linked allocation. any free block can be utilized in order to satisfy the file block requests. file can continue to grow as long as the free blocks are available. directory entry will only contain the starting block address. disadvantages random access is not provided. pointers require some space in the disk blocks. any of the pointers in the linked list must not be broken otherwise the file will get corrupted. need to traverse each block.
next topic file allocation table
← prev next →
next → ← prev file allocation table the main disadvantage of linked list allocation is that the random access to a particular block is not provided. in order to access a block, we need to access all its previous blocks. file allocation table overcomes this drawback of linked list allocation. in this scheme, a file allocation table is maintained, which gathers all the disk block links. the table has one entry for each disk block and is indexed by block number. file allocation table needs to be cached in order to reduce the number of head seeks. now the head doesn't need to traverse all the disk blocks in order to access one successive block. it simply accesses the file allocation table, read the desired block entry from there and access that block. this is the way by which the random access is accomplished by using fat. it is used by ms-dos and pre-nt windows versions.
advantages uses the whole disk block for data. a bad disk block doesn't cause all successive blocks lost. random access is provided although its not too fast. only fat needs to be traversed in each file operation. disadvantages each disk block needs a fat entry. fat size may be very big depending upon the number of fat entries. number of fat entries can be reduced by increasing the block size but it will also increase internal fragmentation.
next topic indexed allocation
← prev next →
next → ← prev indexed allocation limitation of fat limitation in the existing technology causes the evolution of a new technology. till now, we have seen various allocation methods; each of them was carrying several advantages and disadvantages. file allocation table tries to solve as many problems as possible but leads to a drawback. the more the number of blocks, the more will be the size of fat. therefore, we need to allocate more space to a file allocation table. since, file allocation table needs to be cached therefore it is impossible to have as many space in cache. here we need a new technology which can solve such problems. indexed allocation scheme instead of maintaining a file allocation table of all the disk pointers, indexed allocation scheme stores all the disk pointers in one of the blocks called as indexed block. indexed block doesn't hold the file data, but it holds the pointers to all the disk blocks allocated to that particular file. directory entry will only contain the index block address.
advantages supports direct access a bad data block causes the lost of only that block. disadvantages a bad index block could cause the lost of entire file. size of a file depends upon the number of pointers, a index block can hold. having an index block for a small file is totally wastage. more pointer overhead
next topic linked index allocation
← prev next →
next → ← prev linked index allocation single level linked index allocation in index allocation, the file size depends on the size of a disk block. to allow large files, we have to link several index blocks together. in linked index allocation, small header giving the name of the file
set of the first  block addresses
pointer to another index block for the larger files, the last entry of the index block is a pointer which points to another index block. this is also called as linked schema.
advantage: it removes file size limitations disadvantage: random access becomes a bit harder multilevel index allocation in multilevel index allocation, we have various levels of indices. there are outer level index blocks which contain the pointers to the inner level index blocks and the inner level index blocks contain the pointers to the file data. the outer level index is used to find the inner level index.
the inner level index is used to find the desired data block. advantage: random access becomes better and efficient. disadvantage: access time for a file will be higher.
next topic inode
← prev next →
next → ← prev inode in unix based operating systems, each file is indexed by an inode. inode are the special disk block which is created with the creation of the file system. the number of files or directories in a file system depends on the number of inodes in the file system. an inode includes the following information attributes (permissions, time stamp, ownership details, etc) of the file a number of direct blocks which contains the pointers to first  blocks of the file. a single indirect pointer which points to an index block. if the file cannot be indexed entirely by the direct blocks then the single indirect pointer is used. a double indirect pointer which points to a disk block that is a collection of the pointers to the disk blocks which are index blocks. double index pointer is used if the file is too big to be indexed entirely by the direct blocks as well as the single indirect pointer. a triple index pointer that points to a disk block that is a collection of pointers. each of the pointers is separately pointing to a disk block which also contains a collection of pointers which are separately pointing to an index block that contains the pointers to the file blocks.
next topic free space management
← prev next →
free space management
a file system is responsible to allocate the free blocks to the file therefore it has to keep track of all the free blocks present in the disk. there are mainly two approaches by using which, the free blocks in the disk are managed.
. bit vector
in this approach, the free space list is implemented as a bit map vector. it contains the number of bits where each bit represents each block.
if the block is empty then the bit is  otherwise it is . initially all the blocks are empty therefore each bit in the bit map vector contains .
las the space allocation proceeds, the file system starts allocating blocks to the files and setting the respective bit to .
. linked list
it is another approach for free space management. this approach suggests linking together all the free blocks and keeping a pointer in the cache which points to the first free block.
therefore, all the free blocks on the disks will be linked together with a pointer. whenever a block gets allocated, its previous free block will be linked to its next free block.
disk scheduling
as we know, a process needs two type of time, cpu time and io time. for i/o, it requests the operating system to access the disk.
however, the operating system must be fare enough to satisfy each request and at the same time, operating system must maintain the efficiency and speed of process execution.
the technique that operating system uses to determine the request which is to be satisfied next is called disk scheduling.
let's discuss some important terms related to disk scheduling.
seek time
seek time is the time taken in locating the disk arm to a specified track where the read/write request will be satisfied.
rotational latency
it is the time taken by the desired sector to rotate itself to the position from where it can access the r/w heads.
transfer time
it is the time taken to transfer the data.
disk access time
disk access time is given as,
disk access time = rotational latency + seek time + transfer time
disk response time
it is the average of time spent by each request waiting for the io operation.
purpose of disk scheduling
the main purpose of disk scheduling algorithm is to select a disk request from the queue of io requests and decide the schedule when this request will be processed.
goal of disk scheduling algorithm
fairness
high throughout
minimal traveling head time
disk scheduling algorithms
the list of various disks scheduling algorithm is given below. each algorithm is carrying some advantages and disadvantages. the limitation of each algorithm leads to the evolution of a new algorithm.
fcfs scheduling algorithm
sstf (shortest seek time first) algorithm
scan scheduling
c-scan scheduling
look scheduling
c-look scheduling
fcfs scheduling algorithm
it is the simplest disk scheduling algorithm. it services the io requests in the order in which they arrive. there is no starvation in this algorithm, every request is serviced.
disadvantages
the scheme does not optimize the seek time.
the request may come from different processes therefore there is the possibility of inappropriate movement of the head.
example
consider the following disk request sequence for a disk with  tracks , , , , , , , , , , 
head pointer starting at  and moving in left direction. find the number of head movements in cylinders using fcfs scheduling.
solution
number of cylinders moved by the head
= (-)+(-)+(-)+(-)+(-)+(-)+(-)+(-)+(-)+(-)
=  +  +  +  +  +  +  +  +  + 
= 
sstf scheduling algorithm
shortest seek time first (sstf) algorithm selects the disk i/o request which requires the least disk arm movement from its current position regardless of the direction. it reduces the total seek time as compared to fcfs.
it allows the head to move to the closest track in the service queue.
disadvantages
it may cause starvation for some requests.
switching direction on the frequent basis slows the working of algorithm.
it is not the most optimal algorithm.
example
consider the following disk request sequence for a disk with  tracks
, , , , , , , , , 
head pointer starting at . find the number of head movements in cylinders using sstf scheduling.
solution:
number of cylinders =  +  +  +  +  +  +  +  +  +  = 
scan and c-scan algorithm
scan algorithm
it is also called as elevator algorithm. in this algorithm, the disk arm moves into a particular direction till the end, satisfying all the requests coming in its path,and then it turns backand moves in the reverse direction satisfying requests coming in its path.
it works in the way an elevator works, elevator moves in a direction completely till the last floor of that direction and then turns back.
example
consider the following disk request sequence for a disk with  tracks
, , , , , , , 
head pointer starting at  and moving in left direction. find the number of head movements in cylinders using scan scheduling.
number of cylinders =  +  +  +  +  +  +  +  +  = 
c-scan algorithm
in c-scan algorithm, the arm of the disk moves in a particular direction servicing requests until it reaches the last cylinder, then it jumps to the last cylinder of the opposite direction without servicing any request then it turns back and start moving in that direction servicing the remaining requests.
example
consider the following disk request sequence for a disk with  tracks
head pointer starting at  and moving in left direction. find the number of head movements in cylinders using c-scan scheduling.
no. of cylinders crossed =  +  +  +  +  +  +  +  +  +  = 
next → ← prev look scheduling it is like scan scheduling algorithm to some extant except the difference that, in this scheduling algorithm, the arm of the disk stops moving inwards (or outwards) when no more request in that direction exists. this algorithm tries to overcome the overhead of scan algorithm which forces disk arm to move in one direction till the end regardless of knowing if any request exists in the direction or not. example consider the following disk request sequence for a disk with  tracks , , , , , , ,  head pointer starting at  and moving in left direction. find the number of head movements in cylinders using look scheduling.
number of cylinders crossed =  +  +  + + +  +  +  +  =  c look scheduling c look algorithm is similar to c-scan algorithm to some extent. in this algorithm, the arm of the disk moves outwards servicing requests until it reaches the highest request cylinder, then it jumps to the lowest request cylinder without servicing any request then it again start moving outwards servicing the remaining requests. it is different from c scan algorithm in the sense that, c scan force the disk arm to move till the last cylinder regardless of knowing whether any request is to be serviced on that cylinder or not. example consider the following disk request sequence for a disk with  tracks , , , , , , ,  head pointer starting at  and moving in left direction. find the number of head movements in cylinders using c look scheduling.
number of cylinders crossed =  +  +  +  +  +  +  +  = 
next topic numerical on sstf and scan
← prev next →
next → ← prev numerical on sstf and scan question: suppose the following disk request sequence (track numbers) for a disk with  tracks is given: , , , , , ,  and . assume that the initial position of the r/w head is on track . the additional distance that will be traversed by the r/w head when the shortest seek time first (sstf) algorithm is used compared to the scan (elevator) algorithm (assuming that scan algorithm moves towards  when it starts execution) is _________ tracks (a) 
(b) 
(c) 
(d)  using sstf algorithm number of track are . initial position of r/w head is . the requests are: , , , , , ,  and 
number of crossed cylinders =  +  +  +  +  +  +  =  using scan algorithm
number of cylinders crosses =  +  +  +  +  +  +  +  +  =  therefore the answer is (a). the scan algorithm travels for  additional tracks.
next topic numerical on disk scheduling
← prev next →
next → ← prev numerical on disk scheduling algorithms q. consider a disk with  tracks and the queue has random requests from different processes in the order: , , , , , , , ,  initially arm is at . find the average seek length using fifo, sstf, scan and c-scan algorithm. solution :
next topic #
← prev next →
next → ← prev functions of operation system an operating system is a program that acts as a user-computer gui (graphical user interface). it controls the execution of all types of applications. the operating system performs the following functions in a device. instruction input/output management memory management file management processor management job priority special control program scheduling of resources and jobs security monitoring activities job accounting instruction: the operating system establishes a mutual understanding between the various instructions given by the user. input/output management: what output will come from the input given by the user, the operating system runs this program. this management involves coordinating various input and output devices. it assigns the functions of those devices where one or more applications are executed. memory management: the operating system handles the responsibility of storing any data, system programs, and user programs in memory. this function of the operating system is called memory management. file management: the operating system is helpful in making changes in the stored files and in replacing them. it also plays an important role in transferring various files to a device. processor management: the processor is the execution of a program that accomplishes the specified work in that program. it can be defined as an execution unit where a program runs. job priority: the work of job priority is creation and promotion. it determines what action should be done first in a computer system. special control program: the operating systems make automatic changes to the task through specific control programs. these programs are called special control program. scheduling of resources and jobs: the operating system prepares the list of tasks to be performed for the device of the computer system. the operating system decides which device to use for which task. this action becomes complicated when multiple tasks are to be performed simultaneously in a computer system. the scheduling programs of the operating system determine the order in which tasks are completed. it performs these tasks based on the priority of performing the tasks given by the user. it makes the tasks available based on the priority of the device. security: computer security is a very important aspect of any operating system. the reliability of an operating system is determined by how much better security it provides us. modern operating systems use a firewall for security. a firewall is a security system that monitors every activity happening in the computer and blocks that activity in case of any threat. monitoring activities: the operating system takes care of the activities of the computer system during various processes. this aborts the program if there are errors. the operating system sends instant messages to the user for any unexpected error in the input/output device. it also provides security to the system when the operating system is used in systems operated by multiple users. so that illegal users cannot get data from the system. job accounting: it keeps track of time & resources used by various jobs and users. next topic mobile os
next → ← prev mobile operating system a mobile operating system is an operating system that helps to run other application software on mobile devices. it is the same kind of software as the famous computer operating systems like linux and windows, but now they are light and simple to some extent. the operating systems found on smartphones include symbian os, iphone os, rim's blackberry, windows mobile, palm webos, android, and maemo. android, webos, and maemo are all derived from linux. the iphone os originated from bsd and nextstep, which are related to unix. it combines the beauty of computer and hand use devices. it typically contains a cellular built-in modem and sim tray for telephony and internet connections. if you buy a mobile, the manufacturer company chooses the os for that specific device. popular platforms of the mobile os . android os: the android operating system is the most popular operating system today. it is a mobile os based on the linux kernel and open-source software. the android operating system was developed by google. the first android device was launched in . . bada (samsung electronics): bada is a samsung mobile operating system that was launched in . the samsung wave was the first mobile to use the bada operating system. the bada operating system offers many mobile features, such as -d graphics, application installation, and multipoint-touch. . blackberry os: the blackberry operating system is a mobile operating system developed by research in motion (rim). this operating system was designed specifically for blackberry handheld devices. this operating system is beneficial for the corporate users because it provides synchronization with microsoft exchange, novell groupwise email, lotus domino, and other business software when used with the blackberry enterprise server. . iphone os / ios: the ios was developed by the apple inc for the use on its device. the ios operating system is the most popular operating system today. it is a very secure operating system. the ios operating system is not available for any other mobiles. . symbian os: symbian operating system is a mobile operating system that provides a high-level of integration with communication. the symbian operating system is based on the java language. it combines middleware of wireless communications and personal information management (pim) functionality. the symbian operating system was developed by symbian ltd in  for the use of mobile phones. nokia was the first company to release symbian os on its mobile phone at that time. . windows mobile os: the window mobile os is a mobile operating system that was developed by microsoft. it was designed for the pocket pcs and smart mobiles. . harmony os: the harmony operating system is the latest mobile operating system that was developed by huawei for the use of its devices. it is designed primarily for iot devices. . palm os: the palm operating system is a mobile operating system that was developed by palm ltd for use on personal digital assistants (pads). it was introduced in . palm os is also known as the garnet os. . webos (palm/hp): the webos is a mobile operating system that was developed by palm. it based on the linux kernel. the hp uses this operating system in its mobile and touchpads. next topic swapping in os
next → ← prev swapping in operating system swapping is a memory management scheme in which any process can be temporarily swapped from main memory to secondary memory so that the main memory can be made available for other processes. it is used to improve main memory utilization. in secondary memory, the place where the swapped-out process is stored is called swap space. the purpose of the swapping in operating system is to access the data present in the hard disk and bring it to ram so that the application programs can use it. the thing to remember is that swapping is used only when data is not present in ram. although the process of swapping affects the performance of the system, it helps to run larger and more than one process. this is the reason why swapping is also referred to as memory compaction. the concept of swapping has divided into two more concepts: swap-in and swap-out. swap-out is a method of removing a process from ram and adding it to the hard disk.
swap-in is a method of removing a program from a hard disk and putting it back into the main memory or ram. example: suppose the user process's size is kb and is a standard hard disk where swapping has a data transfer rate of mbps. now we will calculate how long it will take to transfer from main memory to secondary memory. user process size is kb data transfer rate is mbps =  kbps time = process size / transfer rate =  /  =  seconds =  milliseconds now taking swap-in and swap-out time, the process will take  milliseconds. advantages of swapping it helps the cpu to manage multiple processes within a single main memory. it helps to create and use virtual memory. swapping allows the cpu to perform multiple tasks simultaneously. therefore, processes do not have to wait very long before they are executed. it improves the main memory utilization. disadvantages of swapping if the computer system loses power, the user may lose all information related to the program in case of substantial swapping activity. if the swapping algorithm is not good, the composite method can increase the number of page fault and decrease the overall processing performance. note: in a single tasking operating system, only one process occupies the user program area of memory and stays in memory until the process is complete.
in a multitasking operating system, a situation arises when all the active processes cannot coordinate in the main memory, then a process is swap out from the main memory so that other processes can enter it. next topic threads in os
← prev next →
next → ← prev threads in operating system there is a way of thread execution inside the process of any operating system. apart from this, there can be more than one thread inside a process. thread is often referred to as a lightweight process. the process can be split down into so many threads. for example, in a browser, many tabs can be viewed as threads. ms word uses many threads - formatting text from one thread, processing input from another thread, etc. types of threads in the operating system, there are two types of threads. kernel level thread. user-level thread. user-level thread the operating system does not recognize the user-level thread. user threads can be easily implemented and it is implemented by the user. if a user performs a user-level thread blocking operation, the whole process is blocked. the kernel level thread does not know nothing about the user level thread. the kernel-level thread manages user-level threads as if they are single-threaded processes?examples: java thread, posix threads, etc. advantages of user-level threads the user threads can be easily implemented than the kernel thread. user-level threads can be applied to such types of operating systems that do not support threads at the kernel-level. it is faster and efficient. context switch time is shorter than the kernel-level threads. it does not require modifications of the operating system. user-level threads representation is very simple. the register, pc, stack, and mini thread control blocks are stored in the address space of the user-level process. it is simple to create, switch, and synchronize threads without the intervention of the process. disadvantages of user-level threads user-level threads lack coordination between the thread and the kernel. if a thread causes a page fault, the entire process is blocked. kernel level thread the kernel thread recognizes the operating system. there are a thread control block and process control block in the system for each thread and process in the kernel-level thread. the kernel-level thread is implemented by the operating system. the kernel knows about all the threads and manages them. the kernel-level thread offers a system call to create and manage the threads from user-space. the implementation of kernel threads is difficult than the user thread. context switch time is longer in the kernel thread. if a kernel thread performs a blocking operation, the banky thread execution can continue. example: window solaris. advantages of kernel-level threads the kernel-level thread is fully aware of all threads. the scheduler may decide to spend more cpu time in the process of threads being large numerical. the kernel-level thread is good for those applications that block the frequency. disadvantages of kernel-level threads the kernel thread manages and schedules all threads. the implementation of kernel threads is difficult than the user thread. the kernel-level thread is slower than user-level threads. components of threads any thread has the following components. program counter register set stack space benefits of threads enhanced throughput of the system: when the process is split into many threads, and each thread is treated as a job, the number of jobs done in the unit time increases. that is why the throughput of the system also increases.
when the process is split into many threads, and each thread is treated as a job, the number of jobs done in the unit time increases. that is why the throughput of the system also increases. effective utilization of multiprocessor system: when you have more than one thread in one process, you can schedule more than one thread in more than one processor.
when you have more than one thread in one process, you can schedule more than one thread in more than one processor. faster context switch: the context switching period between threads is less than the process context switching. the process context switch means more overhead for the cpu.
the context switching period between threads is less than the process context switching. the process context switch means more overhead for the cpu. responsiveness: when the process is split into several threads, and when a thread completes its execution, that process can be responded to as soon as possible.
when the process is split into several threads, and when a thread completes its execution, that process can be responded to as soon as possible. communication: multiple-thread communication is simple because the threads share the same address space, while in process, we adopt just a few exclusive communication strategies for communication between two processes.
← prev next →
next → ← prev fedora operating system fedora operating system is an open-source operating system that is based on the linux os kernel architecture. a group of developers was developed the fedora operating system under the fedora project. it is sponsored by red hat. it is designed as a secure operating system for the general-purpose. fedora operating system offers a suite of virus protection, system tools, office productivity services, media playback, and other desktop application. according to the fedora project, it is always free to use, modify, and distribute. fedora os is integrated with applications and packaged software. this operating system enhances the abilities of the software. it offers the same consistency, procedures, and functionality as a traditional os. fedora operating system is the second most commonly used distribution of linux after ubuntu. there are over  distributions based on the fedora operating system, including the xo operating system of red hat enterprise linux. features of fedora operating system list of the fedora os features: fedora os offers many architectures.
fedora os is a very reliable and stable operating system.
it provides unique security features.
fedora os provides a very powerful firewall.
fedora os is very easy to use.
it supports a large community.
fedora os is actively developed.
fedora os is an open-source os.
the interface of fedora os is very attractive.
this operating system offers live mode tools.
this operating system enhances internet speed. fedora os comes with many pre-installed applications and tools, such as internet browser, pdf and word files viewer, pre-installed games, libre office suite, programming language support, etc. fedora is a very stable, secure, and light-weight operating system. it supports different types of architectures, such as ibm z, amd x-x, intel i, ibm powerle, arm-hfp, mips-el, arm aarch, ibm power, etc. usually, it also works on the latest linux kernel. fedora server fedora server is a very flexible and powerful os. it keeps all your infrastructure and services under your control. fedora operating system offers the latest data center technologies. advantages of fedora operating system fedora os is a very reliable and stable operating system. it enhances the security in this operating system. it offers many graphical tools. this operating system updates automatically. this os supports many file formats. it also offers many education software. it supports a large community. it provides unique security features. disadvantages of fedora operating system it requires a long time to set up. it requires additional software tools for the server. it does not provide any standard model for multi-file objects. fedora has its own server, so we can't work on another server in real-time. next topic uses of operating system
← prev next →
next → ← prev uses of operating system the operating system is used everywhere today, such as banks, schools, hospitals, companies, mobiles, etc. no device can operate without an operating system because it controls all the user's commands. linux/unix operating system is used in the bank because it is a very secure operating system.
operating system is used in the bank because it is a very secure operating system. symbian os, windows mobile, ios, and android os are used in mobile phone operating systems as these operating systems are a lightweight operating system. features of operating system the operating system has many notable features that are developing day by day. the growth of the operating system is commendable as it was developed in  to handle storage tape. it acts as an interface. the features of operating system are given below. error detection and handling
handling i/o operations
virtual memory multitasking
program execution
allows disk access and file systems
memory management
protected and supervisor mode
security
resource allocation
easy to run
information and resource protection
multitasking is a very critical feature of the os. with its help, we can run many programs simultaneously.
the operating system provides a platform to run any application program in the computer. due to which we can do our work with the help of that application.
it helps the user in file management. through this, the user can save the data according to his needs.
you use your mouse to open the application and click on the menu. all this is possible due to the modern operating system. this operating system allows you to do this with the help of gui (graphical user interface).
the operating system creates a communication link between the user and the computer, allowing the user to run any application program and obtain the required output properly.
it is almost impossible for a user to use a computer system without an operating system. many processes run simultaneously when a program is executed, which is not easy for a person to manage. next topic producer-consumer problem
← prev next →
next → ← prev producer-consumer problem the producer-consumer problem is a classical multi-process synchronization problem, that is we are trying to achieve synchronization between more than one process. there is one producer in the producer-consumer problem, producer is producing some items, whereas there is one consumer that is consuming the items produced by the producer. the same memory buffer is shared by both producers and consumers which is of fixed-size. the task of the producer is to produce the item, put it into the memory buffer, and again start producing items. whereas the task of the consumer is to consume the item from the memory buffer. let's understand what is the problem? below are a few points that considered as the problems occur in producer-consumer: the producer should produce data only when the buffer is not full. in case it is found that the buffer is full, the producer is not allowed to store any data into the memory buffer.
data can only be consumed by the consumer if and only if the memory buffer is not empty. in case it is found that the buffer is empty, the consumer is not allowed to use any data from the memory buffer.
← prev next →
a philosopher at an even position should pick the right chopstick and then the left chopstick while a philosopher at an odd position should pick the left chopstick and then the right chopstick.
only in case if both the chopsticks ( left and right ) are available at the same time, only then a philosopher should be allowed to pick their chopsticks
all the four starting philosophers ( p, p, p, and p) should pick the left chopstick and then the right chopstick, whereas the last philosopher p should pick the right chopstick and then the left chopstick. this will force p to hold his right chopstick first since the right chopstick of p is c, which is already held by philosopher p and its value is set to , i.e c is already , because of which p will get trapped into an infinite loop and chopstick c remains vacant. hence philosopher p has both left c and right c chopstick available, therefore it will start eating and will put down its both chopsticks once finishes and let others eat which removes the problem of deadlock. the design of the problem was to illustrate the challenges of avoiding deadlock, a deadlock state of a system is a state in which no progress of system is possible. consider a proposal where each philosopher is instructed to behave as follows: the philosopher is instructed to think till the left fork is available, when it is available, hold it.
the philosopher is instructed to think till the right fork is available, when it is available, hold it.
the philosopher is instructed to eat when both forks are available.
then, put the right fork down first
then, put the left fork down next
repeat from the beginning. next topic readers writers problem
next → ← prev history of the operating system operating system the operating system is a system program that serves as an interface between the computing system and the end-user. operating systems create an environment where the user can run any programs or communicate with software or applications in a comfortable and well-organized way. furthermore, an operating is a software program that manages and controls the execution of application programs, software resources and computer hardware. it also helps manage the software/hardware resource, such as file management, memory management, input/ output and many peripheral devices like a disk drive, printers, etc. these are the popular operating system: linux os, windows os, mac os, vms, os/ etc. functions of operating system processor management
act as a resource manager
memory management
file management
security
device management
input devices / output devices
deadlock prevention
time management
coordinate with system software or hardware types of operating system batch operating system time-sharing operating system embedded operating system multiprogramming operating system network operating system distributed operating system multiprocessing operating system real-time operating system batch operating system in batch operating system, there is no direct interaction between user and computer. therefore, the user needs to prepare jobs and save offline mode to punch card or paper tape or magnetic tape. after creating the jobs, hand it over to the computer operator; then the operator sort or creates the similar types of batches like b, b, and b. now, the computer operator submits batches into the cpu to execute the jobs one by one. after that, cpus start executing jobs, and when all jobs are finished, the computer operator provides the output to the user. time-sharing operating system it is the type of operating system that allows us to connect many people located at different locations to share and use a specific system at a single time. the time-sharing operating system is the logical extension of the multiprogramming through which users can run multiple tasks concurrently. furthermore, it provides each user his terminal for input or output that impacts the program or processor currently running on the system. it represents the cpu's time is shared between many user processes. or, the processor's time that is shared between multiple users simultaneously termed as time-sharing. embedded operating system the embedded operating system is the specific purpose operating system used in the computer system's embedded hardware configuration. these operating systems are designed to work on dedicated devices like automated teller machines (atms), airplane systems, digital home assistants, and the internet of things (iot) devices. multiprogramming operating system due to the cpu's underutilization and the waiting for i/o resource till that cpu remains idle. it shows the improper use of system resources. hence, the operating system introduces a new concept that is known as multiprogramming. a multiprogramming operating system refers to the concepts wherein two or more processes or programs activate simultaneously to execute the processes one after another by the same computer system. when a program is in run mode and uses cpu, another program or file uses i/o resources at the same time or waiting for another system resources to become available. it improves the use of system resources, thereby increasing system throughput. such a system is known as a multiprogramming operating system. network operating system a network operating system is an important category of the operating system that operates on a server using network devices like a switch, router, or firewall to handle data, applications and other network resources. it provides connectivity among the autonomous operating system, called as a network operating system. the network operating system is also useful to share data, files, hardware devices and printer resources among multiple computers to communicate with each other. types of network operating system peer-to-peer network operating system: the type of network operating system allows users to share files, resources between two or more computer machines using a lan.
the type of network operating system allows users to share files, resources between two or more computer machines using a lan. client-server network operating system: it is the type of network operating system that allows the users to access resources, functions, and applications through a common server or center hub of the resources. the client workstation can access all resources that exist in the central hub of the network. multiple clients can access and share different types of the resource over the network from different locations.
distributed operating system a distributed operating system provides an environment in which multiple independent cpu or processor communicates with each other through physically separate computational nodes. each node contains specific software that communicates with the global aggregate operating system. with the ease of a distributed system, the programmer or developer can easily access any operating system and resource to execute the computational tasks and achieve a common goal. it is the extension of a network operating system that facilitates a high degree of connectivity to communicate with other users over the network. multiprocessing operating system it is the type of operating system that refers to using two or more central processing units (cpu) in a single computer system. however, these multiprocessor systems or parallel operating systems are used to increase the computer system's efficiency. with the use of a multiprocessor system, they share computer bus, clock, memory and input or output device for concurrent execution of process or program and resource management in the cpu. real-time operating system a real-time operating system is an important type of operating system used to provide services and data processing resources for applications in which the time interval required to process & respond to input/output should be so small without any delay real-time system. for example, real-life situations governing an automatic car, traffic signal, nuclear reactor or an aircraft require an immediate response to complete tasks within a specified time delay. hence, a real-time operating system must be fast and responsive for an embedded system, weapon system, robots, scientific research & experiments and various real-time objects. types of the real-time operating system: hard real-time system
these types of os are used with those required to complete critical tasks within the defined time limit. if the response time is high, it is not accepted by the system or may face serious issues like a system failure. in a hard real-time system, the secondary storage is either limited or missing, so these system stored data in the rom.
these types of os are used with those required to complete critical tasks within the defined time limit. if the response time is high, it is not accepted by the system or may face serious issues like a system failure. in a hard real-time system, the secondary storage is either limited or missing, so these system stored data in the rom. soft real-time system
a soft real-time system is a less restrictive system that can accept software and hardware resources delays by the operating system. in a soft real-time system, a critical task prioritizes less important tasks, and that priority retains active until completion of the task. also, a time limit is set for a specific job, which enables short time delays for further tasks that are acceptable. for example, computer audio or video, virtual reality, reservation system, projects like undersea, etc. generations of operating system the first generation ( to early s) when the first electronic computer was developed in , it was created without any operating system. in early times, users have full access to the computer machine and write a program for each task in absolute machine language. the programmer can perform and solve only simple mathematical calculations during the computer generation, and this calculation does not require an operating system. the second generation ( - ) the first operating system (os) was created in the early s and was known as gmos. general motors has developed os for the ibm computer. the second-generation operating system was based on a single stream batch processing system because it collects all similar jobs in groups or batches and then submits the jobs to the operating system using a punch card to complete all jobs in a machine. at each completion of jobs (either normally or abnormally), control transfer to the operating system that is cleaned after completing one job and then continues to read and initiates the next job in a punch card. after that, new machines were called mainframes, which were very big and used by professional operators. the third generation ( - ) during the late s, operating system designers were very capable of developing a new operating system that could simultaneously perform multiple tasks in a single computer program called multiprogramming. the introduction of multiprogramming plays a very important role in developing operating systems that allow a cpu to be busy every time by performing different tasks on a computer at the same time. during the third generation, there was a new development of minicomputer's phenomenal growth starting in  with the dec pdp-. these pdp's leads to the creation of personal computers in the fourth generation. the fourth generation ( - present day) the fourth generation of operating systems is related to the development of the personal computer. however, the personal computer is very similar to the minicomputers that were developed in the third generation. the cost of a personal computer was very high at that time; there were small fractions of minicomputers costs. a major factor related to creating personal computers was the birth of microsoft and the windows operating system. microsoft created the first window operating system in . after introducing the microsoft windows os, bill gates and paul allen had the vision to take personal computers to the next level. therefore, they introduced the ms-dos in ; however, it was very difficult for the person to understand its cryptic commands. today, windows has become the most popular and most commonly used operating system technology. and then, windows released various operating systems such as windows , windows , windows xp and the latest operating system, windows . currently, most windows users use the windows  operating system. besides the windows operating system, apple is another popular operating system built in the s, and this operating system was developed by steve jobs, a co-founder of apple. they named the operating system macintosh os or mac os. advantages of operating system it is helpful to monitor and regulate resources.
it can easily operate since it has a basic graphical user interface to communicate with your device.
it is used to create interaction between the users and the computer application or hardware.
the performance of the computer system is based on the cpu.
the response time and throughput time of any process or program are fast.
it can share different resources like fax, printer, etc.
it also offers a forum for various types of applications like system and web application. disadvantage of the operating system it allows only a few tasks that can run at the same time.
it any error occurred in the operating system; the stored data can be destroyed.
it is a very difficult task or works for the os to provide entire security from the viruses because any threat or virus can occur at any time in a system.
an unknown user can easily use any system without the permission of the original user.
the cost of operating system costs is very high. next topic types of os
← prev next →
next → ← prev banker's algorithm in operating system it is a banker algorithm used to avoid deadlock and allocate resources safely to each process in the computer system. the 's-state' examines all possible tests or activities before deciding whether the allocation should be allowed to each process. it also helps the operating system to successfully share the resources between all the processes. the banker's algorithm is named because it checks whether a person should be sanctioned a loan amount or not to help the bank system safely simulate allocation resources. in this section, we will learn the banker's algorithm in detail. also, we will solve problems based on the banker's algorithm. to understand the banker's algorithm first we will see a real word example of it. suppose the number of account holders in a particular bank is 'n', and the total money in a bank is 't'. if an account holder applies for a loan; first, the bank subtracts the loan amount from full cash and then estimates the cash difference is greater than t to approve the loan amount. these steps are taken because if another person applies for a loan or withdraws some amount from the bank, it helps the bank manage and operate all things without any restriction in the functionality of the banking system. similarly, it works in an operating system. when a new process is created in a computer system, the process must provide all types of information to the operating system like upcoming processes, requests for their resources, counting them, and delays. based on these criteria, the operating system decides which process sequence should be executed or waited so that no deadlock occurs in a system. therefore, it is also known as deadlock avoidance algorithm or deadlock detection in the operating system. advantages following are the essential characteristics of the banker's algorithm: it contains various resources that meet the requirements of each process. each process should provide information to the operating system for upcoming resource requests, the number of resources, and how long the resources will be held. it helps the operating system manage and control process requests for each type of resource in the computer system. the algorithm has a max resource attribute that represents indicates each process can hold the maximum number of resources in a system. disadvantages it requires a fixed number of processes, and no additional processes can be started in the system while executing the process. the algorithm does no longer allows the processes to exchange its maximum needs while processing its tasks. each process has to know and state their maximum resource requirement in advance for the system. the number of resource requests can be granted in a finite time, but the time limit for allocating the resources is one year. when working with a banker's algorithm, it requests to know about three things: how much each process can request for each resource in the system. it is denoted by the [max] request. how much each process is currently holding each resource in a system. it is denoted by the [allocated] resource. it represents the number of each resource currently available in the system. it is denoted by the [available] resource. following are the important data structures terms applied in the banker's algorithm as follows: suppose n is the number of processes, and m is the number of each type of resource used in a computer system. available: it is an array of length 'm' that defines each type of resource available in the system. when available[j] = k, means that 'k' instances of resources type r[j] are available in the system. max: it is a [n x m] matrix that indicates each process p[i] can store the maximum number of resources r[j] (each type) in a system. allocation: it is a matrix of m x n orders that indicates the type of resources currently allocated to each process in the system. when allocation [i, j] = k, it means that process p[i] is currently allocated k instances of resources type r[j] in the system. need: it is an m x n matrix sequence representing the number of remaining resources for each process. when the need[i] [j] = k, then process p[i] may require k more instances of resources type rj to complete the assigned work.
nedd[i][j] = max[i][j] - allocation[i][j]. finish: it is the vector of the order m. it includes a boolean value (true/false) indicating whether the process has been allocated to the requested resources, and all resources have been released after finishing its task. the banker's algorithm is the combination of the safety algorithm and the resource request algorithm to control the processes and avoid deadlock in a system: safety algorithm it is a safety algorithm used to check whether or not a system is in a safe state or follows the safe sequence in a banker's algorithm: . there are two vectors wok and finish of length m and n in a safety algorithm. initialize: work = available
finish[i] = false; for i = , , , , … n - . . check the availability status for each type of resources [i], such as: need[i] <= work
finish[i] == false
if the i does not exist, go to step . . work = work +allocation(i) // to get new resource allocation finish[i] = true go to step  to check the status of resource availability for the next process. . if finish[i] == true; it means that the system is safe for all processes. resource request algorithm a resource request algorithm checks how a system will behave when a process makes each type of resource request in a system as a request matrix. let create a resource request array r[i] for each process p[i]. if the resource request i [j] equal to 'k', which means the process p[i] requires 'k' instances of resources type r[j] in the system. . when the number of requested resources of each type is less than the need resources, go to step  and if the condition fails, which means that the process p[i] exceeds its maximum claim for the resource. as the expression suggests: if request(i) <= need
go to step ; . and when the number of requested resources of each type is less than the available resource for each process, go to step (). as the expression suggests: if request(i) <= available
else process p[i] must wait for the resource since it is not available for use. . when the requested resource is allocated to the process by changing state: available = available - request
allocation(i) = allocation(i) + request (i)
need i = need i - request i when the resource allocation state is safe, its resources are allocated to the process p(i). and if the new state is unsafe, the process p (i) has to wait for each type of request r(i) and restore the old resource-allocation state. example: consider a system that contains five processes p, p, p, p, p and the three resource types a, b and c. following are the resources types: a has , b has  and the resource type c has  instances. process allocation
a b c max
a b c available
a b c p          p       p       p       p       answer the following questions using the banker's algorithm: what is the reference of the need matrix? determine if the system is safe or not. what will happen if the resource request (, , ) for process p can the system accept this request immediately? ans. : context of the need matrix is as follows: need [i] = max [i] - allocation [i]
need for p: (, , ) - (, , ) = , , 
need for p: (, , ) - (, , ) = , , 
need for p: (, , ) - (, , ) = , , 
need for p: (, , ) - (, , ) = , , 
need for p: (, , ) - (, , ) = , ,  process need
a b c p    p    p    p    p    hence, we created the context of need matrix. ans. : apply the banker's algorithm: available resources of a, b and c are , , and . now we check if each type of resource request is available for each process. step : for process p: need  , ,  similarly, we examine another process p. step : for process p: p need  , ,  similarly, we examine another process p. step : for process p: p need  , ,  now, we again examine each type of resource request for processes p and p. step : for process p: p need  , ,  so, we examine another process p. step : for process p: p need  , ,  hence, we execute the banker's algorithm to find the safe state and the safe sequence like p, p, p, p and p. ans. : for granting the request (, , ), first we have to check that request <= available, that is (, , ) <= (, , ), since the condition is true. so the process p gets the request immediately. next topic operating system mcq
← prev next →
next → ← prev operating system mcq ) which of the following is not an operating system? windows linux oracle dos show answer workspace answer: (c) oracle explanation: oracle is an rdbms (relational database management system). it is known as oracle database, oracle db, or oracle only. the first database for enterprise grid computing is the oracle database. ) what is the maximum length of the filename in dos?     show answer workspace answer: (c)  explanation: the maximum length of the filename is  characters in the dos operating system. it is commonly known as an . filename. ) when was the first operating system developed?     show answer workspace answer: (c)  explanation: the first operating system was developed in the early 's. it was also called a single-stream batch processing system because it presented data in groups. ) when were ms windows operating systems proposed?     show answer workspace answer: (d)  explanation: the first ms windows operating system was introduced in early . ) which of the following is the extension of notepad? .txt .xls .ppt .bmp show answer workspace answer: (a) .txt explanation: the .txt file extension is a standard text document extension that contains the unformatted text. it is the default file extension for the notepad. ) what else is a command interpreter called? prompt kernel shell command show answer workspace answer: (c) shell explanation: the command interpreter is also called the shell. ) what is the full name of fat? file attribute table file allocation table font attribute table format allocation table show answer workspace answer: (b) file allocation table. explanation: the fat stands for file allocation table. the fat is a file system architecture. it is used in computer systems and memory cards. a fat of the contents of a computer disk indicates which field is used for which file. ) bios is used? by operating system by compiler by interpreter by application software show answer workspace answer: (a) by operating system explanation: bios is used by the operating system. it is used to configure and identify the hardware in a system such as the hard drive, floppy drive, optical drive, cpu, and memory. ) what is the mean of the booting in the operating system? restarting computer install the program to scan to turn off show answer workspace answer: (a) restarting computer explanation: booting is a process of the restart the computer. after restarting it, there is no software in the computer's main memory. ) when does page fault occur? the page is present in memory. the deadlock occurs. the page does not present in memory. the buffering occurs. show answer workspace answer: (c) the page does not present in memory. explanation: page faults occur when a process tries to access a block page of the memory and that page is not stored in ram (read only memory) or memory. ) banker's algorithm is used? to prevent deadlock to deadlock recovery to solve the deadlock none of these show answer workspace answer: (a) to prevent deadlock explanation: banker's algorithm is used to prevent the deadlock condition. the banker algorithm is sometimes called the detection algorithm. it is named the banker algorithm because it is used to determine whether a loan can be granted in the banking system or not. ) when you delete a file in your computer, where does it go? recycle bin hard disk taskbar none of these show answer workspace answer: (a) recycle bin explanation: when you delete a file on your computer device, it is transferred to your computer system's recycle bin or trash. ) which is the linux operating system? private operating system windows operating system open-source operating system none of these show answer workspace answer: (c) open-source operating system explanation: the linux operating system is an open-source operating system made up of a kernel. it is a very safe operating system. ) what is the full name of the dsm? direct system module direct system memory demoralized system memory distributed shared memory show answer workspace answer: (d) distributed shared memory explanation: the dsm stands for distributed shared memory. ) what is the full name of the idl? interface definition language interface direct language interface data library none of these show answer workspace answer: (a) interface definition language explanation: the idl stands for interface definition language. it is used to establish communications between clients and servers in rpc (remote procedure call). ) what is bootstrapping called? cold boot cold hot boot cold hot strap hot boot show answer workspace answer: (a) cold boot explanation: bootstrapping is also known as the cool boot. ) what is the fence register used for? to disk protection to cpu protection to memory protection none of these show answer workspace answer: (c) to memory protection explanation: the fence register is used for memory protection on the computer. it is a way to access the memory in the computer. ) if the page size increases, the internal fragmentation is also?..? decreases increases remains constant none of these show answer workspace answer: (b) increases explanation: none ) which of the following is a single-user operating system? windows mac ms-dos none of these show answer workspace answer: (c) ms-dos explanation: the single-user operating system is the operating system in which only one user can access the computer system at a time, and ms-dos is the best example of a single-user operating system. ) the size of virtual memory is based on which of the following? cpu ram address bus data bus show answer workspace answer: (c) address bus explanation: the size of virtual memory is based on the address bus. ) if a page number is not found in the translation lookaside buffer, then it is known as a? translation lookaside buffer miss buffer miss translation lookaside buffer hit all of the mentioned show answer workspace answer: (a) translation lookaside buffer miss explanation: a translation lookaside buffer miss arises when the page table entry needed to translate a virtual address to a physical address is not available in the translation lookaside buffer. ) which of the following is not application software? windows  wordpad photoshop ms-excel show answer workspace answer: (a) windows  explanation: windows  is not an application software because it is a operating system. ) which of the following supports windows  bit? window xp window  window  none of these show answer workspace answer: (a) window xp explanation: windows xp supports the -bits. windows xp is designed to expand the memory address space. its original name is microsoft windows xp professional x and it is based on the x- architecture. ) which of the following windows does not have a start button? windows  windows  windows xp none of these show answer workspace answer: (b) windows  explanation: windows  does not have a start button because it uses the tablet mode, but windows . has a start button. ) which of the following operating systems does not support more than one program at a time? linux windows mac dos show answer workspace answer: (d) dos explanation: dos stands for disk operating system. disk operating system is a single-user operating system that does not support more than one program at a time. ) which of the following is a condition that causes deadlock? mutual exclusion hold and wait circular wait no preemption all of these show answer workspace answer: (e) all of these explanation: none ) who provides the interface to access the services of the operating system? api system call library assembly instruction show answer workspace answer: (b) system call explanation: the system call provides an interface for user programs to access the services of the operating system through the api (application program interface). ) where are placed the list of processes that are prepared to be executed and waiting? job queue ready queue execution queue process queue show answer workspace answer: (b) ready queue explanation: the ready queue is a set of all the processes that processes are ready to execute and wait. ) who among the following can block the running process? fork read down all of these show answer workspace answer: (d) all of these explanation: none ) which of the following does not interrupt the running process? timer interrupt device power failure scheduler process show answer workspace answer: (b) scheduler process explanation: scheduler process does not interrupt in any running process. its job is to select the processes for long-term, short-term, and short-term scheduler. ) what is microsoft window? operating system graphics program word processing database program show answer workspace answer: (a) operating system explanation: microsoft windows is an operating system that was developed by microsoft company. the microsoft windows is available in -bits and -bits in the market. ) which of the following is group of programs? accessories paint word all of above show answer workspace answer: (a) accessories explanation: the windows accessories are a group of programs in the operating system. windows xp offers many accessories or software that you can use to help with your work. the accessories are not full features programs, but it is useful for a specific task in the operating systems. it provides many programs such as a painting program, a calculator, a word processor, a notepad, and internet software. ) which of the following is an example of a real time operating system? mac ms-dos windows  process control show answer workspace answer: (d) process control explanation: process control is a best example of a real time operating system. ) which of the following operating systems do you use for a client-server network? mac linux windows xp windows  show answer workspace answer: (d) windows  explanation: windows  operating systems were used to implement a client server network. it is a server os that was developed by microsoft in april , . it includes some features of windows xp. ) which windows was introduced to my computer? windows  windows xp windows  windows  show answer workspace answer: (c) windows  explanation: windows  was first window to introduced the my computer. ) what type of commands are required to perform various tasks in dos? internal commands external commands valuable commands primary commands show answer workspace answer: (b) external commands explanation: external commands are required to perform various tasks in dos. ) what is the number of characters contained in the primary name of the file of ms-dos? up to  characters  characters up to  characters none of the above show answer workspace answer: (a) up to  characters explanation: ms-dos operating system uses the file system that supports the . characters. the eight characters are used to the filename, and three characters are used to the extension. ) which command is used to fetch a group (.doc) of files that have just been deleted? undelete undelete/all undelete *.doc all of above show answer workspace answer: (c) undelete *.doc explanation: undelete *.doc command is used to fetch a group (.doc) of files that have just been deleted. ) which of the following is system software? operating system compiler utilities all of the above show answer workspace answer: (d) all of the above explanation: the system software is a type of computer program designed to run hardware and software programs on a computer. according to some definitions, system software also includes system utilities, system restore, development tools, compilers, and debuggers. ) which program runs first after booting the computer and loading the gui? desktop manager file manager windows explorer authentication show answer workspace answer: (d) authentication explanation: the authentication program is run first after booting the computer and loading the gui. authentication is a process of verifying the person or device. for example, when you log in to facebook, you enter a username and password. next topic operating system tutoria
